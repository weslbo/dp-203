{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "synapse-weslbo"
		},
		"AdventureWorksDBLinkedService_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AdventureWorksDBLinkedService'"
		},
		"AzureDatabricks1_accessToken": {
			"type": "secureString",
			"metadata": "Secure string for 'accessToken' of 'AzureDatabricks1'"
		},
		"DP203AzureDataLakeStorage_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'DP203AzureDataLakeStorage'"
		},
		"SalesCosmos_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'SalesCosmos'"
		},
		"SqlDwh_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'SqlDwh'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=\"@{concat('synapse-', linkedService().suffix,'.sql.azuresynapse.net')}\";Initial Catalog=sqldwh"
		},
		"synapse-weslbo-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'synapse-weslbo-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:synapse-weslbo.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"DP203AzureDataLakeStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://datalakeweslbo.dfs.core.windows.net/"
		},
		"DataLake_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "@{concat('https://datalake', linkedService().suffix, '.dfs.core.windows.net')}"
		},
		"GitHubDP203AzureDataEngineer_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://raw.githubusercontent.com/MicrosoftLearning/DP-203-Azure-Data-Engineer/"
		},
		"GitHubDP500AzureDataAnalyst_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://raw.githubusercontent.com/MicrosoftLearning/DP-500-Azure-Data-Analyst/"
		},
		"KeyVault_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "@{concat('https://keyvault-', linkedService().suffix, '.vault.azure.net/')}"
		},
		"Products_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://raw.githubusercontent.com/MicrosoftLearning/dp-203-azure-data-engineer/master/Allfiles/labs/01/adventureworks/products.csv"
		},
		"synapse-weslbo-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://datalakeweslbo.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Lab-Ingest data with a pipeline')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Copy products data",
				"activities": [
					{
						"name": "Copy products",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [
							{
								"name": "Destination",
								"value": "files/product_data/products.csv"
							}
						],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "HttpReadSettings",
									"requestMethod": "GET"
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings",
									"skipLineCount": 0
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".txt"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "GithubDP_203_Lab01_Products",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DataLake_File_Lab01_Products",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"folder": {
					"name": "dp-203/01-Intro to data engineering"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/GithubDP_203_Lab01_Products')]",
				"[concat(variables('workspaceId'), '/datasets/DataLake_File_Lab01_Products')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Load Product Data')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "LoadProducts",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "LoadProductsData",
								"type": "DataFlowReference",
								"parameters": {
									"suffix": {
										"value": "'@{replace(pipeline().DataFactory, 'synapse-', '')}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"ProductsText": {
										"suffix": {
											"value": "@replace(pipeline().DataFactory, 'synapse-', '')",
											"type": "Expression"
										}
									},
									"ProductTable": {
										"suffix": {
											"value": "@replace(pipeline().DataFactory, 'synapse-', '')",
											"type": "Expression"
										}
									},
									"DimProductTable": {
										"suffix": "@replace(pipeline().DataFactory, 'synapse-', '')"
									}
								}
							},
							"staging": {
								"linkedService": {
									"referenceName": "DataLake",
									"type": "LinkedServiceReference",
									"parameters": {
										"suffix": {
											"value": "@replace(pipeline().DataFactory, 'synapse-', '')",
											"type": "Expression"
										}
									}
								},
								"folderPath": "files/labs-dp-203/10/stage_products"
							},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"folder": {
					"name": "dp-203/05-Pipelines/10-Lab-Build a data pipeline in Azure Synapse Analytics"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/LoadProductsData')]",
				"[concat(variables('workspaceId'), '/linkedServices/DataLake')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Transform Sales Data')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Run Spark Transform",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "11-Spark Transform",
								"type": "NotebookReference"
							},
							"parameters": {
								"runId": {
									"value": {
										"value": "@pipeline().RunId",
										"type": "Expression"
									},
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "sparkpool",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"folder": {
					"name": "dp-203/05-Pipelines/11-Lab-Apache Spark notebooks in a pipeline"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/11-Spark Transform')]",
				"[concat(variables('workspaceId'), '/bigDataPools/sparkpool')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dp-203-Copy data')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "This setup pipeline needs to executed after the DP-203 environment has been deployed. It will copy files and setup data sources",
				"activities": [
					{
						"name": "For each file in Github 203",
						"description": "For each file in the official github account https://github.com/MicrosoftLearning/DP-203-Azure-Data-Engineer, copy the files into the data lake",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Delete DataLake files",
								"dependencyConditions": [
									"Completed"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@variables('filesToCopyDP203')",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "Copy file",
									"description": "Copy file from github into datalake",
									"type": "Copy",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "BinarySource",
											"storeSettings": {
												"type": "HttpReadSettings",
												"requestMethod": "GET"
											},
											"formatSettings": {
												"type": "BinaryReadSettings"
											}
										},
										"sink": {
											"type": "BinarySink",
											"storeSettings": {
												"type": "AzureBlobFSWriteSettings"
											}
										},
										"enableStaging": false
									},
									"inputs": [
										{
											"referenceName": "GithubDP_203_AllFiles",
											"type": "DatasetReference",
											"parameters": {
												"fileName": {
													"value": "@item()",
													"type": "Expression"
												}
											}
										}
									],
									"outputs": [
										{
											"referenceName": "DataLake_FileContainer_AllFiles_dp203",
											"type": "DatasetReference",
											"parameters": {
												"fileName": {
													"value": "@item()",
													"type": "Expression"
												}
											}
										}
									]
								}
							]
						}
					},
					{
						"name": "Delete DataLake files",
						"type": "Delete",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DataLake_FileContainer_AllFiles_dp203",
								"type": "DatasetReference",
								"parameters": {
									"fileName": "*"
								}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"fileListPath": "files/labs-dp-203",
								"enablePartitionDiscovery": false
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"variables": {
					"filesToCopyDP203": {
						"type": "Array",
						"defaultValue": [
							"master/Allfiles/labs/01/setup.sql",
							"master/Allfiles/labs/01/adventureworks/products.csv",
							"master/Allfiles/labs/01/data/DimCurrency.fmt",
							"master/Allfiles/labs/01/data/DimCurrency.txt",
							"master/Allfiles/labs/01/data/DimCustomer.txt",
							"master/Allfiles/labs/01/data/DimDate.fmt",
							"master/Allfiles/labs/01/data/DimDate.txt",
							"master/Allfiles/labs/01/data/DimGeography.fmt",
							"master/Allfiles/labs/01/data/DimGeography.txt",
							"master/Allfiles/labs/01/data/DimProduct.fmt",
							"master/Allfiles/labs/01/data/DimProduct.txt",
							"master/Allfiles/labs/01/data/DimProductCategory.fmt",
							"master/Allfiles/labs/01/data/DimProductCategory.txt",
							"master/Allfiles/labs/01/data/DimProductSubCategory.fmt",
							"master/Allfiles/labs/01/data/DimProductSubCategory.txt",
							"master/Allfiles/labs/01/data/DimPromotion.fmt",
							"master/Allfiles/labs/01/data/DimPromotion.txt",
							"master/Allfiles/labs/01/data/DimSalesTerritory.fmt",
							"master/Allfiles/labs/01/data/DimSalesTerritory.txt",
							"master/Allfiles/labs/01/data/FactInternetSales.fmt",
							"master/Allfiles/labs/01/data/FactInternetSales.txt",
							"master/Allfiles/labs/01/files/ingest-data.kql",
							"master/Allfiles/labs/01/files/sales.csv",
							"master/Allfiles/labs/01/iot/devices.csv",
							"master/Allfiles/labs/02/data/2019.csv",
							"master/Allfiles/labs/02/data/2019.snappy.parquet",
							"master/Allfiles/labs/02/data/2020.csv",
							"master/Allfiles/labs/02/data/2020.snappy.parquet",
							"master/Allfiles/labs/02/data/2021.csv",
							"master/Allfiles/labs/02/data/2021.snappy.parquet",
							"master/Allfiles/labs/02/data/SO43700.json",
							"master/Allfiles/labs/02/data/SO43701.json",
							"master/Allfiles/labs/02/data/SO43703.json",
							"master/Allfiles/labs/02/data/SO43704.json",
							"master/Allfiles/labs/02/data/SO43705.json",
							"master/Allfiles/labs/03/data/2019.csv",
							"master/Allfiles/labs/03/data/2020.csv",
							"master/Allfiles/labs/03/data/2021.csv",
							"master/Allfiles/labs/04/data/customer.csv",
							"master/Allfiles/labs/04/data/product.csv",
							"master/Allfiles/labs/04/data/salesorder.csv",
							"master/Allfiles/labs/05/data/2019.csv",
							"master/Allfiles/labs/05/data/2020.csv",
							"master/Allfiles/labs/05/data/2021.csv",
							"master/Allfiles/labs/06/data/2019.csv",
							"master/Allfiles/labs/06/data/2020.csv",
							"master/Allfiles/labs/06/data/2021.csv",
							"master/Allfiles/labs/06/notebooks/Spark Transform.ipynb",
							"master/Allfiles/labs/07/data/products.csv",
							"master/Allfiles/labs/08/setup.sql",
							"master/Allfiles/labs/08/Solution.sql",
							"master/Allfiles/labs/08/data/DimAccount.fmt",
							"master/Allfiles/labs/08/data/DimAccount.txt",
							"master/Allfiles/labs/08/data/DimCurrency.fmt",
							"master/Allfiles/labs/08/data/DimCurrency.txt",
							"master/Allfiles/labs/08/data/DimCustomer.fmt",
							"master/Allfiles/labs/08/data/DimCustomer.txt",
							"master/Allfiles/labs/08/data/DimDate.fmt",
							"master/Allfiles/labs/08/data/DimDate.txt",
							"master/Allfiles/labs/08/data/DimDepartmentGroup.fmt",
							"master/Allfiles/labs/08/data/DimDepartmentGroup.txt",
							"master/Allfiles/labs/08/data/DimEmployee.fmt",
							"master/Allfiles/labs/08/data/DimEmployee.txt",
							"master/Allfiles/labs/08/data/DimGeography.fmt",
							"master/Allfiles/labs/08/data/DimGeography.txt",
							"master/Allfiles/labs/08/data/DimOrganization.fmt",
							"master/Allfiles/labs/08/data/DimOrganization.txt",
							"master/Allfiles/labs/08/data/DimProduct.fmt",
							"master/Allfiles/labs/08/data/DimProduct.txt",
							"master/Allfiles/labs/08/data/DimProductCategory.fmt",
							"master/Allfiles/labs/08/data/DimProductCategory.txt",
							"master/Allfiles/labs/08/data/DimProductSubCategory.fmt",
							"master/Allfiles/labs/08/data/DimProductSubCategory.txt",
							"master/Allfiles/labs/08/data/DimPromotion.fmt",
							"master/Allfiles/labs/08/data/DimPromotion.txt",
							"master/Allfiles/labs/08/data/DimReseller.fmt",
							"master/Allfiles/labs/08/data/DimReseller.txt",
							"master/Allfiles/labs/08/data/DimSalesTerritory.fmt",
							"master/Allfiles/labs/08/data/DimSalesTerritory.txt",
							"master/Allfiles/labs/08/data/FactInternetSales.fmt",
							"master/Allfiles/labs/08/data/FactInternetSales.txt",
							"master/Allfiles/labs/08/data/FactResellerSales.fmt",
							"master/Allfiles/labs/08/data/FactResellerSales.txt",
							"master/Allfiles/labs/09/setup.sql",
							"master/Allfiles/labs/09/data/Customer.csv",
							"master/Allfiles/labs/09/data/DimAccount.fmt",
							"master/Allfiles/labs/09/data/DimAccount.txt",
							"master/Allfiles/labs/09/data/DimCurrency.fmt",
							"master/Allfiles/labs/09/data/DimCurrency.txt",
							"master/Allfiles/labs/09/data/DimCustomer.fmt",
							"master/Allfiles/labs/09/data/DimCustomer.txt",
							"master/Allfiles/labs/09/data/DimDate.fmt",
							"master/Allfiles/labs/09/data/DimDate.txt",
							"master/Allfiles/labs/09/data/DimDepartmentGroup.fmt",
							"master/Allfiles/labs/09/data/DimDepartmentGroup.txt",
							"master/Allfiles/labs/09/data/DimEmployee.fmt",
							"master/Allfiles/labs/09/data/DimEmployee.txt",
							"master/Allfiles/labs/09/data/DimGeography.fmt",
							"master/Allfiles/labs/09/data/DimGeography.txt",
							"master/Allfiles/labs/09/data/DimOrganization.fmt",
							"master/Allfiles/labs/09/data/DimOrganization.txt",
							"master/Allfiles/labs/09/data/DimProductCategory.fmt",
							"master/Allfiles/labs/09/data/DimProductCategory.txt",
							"master/Allfiles/labs/09/data/DimProductSubCategory.fmt",
							"master/Allfiles/labs/09/data/DimProductSubCategory.txt",
							"master/Allfiles/labs/09/data/DimPromotion.fmt",
							"master/Allfiles/labs/09/data/DimPromotion.txt",
							"master/Allfiles/labs/09/data/DimReseller.fmt",
							"master/Allfiles/labs/09/data/DimReseller.txt",
							"master/Allfiles/labs/09/data/DimSalesTerritory.fmt",
							"master/Allfiles/labs/09/data/DimSalesTerritory.txt",
							"master/Allfiles/labs/09/data/FactInternetSales.fmt",
							"master/Allfiles/labs/09/data/FactInternetSales.txt",
							"master/Allfiles/labs/09/data/FactResellerSales.fmt",
							"master/Allfiles/labs/09/data/FactResellerSales.txt",
							"master/Allfiles/labs/09/data/Product.csv",
							"master/Allfiles/labs/10/setup.sql",
							"master/Allfiles/labs/10/data/Product.csv",
							"master/Allfiles/labs/11/data/2019.csv",
							"master/Allfiles/labs/11/data/2020.csv",
							"master/Allfiles/labs/11/data/2021.csv",
							"master/Allfiles/labs/11/notebooks/Spark Transform.ipynb",
							"master/Allfiles/labs/18/setup.sql",
							"master/Allfiles/labs/22/data/products.csv",
							"master/Allfiles/labs/23/adventureworks/products.csv",
							"master/Allfiles/labs/24/Databricks-Spark.dbc",
							"master/Allfiles/labs/24/data/2019.csv",
							"master/Allfiles/labs/24/data/2020.csv",
							"master/Allfiles/labs/24/data/2021.csv",
							"master/Allfiles/labs/25/Delta-Lake.dbc",
							"master/Allfiles/labs/25/data/devices1.json",
							"master/Allfiles/labs/25/data/devices2.json",
							"master/Allfiles/labs/25/data/products.csv",
							"master/Allfiles/labs/26/data/products.csv",
							"master/Allfiles/labs/27/Process-Data.dbc",
							"master/Allfiles/labs/27/data/products.csv"
						]
					}
				},
				"folder": {
					"name": "dp-203/00-Setup"
				},
				"annotations": [],
				"lastPublishTime": "2023-02-03T06:46:19Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DataLake_FileContainer_AllFiles_dp203')]",
				"[concat(variables('workspaceId'), '/datasets/GithubDP_203_AllFiles')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSynapseAnalyticsTable1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "SqlDwh",
					"type": "LinkedServiceReference",
					"parameters": {
						"suffix": {
							"value": "@replace(pipeline().DataFactory, 'synapse-', '')",
							"type": "Expression"
						}
					}
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [],
				"typeProperties": {
					"schema": "dbo",
					"table": "ProductSample"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/SqlDwh')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSynapseAnalyticsTable2')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "SqlDwh",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [],
				"typeProperties": {
					"schema": "dbo",
					"table": "products_adventureworks"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/SqlDwh')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSynapseAnalyticsTable3')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "SqlDwh",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [],
				"typeProperties": {
					"schema": "dbo",
					"table": "Products"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/SqlDwh')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ConferenceJsonDestination')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "synapse-weslbo-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "conference.json",
						"fileSystem": "landing"
					}
				},
				"schema": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/synapse-weslbo-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DP500LandingCSV')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "DataLake",
					"type": "LinkedServiceReference",
					"parameters": {
						"suffix": {
							"value": "@replace(pipeline().DataFactory, 'synapse-', '')",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"filename": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@concat(dataset().filename, '.csv')",
							"type": "Expression"
						},
						"folderPath": "adventureworkslt/tables/csv",
						"fileSystem": "landing"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/DataLake')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DP500LandingJson')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "DataLake",
					"type": "LinkedServiceReference",
					"parameters": {
						"suffix": {
							"value": "@replace(pipeline().DataFactory, 'synapse-', '')",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"filename": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Json",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@concat(dataset().filename, '.json')",
							"type": "Expression"
						},
						"folderPath": "adventureworkslt/tables/json",
						"fileSystem": "landing"
					}
				},
				"schema": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/DataLake')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DP500LandingParquet')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "DataLake",
					"type": "LinkedServiceReference",
					"parameters": {
						"suffix": {
							"value": "@replace(pipeline().DataFactory, 'synapse-', '')",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"filename": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@concat(dataset().filename, '.parquet')",
							"type": "Expression"
						},
						"folderPath": "adventureworkslt/tables/parquet",
						"fileSystem": "landing"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/DataLake')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DataLake_FileContainer_AllFiles_dp203')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "DataLake",
					"type": "LinkedServiceReference",
					"parameters": {
						"suffix": {
							"value": "@replace(pipeline().DataFactory, 'synapse-', '')",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"fileName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@replace(dataset().fileName, 'master/Allfiles/labs', 'labs-dp-203')",
							"type": "Expression"
						},
						"fileSystem": "files"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/DataLake')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DataLake_FileContainer_AllFiles_dp500')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "DataLake",
					"type": "LinkedServiceReference",
					"parameters": {
						"suffix": {
							"value": "@replace(pipeline().DataFactory, 'synapse-', '')",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"fileName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@replace(dataset().fileName, 'main/Allfiles', 'labs-dp-500')",
							"type": "Expression"
						},
						"fileSystem": "files"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/DataLake')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DataLake_File_Lab01_Products')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "DataLake",
					"type": "LinkedServiceReference",
					"parameters": {
						"suffix": {
							"value": "@replace(pipeline().DataFactory, 'synapse-', '')",
							"type": "Expression"
						}
					}
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "products.csv",
						"folderPath": "labs-dp-203/01/product_data",
						"fileSystem": "files"
					},
					"columnDelimiter": ",",
					"rowDelimiter": "\n",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/DataLake')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DestinationDataset_846')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "synapse-weslbo-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"cw_fileName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().cw_fileName",
							"type": "Expression"
						},
						"fileSystem": "landing"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/synapse-weslbo-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DestinationDataset_ohv')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "synapse-weslbo-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "customers.csv",
						"fileSystem": "landing"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/synapse-weslbo-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DimProduct')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "SqlDwh",
					"type": "LinkedServiceReference",
					"parameters": {
						"suffix": {
							"value": "@dataset().suffix",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"suffix": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [
					{
						"name": "ProductKey",
						"type": "int",
						"precision": 10
					},
					{
						"name": "ProductAltKey",
						"type": "nvarchar"
					},
					{
						"name": "ProductName",
						"type": "nvarchar"
					},
					{
						"name": "Color",
						"type": "nvarchar"
					},
					{
						"name": "Size",
						"type": "nvarchar"
					},
					{
						"name": "ListPrice",
						"type": "money",
						"precision": 19,
						"scale": 4
					},
					{
						"name": "Discontinued",
						"type": "bit"
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "DimProductLab10"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/SqlDwh')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/GithubDP_203_AllFiles')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "GitHubDP203AzureDataEngineer",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"fileName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "HttpServerLocation",
						"relativeUrl": {
							"value": "@dataset().fileName",
							"type": "Expression"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/GitHubDP203AzureDataEngineer')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/GithubDP_203_Lab01_Products')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Products",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "HttpServerLocation"
					},
					"columnDelimiter": ",",
					"rowDelimiter": "\n",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "ProductID",
						"type": "String"
					},
					{
						"name": "ProductName",
						"type": "String"
					},
					{
						"name": "Category",
						"type": "String"
					},
					{
						"name": "ListPrice",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/Products')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/GithubDP_500_AllFiles')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "GitHubDP500AzureDataAnalyst",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"fileName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "HttpServerLocation",
						"relativeUrl": {
							"value": "@dataset().fileName",
							"type": "Expression"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/GitHubDP500AzureDataAnalyst')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ProductParquet')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "synapse-weslbo-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileSystem": "landing"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/synapse-weslbo-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Products_Csv')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "DataLake",
					"type": "LinkedServiceReference",
					"parameters": {
						"suffix": {
							"value": "@dataset().suffix",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"suffix": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "Product.csv",
						"folderPath": "labs-dp-203/10/data",
						"fileSystem": "files"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "ProductID",
						"type": "String"
					},
					{
						"name": "ProductName",
						"type": "String"
					},
					{
						"name": "Color",
						"type": "String"
					},
					{
						"name": "Size",
						"type": "String"
					},
					{
						"name": "ListPrice",
						"type": "String"
					},
					{
						"name": "Discontinued",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/DataLake')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SalesLTProductDataSet')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AdventureWorksDBLinkedService",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [
					{
						"name": "ProductID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "Name",
						"type": "nvarchar"
					},
					{
						"name": "ProductNumber",
						"type": "nvarchar"
					},
					{
						"name": "Color",
						"type": "nvarchar"
					},
					{
						"name": "StandardCost",
						"type": "money",
						"precision": 19,
						"scale": 4
					},
					{
						"name": "ListPrice",
						"type": "money",
						"precision": 19,
						"scale": 4
					},
					{
						"name": "Size",
						"type": "nvarchar"
					},
					{
						"name": "Weight",
						"type": "decimal",
						"precision": 8,
						"scale": 2
					},
					{
						"name": "ProductCategoryID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "ProductModelID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "SellStartDate",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "SellEndDate",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "DiscontinuedDate",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					},
					{
						"name": "ThumbNailPhoto",
						"type": "varbinary"
					},
					{
						"name": "ThumbnailPhotoFileName",
						"type": "nvarchar"
					},
					{
						"name": "rowguid",
						"type": "uniqueidentifier"
					},
					{
						"name": "ModifiedDate",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					}
				],
				"typeProperties": {
					"schema": "SalesLT",
					"table": "Product"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AdventureWorksDBLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SalesLTProductsParquet')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "DP203AzureDataLakeStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "products.parquet",
						"fileSystem": "bronze"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/DP203AzureDataLakeStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SourceDataset_846')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AdventureWorksDBLinkedService",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"cw_table": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": "SalesLT",
					"table": {
						"value": "@dataset().cw_table",
						"type": "Expression"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AdventureWorksDBLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SourceDataset_ohv')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AdventureWorksDBLinkedService",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [
					{
						"name": "CustomerID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "NameStyle",
						"type": "bit"
					},
					{
						"name": "Title",
						"type": "nvarchar"
					},
					{
						"name": "FirstName",
						"type": "nvarchar"
					},
					{
						"name": "MiddleName",
						"type": "nvarchar"
					},
					{
						"name": "LastName",
						"type": "nvarchar"
					},
					{
						"name": "Suffix",
						"type": "nvarchar"
					},
					{
						"name": "CompanyName",
						"type": "nvarchar"
					},
					{
						"name": "SalesPerson",
						"type": "nvarchar"
					},
					{
						"name": "EmailAddress",
						"type": "nvarchar"
					},
					{
						"name": "Phone",
						"type": "nvarchar"
					},
					{
						"name": "PasswordHash",
						"type": "varchar"
					},
					{
						"name": "PasswordSalt",
						"type": "varchar"
					},
					{
						"name": "rowguid",
						"type": "uniqueidentifier"
					},
					{
						"name": "ModifiedDate",
						"type": "datetime",
						"precision": 23,
						"scale": 3
					}
				],
				"typeProperties": {
					"schema": "SalesLT",
					"table": "Customer"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AdventureWorksDBLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SqlPoolTable1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlPoolTable",
				"schema": [],
				"typeProperties": {
					"schema": "dbo",
					"table": "DimCustomers"
				},
				"sqlPool": {
					"referenceName": "sqldwh",
					"type": "SqlPoolReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/sqldwh')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AdventureWorksDBLinkedService')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('AdventureWorksDBLinkedService_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDatabricks1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDatabricks",
				"typeProperties": {
					"domain": "https://adb-3270789565584912.12.azuredatabricks.net",
					"accessToken": {
						"type": "SecureString",
						"value": "[parameters('AzureDatabricks1_accessToken')]"
					},
					"newClusterNodeType": "Standard_DS3_v2",
					"newClusterNumOfWorker": "2",
					"newClusterSparkEnvVars": {
						"PYSPARK_PYTHON": "/databricks/python3/bin/python3"
					},
					"newClusterVersion": "10.4.x-scala2.12",
					"clusterOption": "Fixed",
					"newClusterInitScripts": []
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DP203AzureDataLakeStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('DP203AzureDataLakeStorage_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('DP203AzureDataLakeStorage_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DataLake')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "The Azure Data Lake that is linked to this workspace",
				"parameters": {
					"suffix": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('DataLake_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/GitHubDP203AzureDataEngineer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Official location of the GitHub repository DP-203: Azure Data Engineer",
				"annotations": [],
				"type": "HttpServer",
				"typeProperties": {
					"url": "[parameters('GitHubDP203AzureDataEngineer_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/GitHubDP500AzureDataAnalyst')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "HttpServer",
				"typeProperties": {
					"url": "[parameters('GitHubDP500AzureDataAnalyst_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/KeyVault')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"suffix": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('KeyVault_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Products')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "used in Lab 1",
				"annotations": [],
				"type": "HttpServer",
				"typeProperties": {
					"url": "[parameters('Products_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SalesCosmos')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "CosmosDb",
				"typeProperties": {
					"connectionString": "[parameters('SalesCosmos_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SqlDwh')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"suffix": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('SqlDwh_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapse-weslbo-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('synapse-weslbo-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapse-weslbo-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('synapse-weslbo-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LoadProductsData')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/05-Pipelines/10-Lab-Build a data pipeline in Azure Synapse Analytics"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Products_Csv",
								"type": "DatasetReference"
							},
							"name": "ProductsText",
							"description": "Products text data"
						},
						{
							"dataset": {
								"referenceName": "DimProduct",
								"type": "DatasetReference"
							},
							"name": "ProductTable"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DimProduct",
								"type": "DatasetReference"
							},
							"name": "DimProductTable",
							"description": "Load DimProduct table"
						}
					],
					"transformations": [
						{
							"name": "MatchedProducts",
							"description": "Matched product data. The lookup returns a set of columns from both sources, essentially forming an outer join that matches the ProductID column in the text file to the ProductAltKey column in the data warehouse table. When a product with the alternate key already exists in the table, the dataset will include the values from both sources. When the product dos not already exist in the data warehouse, the dataset will contain NULL values for the table columns."
						},
						{
							"name": "SetLoadAction",
							"description": "Insert new, upsert existing"
						}
					],
					"scriptLines": [
						"parameters{",
						"     suffix as string",
						"}",
						"source(output(",
						"          ProductID as string,",
						"          ProductName as string,",
						"          Color as string,",
						"          Size as string,",
						"          ListPrice as string,",
						"          Discontinued as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> ProductsText",
						"source(output(",
						"          ProductKey as integer,",
						"          ProductAltKey as string,",
						"          ProductName as string,",
						"          Color as string,",
						"          Size as string,",
						"          ListPrice as decimal(19,4),",
						"          Discontinued as boolean",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table',",
						"     staged: true) ~> ProductTable",
						"ProductsText, ProductTable lookup(ProductID == ProductAltKey,",
						"     multiple: false,",
						"     pickup: 'last',",
						"     asc(ProductKey, true),",
						"     broadcast: 'auto')~> MatchedProducts",
						"MatchedProducts alterRow(insertIf(isNull(ProductKey)),",
						"     upsertIf(not(isNull(ProductKey)))) ~> SetLoadAction",
						"SetLoadAction sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          ProductKey as integer,",
						"          ProductAltKey as string,",
						"          ProductName as string,",
						"          Color as string,",
						"          Size as string,",
						"          ListPrice as decimal(19,4),",
						"          Discontinued as boolean",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:true,",
						"     keys:['ProductAltKey'],",
						"     format: 'table',",
						"     staged: true,",
						"     allowCopyCommand: true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          ProductAltKey = ProductID,",
						"          ProductName = ProductsText@ProductName,",
						"          Color = ProductsText@Color,",
						"          Size = ProductsText@Size,",
						"          ListPrice = ProductsText@ListPrice,",
						"          Discontinued = ProductsText@Discontinued",
						"     )) ~> DimProductTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Products_Csv')]",
				"[concat(variables('workspaceId'), '/datasets/DimProduct')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/00-Drop tables sqldwh')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/00-Setup"
				},
				"content": {
					"query": "-- 00-Drop tables sqldwh\n\nIF EXISTS (SELECT 1 FROM INFORMATION_SCHEMA.VIEWS WHERE TABLE_SCHEMA = 'dbo' AND TABLE_NAME = 'vFactSales') DROP VIEW [dbo].[vFactSales];\n\nwhile(exists(select 1 from INFORMATION_SCHEMA.TABLES))\nbegin\n\tdeclare @sql2 nvarchar(2000)\n\tSELECT TOP 1 @sql2=('DROP TABLE ' + TABLE_SCHEMA + '.[' + TABLE_NAME\n\t+ ']')\n\tFROM INFORMATION_SCHEMA.TABLES\n\texec (@sql2)\n\tPRINT @sql2\nend\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqldwh",
						"poolName": "sqldwh"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01-Query RetailDB')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/02-Serverless SQL/04-Demo-Analyze data in a lake database"
				},
				"content": {
					"query": "-- Demo: Analyze data in a lake database\n-- Task: Query RetailDB\n\n--Make sure to connect to the Build-In, default database\nSELECT TOP (100) [CustomerId]\n    ,[FirstName]\n    ,[LastName]\n    ,[EmailAddress]\n    ,[Phone]\n FROM [RetailDB].[dbo].[Customer]\n\n SELECT TOP (100) [ProductId]\n    ,[ProductName]\n    ,[IntroductionDate]\n    ,[ActualAbandonmentDate]\n    ,[ProductGrossWeight]\n    ,[ItemSku]\n    ,[ListPrice]\n FROM [RetailDB].[dbo].[Product]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "default",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01-Query Sales CSV files')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/02-Serverless SQL/03-Lab-Transform files using a serverless SQL pool"
				},
				"content": {
					"query": "-- Lab: Transform files using a serverless SQL pool\n-- Task: Query Sales CSV files\n\n-- This code uses the OPENROWSET to read data from the CSV files in the sales folder and retrieves the first 100 rows of data.\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://datalakeweslbo.dfs.core.windows.net/files/labs-dp-203/03/data/**',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0'\n    ) AS [result]\n\n-- In this case, the data files include the column names in the first row; so modify the query to add a HEADER_ROW = TRUE parameter to the WITH clause, as shown here \nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://datalakeweslbo.dfs.core.windows.net/files/labs-dp-203/03/data/**',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0',\n        HEADER_ROW = TRUE\n    ) AS [result]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01-Setup tables sqldwh')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/00-Setup"
				},
				"content": {
					"query": "-- 01-Setup tables sqldwh\n\nSET ANSI_NULLS ON\nGO\nSET QUOTED_IDENTIFIER ON\nGO\nCREATE TABLE [dbo].[FactInternetSales](\n\t[SalesOrderNumber] [nvarchar](20) NOT NULL,\n\t[SalesOrderLineNumber] [tinyint] NOT NULL,\n\t[CustomerKey] [int] NOT NULL,\n\t[ProductKey] [int] NOT NULL,\n\t[OrderDateKey] [int] NOT NULL,\n\t[DueDateKey] [int] NOT NULL,\n\t[ShipDateKey] [int] NULL,\n\t[PromotionKey] [int] NOT NULL,\n\t[CurrencyKey] [int] NOT NULL,\n\t[SalesTerritoryKey] [int] NOT NULL,\n\t[OrderQuantity] [smallint] NOT NULL,\n\t[UnitPrice] [money] NOT NULL,\n\t[ExtendedAmount] [money] NOT NULL,\n\t[UnitPriceDiscountPct] [decimal](7, 4) NOT NULL,\n\t[DiscountAmount] [float] NOT NULL,\n\t[ProductStandardCost] [money] NOT NULL,\n\t[TotalProductCost] [money] NOT NULL,\n\t[SalesAmount] [money] NOT NULL,\n\t[TaxAmount] [money] NOT NULL,\n\t[FreightAmount] [money] NOT NULL,\n\t[CarrierTrackingNumber] [nvarchar](25) NULL,\n\t[CustomerPONumber] [nvarchar](25) NULL,\n\t[RevisionNumber] [tinyint] NOT NULL\n)\n\nGO\nCREATE TABLE [dbo].[DimCustomer](\n\t[CustomerKey] [int] IDENTITY(1,1) NOT NULL,\n\t[GeographyKey] [int] NULL,\n\t[CustomerAlternateKey] [nvarchar](15) NOT NULL,\n\t[Title] [nvarchar](8) NULL,\n\t[FirstName] [nvarchar](50) NULL,\n\t[MiddleName] [nvarchar](50) NULL,\n\t[LastName] [nvarchar](50) NULL,\n\t[NameStyle] [bit] NULL,\n\t[BirthDate] [date] NULL,\n\t[MaritalStatus] [nchar](1) NULL,\n\t[Suffix] [nvarchar](10) NULL,\n\t[Gender] [nvarchar](1) NULL,\n\t[EmailAddress] [nvarchar](50) NULL,\n\t[YearlyIncome] [money] NULL,\n\t[TotalChildren] [tinyint] NULL,\n\t[NumberChildrenAtHome] [tinyint] NULL,\n\t[EnglishEducation] [nvarchar](40) NULL,\n\t[SpanishEducation] [nvarchar](40) NULL,\n\t[FrenchEducation] [nvarchar](40) NULL,\n\t[EnglishOccupation] [nvarchar](100) NULL,\n\t[SpanishOccupation] [nvarchar](100) NULL,\n\t[FrenchOccupation] [nvarchar](100) NULL,\n\t[HouseOwnerFlag] [nchar](1) NULL,\n\t[NumberCarsOwned] [tinyint] NULL,\n\t[AddressLine1] [nvarchar](120) NULL,\n\t[AddressLine2] [nvarchar](120) NULL,\n\t[Phone] [nvarchar](20) NULL,\n\t[DateFirstPurchase] [date] NULL,\n\t[CommuteDistance] [nvarchar](15) NULL\n)\n\nGO\nCREATE TABLE [dbo].[DimDate](\n\t[DateKey] [int] NOT NULL,\n\t[FullDateAlternateKey] [date] NOT NULL,\n\t[DayNumberOfWeek] [tinyint] NOT NULL,\n\t[EnglishDayNameOfWeek] [nvarchar](10) NOT NULL,\n\t[SpanishDayNameOfWeek] [nvarchar](10) NOT NULL,\n\t[FrenchDayNameOfWeek] [nvarchar](10) NOT NULL,\n\t[DayNumberOfMonth] [tinyint] NOT NULL,\n\t[DayNumberOfYear] [smallint] NOT NULL,\n\t[WeekNumberOfYear] [tinyint] NOT NULL,\n\t[EnglishMonthName] [nvarchar](10) NOT NULL,\n\t[SpanishMonthName] [nvarchar](10) NOT NULL,\n\t[FrenchMonthName] [nvarchar](10) NOT NULL,\n\t[MonthNumberOfYear] [tinyint] NOT NULL,\n\t[CalendarQuarter] [tinyint] NOT NULL,\n\t[CalendarYear] [smallint] NOT NULL,\n\t[CalendarSemester] [tinyint] NOT NULL,\n\t[FiscalQuarter] [tinyint] NOT NULL,\n\t[FiscalYear] [smallint] NOT NULL,\n\t[FiscalSemester] [tinyint] NOT NULL\n)\n\nGO\nCREATE TABLE [dbo].[DimGeography](\n\t[GeographyKey] [int] IDENTITY(1,1) NOT NULL,\n\t[City] [nvarchar](30) NULL,\n\t[StateProvinceCode] [nvarchar](3) NULL,\n\t[StateProvinceName] [nvarchar](50) NULL,\n\t[CountryRegionCode] [nvarchar](3) NULL,\n\t[EnglishCountryRegionName] [nvarchar](50) NULL,\n\t[SpanishCountryRegionName] [nvarchar](50) NULL,\n\t[FrenchCountryRegionName] [nvarchar](50) NULL,\n\t[PostalCode] [nvarchar](15) NULL,\n\t[SalesTerritoryKey] [int] NULL,\n\t[IpAddressLocator] [nvarchar](15) NULL)\n\nGO\nCREATE TABLE [dbo].[DimProduct](\n\t[ProductKey] [int] IDENTITY(1,1) NOT NULL,\n\t[ProductAlternateKey] [nvarchar](25) NULL,\n\t[ProductSubcategoryKey] [int] NULL,\n\t[WeightUnitMeasureCode] [nchar](3) NULL,\n\t[SizeUnitMeasureCode] [nchar](3) NULL,\n\t[EnglishProductName] [nvarchar](50) NOT NULL,\n\t[SpanishProductName] [nvarchar](50) NOT NULL,\n\t[FrenchProductName] [nvarchar](50) NOT NULL,\n\t[StandardCost] [money] NULL,\n\t[FinishedGoodsFlag] [bit] NOT NULL,\n\t[Color] [nvarchar](15) NOT NULL,\n\t[SafetyStockLevel] [smallint] NULL,\n\t[ReorderPoint] [smallint] NULL,\n\t[ListPrice] [money] NULL,\n\t[Size] [nvarchar](50) NULL,\n\t[SizeRange] [nvarchar](50) NULL,\n\t[Weight] [float] NULL,\n\t[DaysToManufacture] [int] NULL,\n\t[ProductLine] [nchar](2) NULL,\n\t[DealerPrice] [money] NULL,\n\t[Class] [nchar](2) NULL,\n\t[Style] [nchar](2) NULL,\n\t[ModelName] [nvarchar](50) NULL,\n\t[LargePhoto] [varbinary](max) NULL,\n\t[EnglishDescription] [nvarchar](400) NULL,\n\t[FrenchDescription] [nvarchar](400) NULL,\n\t[ChineseDescription] [nvarchar](400) NULL,\n\t[ArabicDescription] [nvarchar](400) NULL,\n\t[HebrewDescription] [nvarchar](400) NULL,\n\t[ThaiDescription] [nvarchar](400) NULL,\n\t[GermanDescription] [nvarchar](400) NULL,\n\t[JapaneseDescription] [nvarchar](400) NULL,\n\t[TurkishDescription] [nvarchar](400) NULL,\n\t[StartDate] [datetime] NULL,\n\t[EndDate] [datetime] NULL,\n\t[Status] [nvarchar](7) NULL)\nWITH  \n  (   \n    CLUSTERED INDEX (ProductKey)  \n  ); \nGO\n\nCREATE TABLE [dbo].[DimProductCategory](\n\t[ProductCategoryKey] [int] IDENTITY(1,1) NOT NULL,\n\t[ProductCategoryAlternateKey] [int] NULL,\n\t[EnglishProductCategoryName] [nvarchar](50) NOT NULL,\n\t[SpanishProductCategoryName] [nvarchar](50) NOT NULL,\n\t[FrenchProductCategoryName] [nvarchar](50) NOT NULL)\n\nGO\nCREATE TABLE [dbo].[DimProductSubcategory](\n\t[ProductSubcategoryKey] [int] IDENTITY(1,1) NOT NULL,\n\t[ProductSubcategoryAlternateKey] [int] NULL,\n\t[EnglishProductSubcategoryName] [nvarchar](50) NOT NULL,\n\t[SpanishProductSubcategoryName] [nvarchar](50) NOT NULL,\n\t[FrenchProductSubcategoryName] [nvarchar](50) NOT NULL,\n\t[ProductCategoryKey] [int] NULL)\nGO\n\nCREATE TABLE [dbo].[DimSalesTerritory](\n\t[SalesTerritoryKey] [int] IDENTITY(1,1) NOT NULL,\n\t[SalesTerritoryAlternateKey] [int] NULL,\n\t[SalesTerritoryRegion] [nvarchar](50) NOT NULL,\n\t[SalesTerritoryCountry] [nvarchar](50) NOT NULL,\n\t[SalesTerritoryGroup] [nvarchar](50) NULL,\n\t[SalesTerritoryImage] [varbinary](max) NULL)\nWITH  \n  (   \n    CLUSTERED INDEX (SalesTerritoryKey)  \n  ); \nGO\n\n\nCREATE TABLE [dbo].[FactResellerSales](\n\t[SalesOrderNumber] [nvarchar](20) NOT NULL,\n\t[SalesOrderLineNumber] [tinyint] NOT NULL,\n\t[ResellerKey] [int] NOT NULL,\n\t[ProductKey] [int] NOT NULL,\n\t[OrderDateKey] [int] NOT NULL,\n\t[DueDateKey] [int] NOT NULL,\n\t[ShipDateKey] [int] NULL,\n\t[EmployeeKey] [int] NOT NULL,\n\t[PromotionKey] [int] NOT NULL,\n\t[CurrencyKey] [int] NOT NULL,\n\t[SalesTerritoryKey] [int] NOT NULL,\n\t[OrderQuantity] [smallint] NOT NULL,\n\t[UnitPrice] [money] NOT NULL,\n\t[ExtendedAmount] [money] NOT NULL,\n\t[UnitPriceDiscountPct] [decimal](7, 4) NOT NULL,\n\t[DiscountAmount] [money] NOT NULL,\n\t[ProductStandardCost] [money] NOT NULL,\n\t[TotalProductCost] [money] NOT NULL,\n\t[SalesAmount] [money] NOT NULL,\n\t[TaxAmount] [money] NOT NULL,\n\t[FreightAmount] [money] NOT NULL,\n\t[CarrierTrackingNumber] [nvarchar](25) NULL,\n\t[CustomerPONumber] [nvarchar](25) NULL,\n\t[RevisionNumber] [tinyint] NOT NULL)\nGO\n\nCREATE VIEW [dbo].[vFactSales]\nAS\n\tSELECT\n\t\tCAST(N'Reseller' AS NVARCHAR(10)) AS [Channel]\n\t\t,CAST(RIGHT([SalesOrderNumber], (LEN([SalesOrderNumber]) - 2)) AS INT) AS [SalesOrderKey]\n\t\t,((CAST(RIGHT([SalesOrderNumber], (LEN([SalesOrderNumber]) - 2)) AS INT) * 1000) + [SalesOrderLineNumber]) AS [SalesOrderLineKey]\n\t\t,[SalesOrderNumber]\n\t\t,[SalesOrderLineNumber]\n\t\t,[ResellerKey]\n\t\t,CAST(-1 AS INT) AS [CustomerKey]\n\t\t,[ProductKey]\n\t\t,[OrderDateKey]\n\t\t,[DueDateKey]\n\t\t,[ShipDateKey]\n\t\t,[PromotionKey]\n\t\t,[CurrencyKey]\n\t\t,[SalesTerritoryKey]\n\t\t,[EmployeeKey]\n\t\t,[OrderQuantity]\n\t\t,[UnitPrice]\n\t\t,[ExtendedAmount]\n\t\t,[UnitPriceDiscountPct]\n\t\t,[DiscountAmount]\n\t\t,[ProductStandardCost]\n\t\t,[TotalProductCost]\n\t\t,[SalesAmount]\n\t\t,[TaxAmount]\n\t\t,[FreightAmount]\n\t\t,[CarrierTrackingNumber]\n\t\t,[CustomerPONumber]\n\t\t,[RevisionNumber]\n\tFROM\n\t\t[dbo].[FactResellerSales]\n\tUNION ALL\n\tSELECT\n\t\tCAST(N'Internet' AS NVARCHAR(10)) AS [Channel]\n\t\t,CAST(RIGHT([SalesOrderNumber], (LEN([SalesOrderNumber]) - 2)) AS INT) AS [SalesOrderKey]\n\t\t,((CAST(RIGHT([SalesOrderNumber], (LEN([SalesOrderNumber]) - 2)) AS INT) * 1000) + [SalesOrderLineNumber]) AS [SalesOrderLineKey]\n\t\t,[SalesOrderNumber]\n\t\t,[SalesOrderLineNumber]\n\t\t,CAST(-1 AS INT) AS [ResellerKey]\n\t\t,[CustomerKey]\n\t\t,[ProductKey]\n\t\t,[OrderDateKey]\n\t\t,[DueDateKey]\n\t\t,[ShipDateKey]\n\t\t,[PromotionKey]\n\t\t,[CurrencyKey]\n\t\t,[SalesTerritoryKey]\n\t\t,CAST(-1 AS INT) AS [EmployeeKey]\n\t\t,[OrderQuantity]\n\t\t,[UnitPrice]\n\t\t,[ExtendedAmount]\n\t\t,[UnitPriceDiscountPct]\n\t\t,[DiscountAmount]\n\t\t,[ProductStandardCost]\n\t\t,[TotalProductCost]\n\t\t,[SalesAmount]\n\t\t,[TaxAmount]\n\t\t,[FreightAmount]\n\t\t,[CarrierTrackingNumber]\n\t\t,[CustomerPONumber]\n\t\t,[RevisionNumber]\n\tFROM\n\t\t[dbo].[FactInternetSales];\nGO\n\n\nCREATE TABLE [dbo].[DimAccount](\n\t[AccountKey] [int] IDENTITY(1,1) NOT NULL,\n\t[ParentAccountKey] [int] NULL,\n\t[AccountCodeAlternateKey] [int] NULL,\n\t[ParentAccountCodeAlternateKey] [int] NULL,\n\t[AccountDescription] [nvarchar](50) NULL,\n\t[AccountType] [nvarchar](50) NULL,\n\t[Operator] [nvarchar](50) NULL,\n\t[CustomMembers] [nvarchar](300) NULL,\n\t[ValueType] [nvarchar](50) NULL,\n\t[CustomMemberOptions] [nvarchar](200) NULL)\n\nGO\nCREATE TABLE [dbo].[DimCurrency](\n\t[CurrencyKey] [int] IDENTITY(1,1) NOT NULL,\n\t[CurrencyAlternateKey] [nchar](3) NOT NULL,\n\t[CurrencyName] [nvarchar](50) NOT NULL,\n\t[FormatString] [nvarchar](20) NULL)\n\nGO\nCREATE TABLE [dbo].[DimDepartmentGroup](\n\t[DepartmentGroupKey] [int] IDENTITY(1,1) NOT NULL,\n\t[ParentDepartmentGroupKey] [int] NULL,\n\t[DepartmentGroupName] [nvarchar](50) NULL)\n\nGO\nCREATE TABLE [dbo].[DimEmployee](\n\t[EmployeeKey] [int] IDENTITY(1,1) NOT NULL,\n\t[ParentEmployeeKey] [int] NULL,\n\t[EmployeeNationalIDAlternateKey] [nvarchar](15) NULL,\n\t[ParentEmployeeNationalIDAlternateKey] [nvarchar](15) NULL,\n\t[SalesTerritoryKey] [int] NULL,\n\t[FirstName] [nvarchar](50) NOT NULL,\n\t[LastName] [nvarchar](50) NOT NULL,\n\t[MiddleName] [nvarchar](50) NULL,\n\t[NameStyle] [bit] NOT NULL,\n\t[Title] [nvarchar](50) NULL,\n\t[HireDate] [date] NULL,\n\t[BirthDate] [date] NULL,\n\t[LoginID] [nvarchar](256) NULL,\n\t[EmailAddress] [nvarchar](50) NULL,\n\t[Phone] [nvarchar](25) NULL,\n\t[MaritalStatus] [nchar](1) NULL,\n\t[EmergencyContactName] [nvarchar](50) NULL,\n\t[EmergencyContactPhone] [nvarchar](25) NULL,\n\t[SalariedFlag] [bit] NULL,\n\t[Gender] [nchar](1) NULL,\n\t[PayFrequency] [tinyint] NULL,\n\t[BaseRate] [money] NULL,\n\t[VacationHours] [smallint] NULL,\n\t[SickLeaveHours] [smallint] NULL,\n\t[CurrentFlag] [bit] NOT NULL,\n\t[SalespersonFlag] [bit] NOT NULL,\n\t[DepartmentName] [nvarchar](50) NULL,\n\t[StartDate] [date] NULL,\n\t[EndDate] [date] NULL,\n\t[Status] [nvarchar](50) NULL,\n\t[EmployeePhoto] [varbinary](max) NULL)\nWITH  \n  (   \n    CLUSTERED INDEX (EmployeeKey)  \n  ); \nGO\nCREATE TABLE [dbo].[DimOrganization](\n\t[OrganizationKey] [int] IDENTITY(1,1) NOT NULL,\n\t[ParentOrganizationKey] [int] NULL,\n\t[PercentageOfOwnership] [nvarchar](16) NULL,\n\t[OrganizationName] [nvarchar](50) NULL,\n\t[CurrencyKey] [int] NULL)\n\nGO\nCREATE TABLE [dbo].[DimPromotion](\n\t[PromotionKey] [int] IDENTITY(1,1) NOT NULL,\n\t[PromotionAlternateKey] [int] NULL,\n\t[EnglishPromotionName] [nvarchar](255) NULL,\n\t[SpanishPromotionName] [nvarchar](255) NULL,\n\t[FrenchPromotionName] [nvarchar](255) NULL,\n\t[DiscountPct] [float] NULL,\n\t[EnglishPromotionType] [nvarchar](50) NULL,\n\t[SpanishPromotionType] [nvarchar](50) NULL,\n\t[FrenchPromotionType] [nvarchar](50) NULL,\n\t[EnglishPromotionCategory] [nvarchar](50) NULL,\n\t[SpanishPromotionCategory] [nvarchar](50) NULL,\n\t[FrenchPromotionCategory] [nvarchar](50) NULL,\n\t[StartDate] [datetime] NOT NULL,\n\t[EndDate] [datetime] NULL,\n\t[MinQty] [int] NULL,\n\t[MaxQty] [int] NULL)\nGO\n\nCREATE TABLE [dbo].[DimReseller](\n\t[ResellerKey] [int] IDENTITY(1,1) NOT NULL,\n\t[GeographyKey] [int] NULL,\n\t[ResellerAlternateKey] [nvarchar](15) NULL,\n\t[Phone] [nvarchar](25) NULL,\n\t[BusinessType] [varchar](20) NOT NULL,\n\t[ResellerName] [nvarchar](50) NOT NULL,\n\t[NumberEmployees] [int] NULL,\n\t[OrderFrequency] [char](1) NULL,\n\t[OrderMonth] [tinyint] NULL,\n\t[FirstOrderYear] [int] NULL,\n\t[LastOrderYear] [int] NULL,\n\t[ProductLine] [nvarchar](50) NULL,\n\t[AddressLine1] [nvarchar](60) NULL,\n\t[AddressLine2] [nvarchar](60) NULL,\n\t[AnnualSales] [money] NULL,\n\t[BankName] [nvarchar](50) NULL,\n\t[MinPaymentType] [tinyint] NULL,\n\t[MinPaymentAmount] [money] NULL,\n\t[AnnualRevenue] [money] NULL,\n\t[YearOpened] [int] NULL)\nGO\n\nCREATE TABLE [dbo].[StageProduct](\n    [ProductID] [nvarchar](30) NULL,\n    [ProductName] [nvarchar](50) NULL,\n    [ProductCategory] [nvarchar](24) NULL,\n    [Color] [nvarchar](30) NULL,\n    [Size] [nvarchar](50) NULL,\n    [ListPrice] [money] NULL,\n    [Discontinued] [bit] NULL)\nWITH\n(\n\tDISTRIBUTION = ROUND_ROBIN,\n\tCLUSTERED COLUMNSTORE INDEX\n)\nGO\n\nCREATE TABLE [dbo].[StageCustomer]\n( \n\t[GeographyKey] [int]  NULL,\n\t[CustomerAlternateKey] [nvarchar](15)  NOT NULL,\n\t[Title] [nvarchar](8)  NULL,\n\t[FirstName] [nvarchar](50)  NULL,\n\t[MiddleName] [nvarchar](50)  NULL,\n\t[LastName] [nvarchar](50)  NULL,\n\t[NameStyle] [bit]  NULL,\n\t[BirthDate] [date]  NULL,\n\t[MaritalStatus] [nchar](1)  NULL,\n\t[Suffix] [nvarchar](10)  NULL,\n\t[Gender] [nvarchar](1)  NULL,\n\t[EmailAddress] [nvarchar](50)  NULL,\n\t[YearlyIncome] [money]  NULL,\n\t[TotalChildren] [tinyint]  NULL,\n\t[NumberChildrenAtHome] [tinyint]  NULL,\n\t[EnglishEducation] [nvarchar](40)  NULL,\n\t[SpanishEducation] [nvarchar](40)  NULL,\n\t[FrenchEducation] [nvarchar](40)  NULL,\n\t[EnglishOccupation] [nvarchar](100)  NULL,\n\t[SpanishOccupation] [nvarchar](100)  NULL,\n\t[FrenchOccupation] [nvarchar](100)  NULL,\n\t[HouseOwnerFlag] [nchar](1)  NULL,\n\t[NumberCarsOwned] [tinyint]  NULL,\n\t[AddressLine1] [nvarchar](120)  NULL,\n\t[AddressLine2] [nvarchar](120)  NULL,\n\t[Phone] [nvarchar](20)  NULL,\n\t[DateFirstPurchase] [date]  NULL,\n\t[CommuteDistance] [nvarchar](15)  NULL\n)\nWITH\n(\n\tDISTRIBUTION = ROUND_ROBIN,\n\tCLUSTERED COLUMNSTORE INDEX\n)\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqldwh",
						"poolName": "sqldwh"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01-Use SQL to query CSV files')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/02-Serverless SQL/02-Demo-Query files using a serverless SQL pool"
				},
				"content": {
					"query": "-- Demo: Query files using a serverless SQL pool\n-- Task: Use SQL to query CSV files\n\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://datalakeweslbo.dfs.core.windows.net/files/labs-dp-500/01/data/*.csv',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0'\n    ) AS [result]\n\n/*\nNote the results consist of columns named C1, C2, and so on. In this example, the CSV files do not include \nthe column headers. While it’s possible to work with the data using the generic column names that have been \nassigned, or by ordinal position, it will be easier to understand the data if you define a tabular schema. \nTo accomplish this, add a WITH clause to the OPENROWSET function as shown here (replacing datalakexxxxxxx with \nthe name of your data lake storage account), and then rerun the query\n*/\n\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://datalakeweslbo.dfs.core.windows.net/files/labs-dp-500/01/data/*.csv',\n        FORMAT = 'CSV',\n        PARSER_VERSION='2.0'\n    )\n    WITH (\n        SalesOrderNumber VARCHAR(10) COLLATE Latin1_General_100_BIN2_UTF8,\n        SalesOrderLineNumber INT,\n        OrderDate DATE,\n        CustomerName VARCHAR(25) COLLATE Latin1_General_100_BIN2_UTF8,\n        EmailAddress VARCHAR(50) COLLATE Latin1_General_100_BIN2_UTF8,\n        Item VARCHAR(30) COLLATE Latin1_General_100_BIN2_UTF8,\n        Quantity INT,\n        UnitPrice DECIMAL(18,2),\n        TaxAmount DECIMAL (18,2)\n    ) AS [result]\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01-Use a serverless SQL pool to analyze data')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Used in lab 1 - task \"Use a serverless SQL pool to analyze data\"",
				"folder": {
					"name": "dp-203/01-Intro to data engineering/01-Lab-Explore Azure Synapse Analytics"
				},
				"content": {
					"query": "-- Lab: Explore Azure Synapse Analytics\n-- Task: Use a serverless SQL pool to analyze data\n\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://datalakeweslbo.dfs.core.windows.net/files/labs-dp-203/01/product_data/products.csv',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0'\n        --,HEADER_ROW = TRUE -- Note the results consist of four columns named C1, C2, C3, and C4; \n    ) AS [result]           -- and that the first row in the results contains the names of the data fields. To fix this problem, add a HEADER_ROW = TRUE\n\n\nSELECT\n    Category, COUNT(*) AS ProductCount\nFROM\n    OPENROWSET(\n        BULK 'https://datalakeweslbo.dfs.core.windows.net/files/labs-dp-203/01/product_data/products.csv',\n        FORMAT = 'CSV',\n        PARSER_VERSION='2.0',\n        HEADER_ROW = TRUE\n    ) AS [result]\nGROUP BY Category\n\n-- select the Chart view, and then select the following settings for the chart\n-- Chart type: Column\n-- Category colunn: Category\n-- legend Series columns: ProductCount",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01_sql_serverless_csv')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-500/03-Synapse"
				},
				"content": {
					"query": "-- REMEMBER to replace the datalake account url\n\n\n-- Example: auto generated code (by right-click on a file name)\n-- Notice 1) column headers are not promoted\n-- Notice 2) have a look at the messages: \"Potential conversion error while reading VARCHAR column 'C1' from UTF8 encoded text. Change database collation to a UTF8 collation or specify explicit column schema in WITH clause and assign UTF8 collation to VARCHAR columns.\"\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://datalakeweslbo.dfs.core.windows.net/landing/adventureworkslt/tables/csv/SalesLT.Product.csv',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0'\n    ) AS [result]\n\n\n-- Example: Column headers promoted\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://datalakeweslbo.dfs.core.windows.net/landing/adventureworkslt/tables/csv/SalesLT.Product.csv',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0', \n        HEADER_ROW=TRUE\n    ) AS [result]\n\n-- Example: We can specify our own column names and data types\n-- Notice 1): External data type 'MONEY' is currently not supported. (need to replace to decimal)\n-- Notice 2): External data type 'DATETIME' is currently not supported. (need to replace to DATETIME2)\n-- Notice 3): External data type 'VARBINARY(MAX)' is currently not supported.\nSELECT TOP 100 \n    ProductID\n    , Name\n    , ProductNumber\n    , Color\n    , StandardCost\n    , ListPrice\n    , Size\n    , Weight\n    , ProductCategoryID\n    , ProductModelID\n    , SellStartDate\n    , SellEndDate\n    , DiscontinuedDate\n    --, ThumbNailPhoto\n    , ThumbnailPhotoFileName\n    , rowguid\n    , ModifiedDate\nFROM\n    OPENROWSET(\n        BULK 'https://datalakeweslbo.dfs.core.windows.net/landing/adventureworkslt/tables/csv/SalesLT.Product.csv',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0', \n        HEADER_ROW=TRUE\n    )\n    WITH \n    (\n        ProductID INT,\n        Name NVARCHAR(50),\n        ProductNumber NVARCHAR(25),\n        Color NVARCHAR(15),\n        StandardCost DECIMAL, --not supported MONEY\n        ListPrice DECIMAL, --not supported MONEY\n        Size NVARCHAR(5),\n        Weight DECIMAL,\n        ProductCategoryID INT,\n        ProductModelID INT,\n        SellStartDate DATETIME2,\n        SellEndDate DATETIME2,\n        DiscontinuedDate DATETIME2,\n        --ThumbNailPhoto VARBINARY(max), --not supported VARBINARY\n        ThumbnailPhotoFileName NVARCHAR(50),\n        rowguid UNIQUEIDENTIFIER,\n        ModifiedDate DATETIME2\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/02-Count rows from ALL tables')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/00-Setup"
				},
				"content": {
					"query": "-- 02-Count rows from ALL tables\n\nCREATE TABLE #temp_row_count\n\n    (   row_num INT,\n        table_name VARCHAR(100),\n        row_count INT\n    )\n\nINSERT INTO #temp_row_count \n(row_num, table_name)\n(SELECT \n    ROW_NUMBER() OVER(ORDER BY SCHEMA_NAME(tbl.schema_id)+'.'+tbl.name ASC),\n    SCHEMA_NAME(tbl.schema_id)+'.'+tbl.name\nFROM sys.tables AS tbl\nWHERE SCHEMA_NAME(tbl.schema_id) = 'dbo')\n\nDECLARE @Counter INT \nDECLARE @Table VARCHAR(100)\nSET @Counter = 1\n\nWHILE @Counter <= (SELECT COUNT(*) FROM #temp_row_count)\nBEGIN\n    SET @Table = (SELECT table_name FROM #temp_row_count WHERE row_num = @Counter)\n    EXEC('UPDATE #temp_row_count SET row_count = (SELECT COUNT(*) FROM '+ @Table +') WHERE row_num='''+ @Counter +'''')\n    SET @Counter = @Counter + 1\nEND\n\nSELECT * FROM #temp_row_count\nDROP TABLE #temp_row_count",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqldwh",
						"poolName": "sqldwh"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/02-Create Sales DB with External table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/02-Serverless SQL/03-Lab-Transform files using a serverless SQL pool"
				},
				"content": {
					"query": "-- Lab: Transform files using a serverless SQL pool\n-- Task: Create Sales DB with External table\n\n-- By defining an external data source in a database, you can use it to reference the data lake \n-- location where you want to store files for external tables. An external file format enables \n-- you to define the format for those files - for example, Parquet or CSV. To use these objects \n-- to work with external tables, you need to create them in a database other than the \n-- default master database.\n\n--DROP DATABASE Sales\n--GO;\n\n-- Database for sales data\nCREATE DATABASE Sales\n  COLLATE Latin1_General_100_BIN2_UTF8;\nGO;\n\nUse Sales;\nGO;\n\n-- External data is in the Files container in the data lake\nCREATE EXTERNAL DATA SOURCE sales_data WITH (\n    LOCATION = 'https://datalakeweslbo.dfs.core.windows.net/files/labs-dp-203'\n);\nGO;\n\n-- Format for table files\nCREATE EXTERNAL FILE FORMAT ParquetFormat\n    WITH (\n            FORMAT_TYPE = PARQUET,\n            DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n        )\nGO;\n\n-- retrieve and aggregate data from the CSV sales files by using the external data source\n-- note that the BULK path is relative to the folder location on which the data source is defined:\nSELECT Item AS Product,\n       SUM(Quantity) AS ItemsSold,\n       ROUND(SUM(UnitPrice) - SUM(TaxAmount), 2) AS NetRevenue\nFROM\n    OPENROWSET(\n        BULK '03/data/*.csv', -- originally: sales/csv/*.csv\n        DATA_SOURCE = 'sales_data',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0',\n        HEADER_ROW = TRUE\n    ) AS orders\nGROUP BY Item;\n\n-- save the results of query in an external table, like this:\nCREATE EXTERNAL TABLE ProductSalesTotals\n    WITH (\n        LOCATION = '03/productsales/',\n        DATA_SOURCE = sales_data,\n        FILE_FORMAT = ParquetFormat\n    )\nAS\nSELECT Item AS Product,\n    SUM(Quantity) AS ItemsSold,\n    ROUND(SUM(UnitPrice) - SUM(TaxAmount), 2) AS NetRevenue\nFROM\n    OPENROWSET(\n        BULK '03/data/*.csv',\n        DATA_SOURCE = 'sales_data',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0',\n        HEADER_ROW = TRUE\n    ) AS orders\nGROUP BY Item;\n\n-- Make sure to navigate to the files section: \n-- labs/03/data/productsales/\n-- Observe that one or more files with names similar to ABC123DE----.parquet have been created. \n-- These files contain the aggregated product sales data. \n-- To prove this, you can select one of the files and use the New SQL script > Select TOP 100 rows menu to query it directly.\n-- This is auto-generated code:\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://datalakeweslbo.dfs.core.windows.net/files/labs-dp-203/03/productsales/*.parquet',\n        FORMAT = 'PARQUET'\n    ) AS [result]\n\n-- Make sure to look in the Data hub, (refresh SQL Database)\n-- Notice the Sales database, external table name dbo.ProductSalesTotals\n\n-- Run following query\nSELECT TOP 10 * FROM ProductSalesTotals\n\n\n-- Clean up\nDROP EXTERNAL TABLE ProductSalesTotals\nDROP EXTERNAL FILE FORMAT ParquetFormat\nDROP EXTERNAL DATA SOURCE sales_data\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "Sales",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/02-Use SQL to query JSON files')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/02-Serverless SQL/02-Demo-Query files using a serverless SQL pool"
				},
				"content": {
					"query": "-- Demo: Query files using a serverless SQL pool\n-- Task: Use SQL to query JSON files\n\n-- The script is designed to query comma-delimited (CSV) data rather then JSON, so you need to make a few modifications before it will work successfully.\nSELECT TOP 100\n    jsonContent\nFROM\n    OPENROWSET(\n        BULK 'https://datalakeweslbo.dfs.core.windows.net/files/labs-dp-500/01/data/*.json',\n        FORMAT = 'CSV',\n        FIELDQUOTE = '0x0b',\n        FIELDTERMINATOR ='0x0b',\n        ROWTERMINATOR = '0x0b'\n    )\n    WITH (\n        jsonContent varchar(MAX)\n    ) AS [result]\n\n\n-- Modify the query as follows so that it uses the JSON_VALUE function to extract individual field values from the JSON data.\nSELECT JSON_VALUE(Doc, '$.SalesOrderNumber') AS OrderNumber,\n    JSON_VALUE(Doc, '$.CustomerName') AS Customer,\n    Doc\nFROM\n    OPENROWSET(\n        BULK 'https://datalakeweslbo.dfs.core.windows.net/files/labs-dp-500/01/data/*.json',\n        FORMAT = 'CSV',\n        FIELDTERMINATOR ='0x0b',\n        FIELDQUOTE = '0x0b',\n        ROWTERMINATOR = '0x0b'\n    ) WITH (Doc NVARCHAR(MAX)) as rows",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/02-Use a dedicated SQL pool to query a data warehouse')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/01-Intro to data engineering/01-Lab-Explore Azure Synapse Analytics"
				},
				"content": {
					"query": "-- Lab: Explore Azure Synapse Analytics\n-- Task: Use a dedicated SQL pool to query a data warehouse\n\nSELECT d.CalendarYear, d.MonthNumberOfYear, d.EnglishMonthName,\n    p.EnglishProductName AS Product, SUM(o.OrderQuantity) AS UnitsSold\nFROM dbo.FactInternetSales AS o\nJOIN dbo.DimDate AS d ON o.OrderDateKey = d.DateKey\nJOIN dbo.DimProduct AS p ON o.ProductKey = p.ProductKey\nGROUP BY d.CalendarYear, d.MonthNumberOfYear, d.EnglishMonthName, p.EnglishProductName\nORDER BY d.MonthNumberOfYear",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqldwh",
						"poolName": "sqldwh"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/02-Work with lake database tables')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/02-Serverless SQL/04-Demo-Analyze data in a lake database"
				},
				"content": {
					"query": "-- Demo: Analyze data in a lake database\n-- Task: Work with lake database tables\n\n-- Make sure to select the RetailDB\n\nSELECT o.SalesOrderID, c.EmailAddress, p.ProductName, o.Quantity\nFROM RetailDB.dbo.SalesOrder AS o\nJOIN RetailDB.dbo.Customer AS c ON o.CustomerId = c.CustomerId\nJOIN RetailDB.dbo.Product AS p ON o.ProductId = p.ProductId",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "default",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/02_sql_serverless_json')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-500/03-Synapse"
				},
				"content": {
					"query": "-- REMEMBER to replace the datalake account url\n\n-- EXAMPLE: select from JSON file. Here, we have explicitly defined the JSON_VALUE elements\nSELECT TOP 100\n      JSON_VALUE(jsonContent, '$.ProductID') as ProductId\n    , JSON_VALUE(jsonContent, '$.Name') as Name\n    , JSON_VALUE(jsonContent, '$.ProductNumber') as ProductNumber\n    , JSON_VALUE(jsonContent, '$.Color') as Color\n    , JSON_VALUE(jsonContent, '$.StandardCost') as StandardCost\n    , JSON_VALUE(jsonContent, '$.ListPrice') as ListPrice\n    , JSON_VALUE(jsonContent, '$.Size') as Size\n    , JSON_VALUE(jsonContent, '$.Weight') as Weight\n    , JSON_VALUE(jsonContent, '$.ProductCategoryID') as ProductCategoryID\n    , JSON_VALUE(jsonContent, '$.ProductModelID') as ProductModelID\n    , JSON_VALUE(jsonContent, '$.SellStartDate') as SellStartDate\n    , JSON_VALUE(jsonContent, '$.SellEndDate') as SellEndDate\n    , JSON_VALUE(jsonContent, '$.DiscontinuedDate') as DiscontinuedDate\n    , JSON_VALUE(jsonContent, '$.ThumbNailPhoto') as ThumbNailPhoto\n    , JSON_VALUE(jsonContent, '$.ThumbnailPhotoFileName') as ThumbnailPhotoFileName\n    , JSON_VALUE(jsonContent, '$.rowguid') as rowguid\n    , JSON_VALUE(jsonContent, '$.ModifiedDate') as ModifiedDate\nFROM\n    OPENROWSET(\n        BULK 'https://datalakeweslbo.dfs.core.windows.net/landing/adventureworkslt/tables/json/SalesLT.Product.json',\n        FORMAT = 'CSV',\n        FIELDQUOTE = '0x0b',\n        FIELDTERMINATOR ='0x0b'\n        --ROWTERMINATOR = '0x0b' -- Doesn't work here as lines are separated by a newline (no comma)\n    )\n    WITH (\n        jsonContent varchar(MAX)\n    ) AS [result]\n\n\n\n-- EXAMPLE: We can specify our own column names and data types\nSELECT TOP 100 *\nFROM \n    OPENROWSET(\n        BULK 'https://datalakeweslbo.dfs.core.windows.net/landing/adventureworkslt/tables/json/SalesLT.Product.json',\n        FORMAT = 'CSV',\n        FIELDQUOTE = '0x0b',\n        FIELDTERMINATOR ='0x0b'\n    )\n    WITH \n    (\n        jsonContent varchar(MAX)\n    ) AS [result]\n    CROSS APPLY openjson(jsonContent)\n        WITH \n        (\n            ProductID INT,\n            Name NVARCHAR(50),\n            ProductNumber NVARCHAR(25),\n            Color NVARCHAR(15),\n            StandardCost MONEY,\n            ListPrice MONEY,\n            Size NVARCHAR(5),\n            Weight DECIMAL,\n            ProductCategoryID INT,\n            ProductModelID INT,\n            SellStartDate DATETIME2,\n            SellEndDate DATETIME2,\n            DiscontinuedDate DATETIME2,\n            ThumbNailPhoto VARBINARY(max), \n            ThumbnailPhotoFileName NVARCHAR(50),\n            RowGuid UNIQUEIDENTIFIER '$.rowguid',   -- notice that we can override the name of the column\n            ModifiedDate DATETIME2\n        )",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/03-Create Stored Procedure')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/02-Serverless SQL/03-Lab-Transform files using a serverless SQL pool"
				},
				"content": {
					"query": "-- Lab: Transform files using a serverless SQL pool\n-- Task: Create Stored Procedure\n\nUSE Sales;\nGO;\n\n-- External data is in the Files container in the data lake\nCREATE EXTERNAL DATA SOURCE sales_data WITH (\n    LOCATION = 'https://datalakeweslbo.dfs.core.windows.net/files/labs-dp-203'\n);\nGO;\n\n-- Format for table files\nCREATE EXTERNAL FILE FORMAT ParquetFormat\n    WITH (\n            FORMAT_TYPE = PARQUET,\n            DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n        );\nGO;\n\n-- If you will need to transform data frequently, you can use a stored procedure to encapsulate a CETAS statement.\n-- create a stored procedure in the Sales database that aggregates sales by \n-- year and saves the results in an external table:\n\nCREATE PROCEDURE sp_GetYearlySales\nAS\nBEGIN\n    IF EXISTS (\n            SELECT * FROM sys.external_tables\n            WHERE name = 'YearlySalesTotals'\n        )\n        DROP EXTERNAL TABLE YearlySalesTotals\n    \n    CREATE EXTERNAL TABLE YearlySalesTotals\n    WITH (\n            LOCATION = '03/yearlysales/',\n            DATA_SOURCE = sales_data,\n            FILE_FORMAT = ParquetFormat\n        )\n    AS\n    SELECT YEAR(OrderDate) AS CalendarYear,\n           SUM(Quantity) AS ItemsSold,\n           ROUND(SUM(UnitPrice) - SUM(TaxAmount), 2) AS NetRevenue\n    FROM\n        OPENROWSET(\n            BULK '03/data/*.csv',\n            DATA_SOURCE = 'sales_data',\n            FORMAT = 'CSV',\n            PARSER_VERSION = '2.0',\n            HEADER_ROW = TRUE\n        ) AS orders\n    GROUP BY YEAR(OrderDate)\nEND\n\nGO;\n\n-- Execute the stored procedure:\nEXEC sp_GetYearlySales;\n\n-- On the files tab containing the file system for your data lake, \n-- view the contents of the sales folder (refreshing the view if necessary) \n-- and verify that a new yearlysales folder has been created.\n\nSELECT * FROM YearlySalesTotals\n\n\nDROP PROCEDURE sp_GetYearlySales \nDROP EXTERNAL TABLE YearlySalesTotals\nDROP EXTERNAL FILE FORMAT ParquetFormat\nDROP EXTERNAL DATA SOURCE sales_data\n\nUSE Master\nDROP DATABASE Sales",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/03-Use SQL to query parquet files')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/02-Serverless SQL/02-Demo-Query files using a serverless SQL pool"
				},
				"content": {
					"query": "-- Demo: Query files using a serverless SQL pool\n-- Task: Use SQL to query parquet files\n\n /*\n Run the code, and note that it returns sales order data in the same schema as the CSV files you explored earlier. \n The schema information is embedded in the parquet file, so the appropriate column names are shown in the results.\n */\n\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://datalakeweslbo.dfs.core.windows.net/files/labs-dp-500/01/data/*.parquet',\n        FORMAT = 'PARQUET'\n    ) AS [result]\n\n/* \nNote that in the following query the results include order counts for all three years - the wildcard used in the BULK \npath causes the query to return data from all subfolders. The subfolders reflect partitions in the parquet data, which \nis a technique often used to optimize performance for systems that can process multiple partitions of data in parallel. \nYou can also use partitions to filter the data.\n*/\n\nSELECT YEAR(OrderDate) AS OrderYear,\n    COUNT(*) AS OrderedItems\nFROM\n    OPENROWSET(\n        BULK 'https://datalakeweslbo.dfs.core.windows.net/files/labs-dp-500/01/data/*.parquet',\n        FORMAT = 'PARQUET'\n    ) AS [result]\nGROUP BY YEAR(OrderDate)\nORDER BY OrderYear\n\n/*\nReview the results and note that they include only the sales counts for 2019 and 2020. This filtering is \nachieved by inclusing a wildcard for the partition folder value in the BULK path (year=*) and a WHERE \nclause based on the filepath property of the results returned by OPENROWSET (which in this case has the alias [result]).\n*/\n\nSELECT YEAR(OrderDate) AS OrderYear,\n    COUNT(*) AS OrderedItems\nFROM\n    OPENROWSET(\n        BULK 'https://datalakeweslbo.dfs.core.windows.net/files/labs-dp-500/01/data/*.parquet',\n        FORMAT = 'PARQUET'\n    ) AS [result]\nWHERE replace([result].filepath(1), '.snappy', '') IN ('2019', '2020')\nGROUP BY YEAR(OrderDate)\nORDER BY OrderYear",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/03-Validate Insert after Spark')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/02-Serverless SQL/04-Demo-Analyze data in a lake database"
				},
				"content": {
					"query": "-- Demo: Analyze data in a lake database\n-- Task: Validate Insert after Spark\n\nUse RetailDB\n\n-- This statement should not return any data before the notebook has executed.\n\nSELECT *\nFROM SalesOrder WHERE SalesOrderId = 99999\n\n-- cleanup for next demo\n-- This will not work: DML Operations are not supported with external tables.\nDELETE FROM SalesOrder WHERE SalesOrderId = 99999",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "default",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/03_sql_serverless_parquet')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-500/03-Synapse"
				},
				"content": {
					"query": "-- REMEMBER to replace the datalake account url\n\n-- EXAMPLE: This is auto-generated code\n-- NOTICE: we do not need to specify data types (inferred from parquet file)\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://datalakecnweslbo.dfs.core.windows.net/landing/adventureworkslt/tables/parquet/SalesLT.Product.parquet',\n        FORMAT = 'PARQUET'\n    ) AS [result]\n\n-- EXAMPLE: We can specify our own column names and data types\n-- NOTICE this will read only the column name we specify (improve performance)\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://datalakecnweslbo.dfs.core.windows.net/landing/adventureworkslt/tables/parquet/SalesLT.Product.parquet',\n        FORMAT = 'PARQUET'\n    )\n    WITH \n    (\n        ProductID INT,\n        Name NVARCHAR(50),\n        ProductNumber NVARCHAR(25),\n        Color NVARCHAR(15)\n    ) AS [result]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/04-Access external data in a database')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/02-Serverless SQL/02-Demo-Query files using a serverless SQL pool"
				},
				"content": {
					"query": "-- Demo: Query files using a serverless SQL pool\n-- Task: Access external data in a database\n\nCREATE DATABASE Sales\nCOLLATE Latin1_General_100_BIN2_UTF8; \nGO;\n\nUse Sales\n\nCREATE EXTERNAL DATA SOURCE sales_data WITH (\n    LOCATION = 'https://datalakeweslbo.dfs.core.windows.net/files/labs-dp-203/01/data/'\n);\nGO;\n\n-- The following query uses the external data source to connect to the data lake, and the OPENROWSET function \n-- now only need to reference the relative path to the .csv files.\n\nSELECT *\nFROM\n    OPENROWSET(\n        BULK '*.csv',\n        DATA_SOURCE = 'sales_data',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0'\n    ) AS orders\n\n\n-- Query the parquet files using the data source.\n\nSELECT *\nFROM  \n    OPENROWSET(\n        BULK '*.snappy.parquet',\n        DATA_SOURCE = 'sales_data',\n        FORMAT='PARQUET'\n    ) AS orders\nWHERE replace(orders.filepath(1), '.snappy', '') = '2019'\n\n-- The external data source makes it easier to access the files in the data lake, but most data analysts using SQL \n-- are used to working with tables in a database. Fortunately, you can also define external file formats and external\n-- tables that encapsulate rowsets from files in database tables.\n\nCREATE EXTERNAL FILE FORMAT CsvFormat\n    WITH (\n        FORMAT_TYPE = DELIMITEDTEXT,\n        FORMAT_OPTIONS(\n        FIELD_TERMINATOR = ',',\n        STRING_DELIMITER = '\"'\n        )\n    );\nGO;\n\nCREATE EXTERNAL TABLE dbo.orders\n(\n    SalesOrderNumber VARCHAR(10),\n    SalesOrderLineNumber INT,\n    OrderDate DATE,\n    CustomerName VARCHAR(25),\n    EmailAddress VARCHAR(50),\n    Item VARCHAR(30),\n    Quantity INT,\n    UnitPrice DECIMAL(18,2),\n    TaxAmount DECIMAL (18,2)\n)\nWITH\n(\n    DATA_SOURCE =sales_data,\n    LOCATION = '*.csv',\n    FILE_FORMAT = CsvFormat\n);\nGO\n\n-- Now that you’ve explored various ways to query files in the data lake by using SQL queries, you can analyze \n-- the results of these queries to gain insights into the data. Often, insights are easier to uncover by visualizing \n-- the query results in a chart; which you can easily do by using the integrated charting functionality in the Synapse Studio query editor.\n\nSELECT YEAR(OrderDate) AS OrderYear,\n    SUM((UnitPrice * Quantity) + TaxAmount) AS GrossRevenue\nFROM dbo.orders\nGROUP BY YEAR(OrderDate)\nORDER BY OrderYear;\n\n-- Cleanup\nDROP EXTERNAL TABLE dbo.orders\nDROP EXTERNAL TABLE dbo.ProductSalesTotals\nDROP EXTERNAL TABLE dbo.YearlySalesTotals\nDROP EXTERNAL FILE FORMAT ParquetFormat\nDROp EXTERNAL FILE FORMAT CsvFormat\nDROP EXTERNAL DATA SOURCE sales_data\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "Sales",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/04_sql_serverless_views')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-500/03-Synapse"
				},
				"content": {
					"query": "-- Do this only once!\nCREATE DATABASE AdventureWorksLTServerless\n\n-- Change database\nUSE AdventureWorksLTServerless\nGO\n\n-- Create a separate schema (optional)\nCREATE SCHEMA SalesLT\nGO\n\n-- Create views for every tables, based on PARQUET format\nCREATE VIEW SalesLT.Address AS SELECT * FROM OPENROWSET(BULK 'https://datalakeweslbo.dfs.core.windows.net/landing/adventureworkslt/tables/parquet/SalesLT.Address.parquet', FORMAT = 'PARQUET') AS [result]\nGO\nCREATE VIEW SalesLT.Customer AS SELECT * FROM OPENROWSET(BULK 'https://datalakeweslbo.dfs.core.windows.net/landing/adventureworkslt/tables/parquet/SalesLT.Customer.parquet', FORMAT = 'PARQUET') AS [result]\nGO\nCREATE VIEW SalesLT.CustomerAddress AS SELECT * FROM OPENROWSET(BULK 'https://datalakeweslbo.dfs.core.windows.net/landing/adventureworkslt/tables/parquet/SalesLT.CustomerAddress.parquet', FORMAT = 'PARQUET') AS [result]\nGO\nCREATE VIEW SalesLT.Product AS SELECT * FROM OPENROWSET(BULK 'https://datalakeweslbo.dfs.core.windows.net/landing/adventureworkslt/tables/parquet/SalesLT.Product.parquet', FORMAT = 'PARQUET') AS [result]\nGO\nCREATE VIEW SalesLT.ProductCategory AS SELECT * FROM OPENROWSET(BULK 'https://datalakeweslbo.dfs.core.windows.net/landing/adventureworkslt/tables/parquet/SalesLT.ProductCategory.parquet', FORMAT = 'PARQUET') AS [result]\nGO\nCREATE VIEW SalesLT.ProductDescription AS SELECT * FROM OPENROWSET(BULK 'https://datalakeweslbo.dfs.core.windows.net/landing/adventureworkslt/tables/parquet/SalesLT.ProductDescription.parquet', FORMAT = 'PARQUET') AS [result]\nGO\nCREATE VIEW SalesLT.ProductModel AS SELECT * FROM OPENROWSET(BULK 'https://datalakeweslbo.dfs.core.windows.net/landing/adventureworkslt/tables/parquet/SalesLT.ProductModel.parquet', FORMAT = 'PARQUET') AS [result]\nGO\nCREATE VIEW SalesLT.ProductModelProductDescription AS SELECT * FROM OPENROWSET(BULK 'https://datalakeweslbo.dfs.core.windows.net/landing/adventureworkslt/tables/parquet/SalesLT.ProductModelProductDescription.parquet', FORMAT = 'PARQUET') AS [result]\nGO\nCREATE VIEW SalesLT.SalesOrderDetail AS SELECT * FROM OPENROWSET(BULK 'https://datalakeweslbo.dfs.core.windows.net/landing/adventureworkslt/tables/parquet/SalesLT.SalesOrderDetail.parquet', FORMAT = 'PARQUET') AS [result]\nGO\nCREATE VIEW SalesLT.SalesOrderHeader AS SELECT * FROM OPENROWSET(BULK 'https://datalakeweslbo.dfs.core.windows.net/landing/adventureworkslt/tables/parquet/SalesLT.SalesOrderHeader.parquet', FORMAT = 'PARQUET') AS [result]\nGO\n\n-- PLAYGROUND: Confirm data can be retrieved\nSELECT TOP 100 * FROM SalesLT.Address\nSELECT TOP 100 * FROM SalesLT.Customer\nSELECT TOP 100 * FROM SalesLT.CustomerAddress\nSELECT TOP 100 * FROM SalesLT.Product\nSELECT TOP 100 * FROM SalesLT.ProductCategory\nSELECT TOP 100 * FROM SalesLT.ProductDescription\nSELECT TOP 100 * FROM SalesLT.ProductModel\nSELECT TOP 100 * FROM SalesLT.ProductModelProductDescription\nSELECT TOP 100 * FROM SalesLT.SalesOrderDetail\nSELECT TOP 100 * FROM SalesLT.SalesOrderHeader\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/05_sql_serverless_external_tables')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-500/03-Synapse"
				},
				"content": {
					"query": "-- Do this only once!\nCREATE DATABASE AdventureWorksLTServerless\n\n-- Change database\nUSE AdventureWorksLTServerless\nGO\n\n-- Create a separate schema (optional)\nCREATE SCHEMA Ext\nGO\n\n-- Create external file format\nIF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\n-- Create external data source, pointing to the data lake\n-- Remember to change datalake url\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'DataLakeLandingZone') \n\tCREATE EXTERNAL DATA SOURCE [DataLakeLandingZone] \n\tWITH (\n\t\tLOCATION = 'abfss://landing@datalakeweslbo.dfs.core.windows.net' \n\t)\nGO\n\n-- Note: updated the data types to match the orginal (instead of relying on nvarchar(4000) all the time)\nCREATE EXTERNAL TABLE [Ext].[Address] (\n\t[AddressID] int,\n\t[AddressLine1] nvarchar(60),\n\t[AddressLine2] nvarchar(60),\n\t[City] nvarchar(30),\n\t[StateProvince] nvarchar(50),\n\t[CountryRegion] nvarchar(50),\n\t[PostalCode] nvarchar(15),\n\t[rowguid] UNIQUEIDENTIFIER,\n\t[ModifiedDate] datetime2(7)\n\t)\n\tWITH (\n\tLOCATION = 'adventureworkslt/tables/parquet/SalesLT.Address.parquet',\n\tDATA_SOURCE = [DataLakeLandingZone],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\nCREATE EXTERNAL TABLE [Ext].[Customer] (\n\t[CustomerID] [int],\n\t[NameStyle] bit,\n\t[Title] [nvarchar](8),\n\t[FirstName] NVARCHAR(50),\n\t[MiddleName] nvarchar(50),\n\t[LastName] nvarchar(50),\n\t[Suffix] [nvarchar](10),\n\t[CompanyName] [nvarchar](128),\n\t[SalesPerson] [nvarchar](256),\n\t[EmailAddress] [nvarchar](50),\n\t[Phone] nvarchar(25),\n\t[PasswordHash] [varchar](128),\n\t[PasswordSalt] [varchar](10),\n\t[rowguid] [uniqueidentifier],\n\t[ModifiedDate] [datetime2](7)\n\t)\n\tWITH (\n\tLOCATION = 'adventureworkslt/tables/parquet/SalesLT.Customer.parquet',\n\tDATA_SOURCE = [DataLakeLandingZone],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\nCREATE EXTERNAL TABLE [Ext].[CustomerAddress] (\n\t[CustomerID] [int],\n\t[AddressID] [int],\n\t[AddressType] nvarchar(50),\n\t[rowguid] [uniqueidentifier],\n\t[ModifiedDate] [datetime2](7)\n\t)\n\tWITH (\n\tLOCATION = 'adventureworkslt/tables/parquet/SalesLT.CustomerAddress.parquet',\n\tDATA_SOURCE = [DataLakeLandingZone],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\nCREATE EXTERNAL TABLE [Ext].[Product] (\n\t[ProductID] [int],\n\t[Name] nvarchar(50),\n\t[ProductNumber] [nvarchar](25),\n\t[Color] [nvarchar](15),\n\t[StandardCost] [money],\n\t[ListPrice] [money],\n\t[Size] [nvarchar](5),\n\t[Weight] [decimal](8, 2),\n\t[ProductCategoryID] [int],\n\t[ProductModelID] [int],\n\t[SellStartDate] [datetime2](7),\n\t[SellEndDate] [datetime2](7),\n\t[DiscontinuedDate] [datetime2](7),\n\t[ThumbNailPhoto] [varbinary](max),\n\t[ThumbnailPhotoFileName] [nvarchar](50),\n\t[rowguid] [uniqueidentifier],\n\t[ModifiedDate] [datetime2](7)\n\t)\n\tWITH (\n\tLOCATION = 'adventureworkslt/tables/parquet/SalesLT.Product.parquet',\n\tDATA_SOURCE = [DataLakeLandingZone],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\nCREATE EXTERNAL TABLE [Ext].[ProductCategory] (\n\t[ProductCategoryID] [int],\n\t[ParentProductCategoryID] [int],\n\t[Name] nvarchar(50),\n\t[rowguid] [uniqueidentifier],\n\t[ModifiedDate] [datetime2](7)\n\t)\n\tWITH (\n\tLOCATION = 'adventureworkslt/tables/parquet/SalesLT.ProductCategory.parquet',\n\tDATA_SOURCE = [DataLakeLandingZone],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\nCREATE EXTERNAL TABLE [Ext].[ProductDescription] (\n\t[ProductDescriptionID] [int],\n\t[Description] [nvarchar](400),\n\t[rowguid] [uniqueidentifier],\n\t[ModifiedDate] [datetime2](7)\n\t)\n\tWITH (\n\tLOCATION = 'adventureworkslt/tables/parquet/SalesLT.ProductDescription.parquet',\n\tDATA_SOURCE = [DataLakeLandingZone],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n-- Note xml data type is not supported (column CatalogDescription)\nCREATE EXTERNAL TABLE [Ext].[ProductModel] (\n\t[ProductModelID] [int],\n\t[Name] nvarchar(50),\n\t[CatalogDescription] nvarchar(4000),\n\t[rowguid] [uniqueidentifier],\n\t[ModifiedDate] [datetime2](7)\n\t)\n\tWITH (\n\tLOCATION = 'adventureworkslt/tables/parquet/SalesLT.ProductModel.parquet',\n\tDATA_SOURCE = [DataLakeLandingZone],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\nCREATE EXTERNAL TABLE [Ext].[ProductModelProductDescription] (\n\t[ProductModelID] [int],\n\t[ProductDescriptionID] [int],\n\t[Culture] [nchar](6),\n\t[rowguid] [uniqueidentifier],\n\t[ModifiedDate] [datetime2](7)\n\t)\n\tWITH (\n\tLOCATION = 'adventureworkslt/tables/parquet/SalesLT.ProductModelProductDescription.parquet',\n\tDATA_SOURCE = [DataLakeLandingZone],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n-- Notice linetotal is a calculated field (isnull(([UnitPrice]*((1.0)-[UnitPriceDiscount]))*[OrderQty],(0.0)))\n-- Replaced with numeric(38,6)\nCREATE EXTERNAL TABLE [Ext].[SalesOrderDetail] (\n\t[SalesOrderID] [int],\n\t[SalesOrderDetailID] [int],\n\t[OrderQty] [smallint],\n\t[ProductID] [int],\n\t[UnitPrice] [money],\n\t[UnitPriceDiscount] [money],\n\t[LineTotal] numeric(38,6),\n\t[rowguid] [uniqueidentifier],\n\t[ModifiedDate] [datetime2](7)\n\n\t)\n\tWITH (\n\tLOCATION = 'adventureworkslt/tables/parquet/SalesLT.SalesOrderDetail.parquet',\n\tDATA_SOURCE = [DataLakeLandingZone],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\nCREATE EXTERNAL TABLE [Ext].[SalesOrderHeader] (\n\t[SalesOrderID] [int],\n\t[RevisionNumber] [tinyint],\n\t[OrderDate] [datetime2](7),\n\t[DueDate] [datetime2](7),\n\t[ShipDate] [datetime2](7),\n\t[Status] [tinyint],\n\t[OnlineOrderFlag] bit ,\n\t[SalesOrderNumber]  nvarchar(23),\n\t[PurchaseOrderNumber] nvarchar(25),\n\t[AccountNumber] nvarchar(15),\n\t[CustomerID] [int],\n\t[ShipToAddressID] [int],\n\t[BillToAddressID] [int],\n\t[ShipMethod] [nvarchar](50),\n\t[CreditCardApprovalCode] [varchar](15),\n\t[SubTotal] [money],\n\t[TaxAmt] [money],\n\t[Freight] [money],\n\t[TotalDue] numeric(19,4),\n\t[Comment] [nvarchar](400),\n\t[rowguid] [uniqueidentifier],\n\t[ModifiedDate] [datetime2](7) \n\t)\n\tWITH (\n\tLOCATION = 'adventureworkslt/tables/parquet/SalesLT.SalesOrderHeader.parquet',\n\tDATA_SOURCE = [DataLakeLandingZone],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM [Ext].[Address]\nSELECT TOP 100 * FROM [Ext].[Customer]\nSELECT TOP 100 * FROM [Ext].[CustomerAddress]\nSELECT TOP 100 * FROM [Ext].[Product]\nSELECT TOP 100 * FROM [Ext].[ProductCategory]\nSELECT TOP 100 * FROM [Ext].[ProductDescription]\nSELECT TOP 100 * FROM [Ext].[ProductModel]\nSELECT TOP 100 * FROM [Ext].[ProductModelProductDescription]\nSELECT TOP 100 * FROM [Ext].[SalesOrderDetail]\nSELECT TOP 100 * FROM [Ext].[SalesOrderHeader]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/06_sql_dedicated_create_tables')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-500/03-Synapse"
				},
				"content": {
					"query": "-- !! Make sure to run this against the DP500DWH database!\n\n\nSET ANSI_NULLS ON\nGO\nSET QUOTED_IDENTIFIER ON\nGO\nCREATE TABLE [dbo].[FactInternetSales](\n\t[SalesOrderNumber] [nvarchar](20) NOT NULL,\n\t[SalesOrderLineNumber] [tinyint] NOT NULL,\n\t[CustomerKey] [int] NOT NULL,\n\t[ProductKey] [int] NOT NULL,\n\t[OrderDateKey] [int] NOT NULL,\n\t[DueDateKey] [int] NOT NULL,\n\t[ShipDateKey] [int] NULL,\n\t[PromotionKey] [int] NOT NULL,\n\t[CurrencyKey] [int] NOT NULL,\n\t[SalesTerritoryKey] [int] NOT NULL,\n\t[OrderQuantity] [smallint] NOT NULL,\n\t[UnitPrice] [money] NOT NULL,\n\t[ExtendedAmount] [money] NOT NULL,\n\t[UnitPriceDiscountPct] [decimal](7, 4) NOT NULL,\n\t[DiscountAmount] [float] NOT NULL,\n\t[ProductStandardCost] [money] NOT NULL,\n\t[TotalProductCost] [money] NOT NULL,\n\t[SalesAmount] [money] NOT NULL,\n\t[TaxAmount] [money] NOT NULL,\n\t[FreightAmount] [money] NOT NULL,\n\t[CarrierTrackingNumber] [nvarchar](25) NULL,\n\t[CustomerPONumber] [nvarchar](25) NULL,\n\t[RevisionNumber] [tinyint] NOT NULL\n)\nWITH\n(\n\tDISTRIBUTION = HASH(SalesOrderNumber),\n\tCLUSTERED COLUMNSTORE INDEX\n)\n\nGO\nCREATE TABLE [dbo].[DimCustomer](\n\t[CustomerKey] [int] IDENTITY(1,1) NOT NULL,\n\t[GeographyKey] [int] NULL,\n\t[CustomerAlternateKey] [nvarchar](15) NOT NULL,\n\t[Title] [nvarchar](8) NULL,\n\t[FirstName] [nvarchar](50) NULL,\n\t[MiddleName] [nvarchar](50) NULL,\n\t[LastName] [nvarchar](50) NULL,\n\t[NameStyle] [bit] NULL,\n\t[BirthDate] [date] NULL,\n\t[MaritalStatus] [nchar](1) NULL,\n\t[Suffix] [nvarchar](10) NULL,\n\t[Gender] [nvarchar](1) NULL,\n\t[EmailAddress] [nvarchar](50) NULL,\n\t[YearlyIncome] [money] NULL,\n\t[TotalChildren] [tinyint] NULL,\n\t[NumberChildrenAtHome] [tinyint] NULL,\n\t[EnglishEducation] [nvarchar](40) NULL,\n\t[SpanishEducation] [nvarchar](40) NULL,\n\t[FrenchEducation] [nvarchar](40) NULL,\n\t[EnglishOccupation] [nvarchar](100) NULL,\n\t[SpanishOccupation] [nvarchar](100) NULL,\n\t[FrenchOccupation] [nvarchar](100) NULL,\n\t[HouseOwnerFlag] [nchar](1) NULL,\n\t[NumberCarsOwned] [tinyint] NULL,\n\t[AddressLine1] [nvarchar](120) NULL,\n\t[AddressLine2] [nvarchar](120) NULL,\n\t[Phone] [nvarchar](20) NULL,\n\t[DateFirstPurchase] [date] NULL,\n\t[CommuteDistance] [nvarchar](15) NULL\n)\nWITH\n(\n\tDISTRIBUTION = REPLICATE,\n\tCLUSTERED COLUMNSTORE INDEX\n)\n\nGO\nCREATE TABLE [dbo].[DimDate](\n\t[DateKey] [int] NOT NULL,\n\t[FullDateAlternateKey] [date] NOT NULL,\n\t[DayNumberOfWeek] [tinyint] NOT NULL,\n\t[EnglishDayNameOfWeek] [nvarchar](10) NOT NULL,\n\t[SpanishDayNameOfWeek] [nvarchar](10) NOT NULL,\n\t[FrenchDayNameOfWeek] [nvarchar](10) NOT NULL,\n\t[DayNumberOfMonth] [tinyint] NOT NULL,\n\t[DayNumberOfYear] [smallint] NOT NULL,\n\t[WeekNumberOfYear] [tinyint] NOT NULL,\n\t[EnglishMonthName] [nvarchar](10) NOT NULL,\n\t[SpanishMonthName] [nvarchar](10) NOT NULL,\n\t[FrenchMonthName] [nvarchar](10) NOT NULL,\n\t[MonthNumberOfYear] [tinyint] NOT NULL,\n\t[CalendarQuarter] [tinyint] NOT NULL,\n\t[CalendarYear] [smallint] NOT NULL,\n\t[CalendarSemester] [tinyint] NOT NULL,\n\t[FiscalQuarter] [tinyint] NOT NULL,\n\t[FiscalYear] [smallint] NOT NULL,\n\t[FiscalSemester] [tinyint] NOT NULL\n)\nWITH\n(\n\tDISTRIBUTION = REPLICATE,\n\tCLUSTERED COLUMNSTORE INDEX\n)\n\nGO\nCREATE TABLE [dbo].[DimGeography](\n\t[GeographyKey] [int] IDENTITY(1,1) NOT NULL,\n\t[City] [nvarchar](30) NULL,\n\t[StateProvinceCode] [nvarchar](3) NULL,\n\t[StateProvinceName] [nvarchar](50) NULL,\n\t[CountryRegionCode] [nvarchar](3) NULL,\n\t[EnglishCountryRegionName] [nvarchar](50) NULL,\n\t[SpanishCountryRegionName] [nvarchar](50) NULL,\n\t[FrenchCountryRegionName] [nvarchar](50) NULL,\n\t[PostalCode] [nvarchar](15) NULL,\n\t[SalesTerritoryKey] [int] NULL,\n\t[IpAddressLocator] [nvarchar](15) NULL\n)\nWITH\n(\n\tDISTRIBUTION = REPLICATE,\n\tCLUSTERED COLUMNSTORE INDEX\n)\n\nGO\nCREATE TABLE [dbo].[DimProduct](\n\t[ProductKey] [int] IDENTITY(1,1) NOT NULL,\n\t[ProductAlternateKey] [nvarchar](25) NULL,\n\t[ProductSubcategoryKey] [int] NULL,\n\t[WeightUnitMeasureCode] [nchar](3) NULL,\n\t[SizeUnitMeasureCode] [nchar](3) NULL,\n\t[EnglishProductName] [nvarchar](50) NOT NULL,\n\t[SpanishProductName] [nvarchar](50) NOT NULL,\n\t[FrenchProductName] [nvarchar](50) NOT NULL,\n\t[StandardCost] [money] NULL,\n\t[FinishedGoodsFlag] [bit] NOT NULL,\n\t[Color] [nvarchar](15) NOT NULL,\n\t[SafetyStockLevel] [smallint] NULL,\n\t[ReorderPoint] [smallint] NULL,\n\t[ListPrice] [money] NULL,\n\t[Size] [nvarchar](50) NULL,\n\t[SizeRange] [nvarchar](50) NULL,\n\t[Weight] [float] NULL,\n\t[DaysToManufacture] [int] NULL,\n\t[ProductLine] [nchar](2) NULL,\n\t[DealerPrice] [money] NULL,\n\t[Class] [nchar](2) NULL,\n\t[Style] [nchar](2) NULL,\n\t[ModelName] [nvarchar](50) NULL,\n\t[LargePhoto] [varbinary](max) NULL,\n\t[EnglishDescription] [nvarchar](400) NULL,\n\t[FrenchDescription] [nvarchar](400) NULL,\n\t[ChineseDescription] [nvarchar](400) NULL,\n\t[ArabicDescription] [nvarchar](400) NULL,\n\t[HebrewDescription] [nvarchar](400) NULL,\n\t[ThaiDescription] [nvarchar](400) NULL,\n\t[GermanDescription] [nvarchar](400) NULL,\n\t[JapaneseDescription] [nvarchar](400) NULL,\n\t[TurkishDescription] [nvarchar](400) NULL,\n\t[StartDate] [datetime] NULL,\n\t[EndDate] [datetime] NULL,\n\t[Status] [nvarchar](7) NULL\n)\nWITH  \n  (   \n\t  DISTRIBUTION = REPLICATE,\n    CLUSTERED INDEX (ProductKey)  \n); \nGO\n\nCREATE TABLE [dbo].[DimProductCategory](\n\t[ProductCategoryKey] [int] IDENTITY(1,1) NOT NULL,\n\t[ProductCategoryAlternateKey] [int] NULL,\n\t[EnglishProductCategoryName] [nvarchar](50) NOT NULL,\n\t[SpanishProductCategoryName] [nvarchar](50) NOT NULL,\n\t[FrenchProductCategoryName] [nvarchar](50) NOT NULL\n)\nWITH\n(\n\tDISTRIBUTION = REPLICATE,\n\tCLUSTERED COLUMNSTORE INDEX\n)\n\nGO\nCREATE TABLE [dbo].[DimProductSubcategory](\n\t[ProductSubcategoryKey] [int] IDENTITY(1,1) NOT NULL,\n\t[ProductSubcategoryAlternateKey] [int] NULL,\n\t[EnglishProductSubcategoryName] [nvarchar](50) NOT NULL,\n\t[SpanishProductSubcategoryName] [nvarchar](50) NOT NULL,\n\t[FrenchProductSubcategoryName] [nvarchar](50) NOT NULL,\n\t[ProductCategoryKey] [int] NULL\n)\nWITH\n(\n\tDISTRIBUTION = REPLICATE,\n\tCLUSTERED COLUMNSTORE INDEX\n)\nGO\n\nCREATE TABLE [dbo].[DimSalesTerritory](\n\t[SalesTerritoryKey] [int] IDENTITY(1,1) NOT NULL,\n\t[SalesTerritoryAlternateKey] [int] NULL,\n\t[SalesTerritoryRegion] [nvarchar](50) NOT NULL,\n\t[SalesTerritoryCountry] [nvarchar](50) NOT NULL,\n\t[SalesTerritoryGroup] [nvarchar](50) NULL,\n\t[SalesTerritoryImage] [varbinary](max) NULL)\nWITH  \n(  \n\tDISTRIBUTION = REPLICATE,\n\tCLUSTERED INDEX (SalesTerritoryKey)  \n)\nGO\n\nCREATE TABLE [dbo].[FactResellerSales](\n\t[SalesOrderNumber] [nvarchar](20) NOT NULL,\n\t[SalesOrderLineNumber] [tinyint] NOT NULL,\n\t[ResellerKey] [int] NOT NULL,\n\t[ProductKey] [int] NOT NULL,\n\t[OrderDateKey] [int] NOT NULL,\n\t[DueDateKey] [int] NOT NULL,\n\t[ShipDateKey] [int] NULL,\n\t[EmployeeKey] [int] NOT NULL,\n\t[PromotionKey] [int] NOT NULL,\n\t[CurrencyKey] [int] NOT NULL,\n\t[SalesTerritoryKey] [int] NOT NULL,\n\t[OrderQuantity] [smallint] NOT NULL,\n\t[UnitPrice] [money] NOT NULL,\n\t[ExtendedAmount] [money] NOT NULL,\n\t[UnitPriceDiscountPct] [decimal](7, 4) NOT NULL,\n\t[DiscountAmount] [money] NOT NULL,\n\t[ProductStandardCost] [money] NOT NULL,\n\t[TotalProductCost] [money] NOT NULL,\n\t[SalesAmount] [money] NOT NULL,\n\t[TaxAmount] [money] NOT NULL,\n\t[FreightAmount] [money] NOT NULL,\n\t[CarrierTrackingNumber] [nvarchar](25) NULL,\n\t[CustomerPONumber] [nvarchar](25) NULL,\n\t[RevisionNumber] [tinyint] NOT NULL\n)\nWITH\n(\n    DISTRIBUTION = HASH(SalesOrderNumber),\n    CLUSTERED COLUMNSTORE INDEX\n);\nGO\n\nCREATE VIEW [dbo].[vFactSales]\nAS\n\tSELECT\n\t\tCAST(N'Reseller' AS NVARCHAR(10)) AS [Channel]\n\t\t,CAST(RIGHT([SalesOrderNumber], (LEN([SalesOrderNumber]) - 2)) AS INT) AS [SalesOrderKey]\n\t\t,((CAST(RIGHT([SalesOrderNumber], (LEN([SalesOrderNumber]) - 2)) AS INT) * 1000) + [SalesOrderLineNumber]) AS [SalesOrderLineKey]\n\t\t,[SalesOrderNumber]\n\t\t,[SalesOrderLineNumber]\n\t\t,[ResellerKey]\n\t\t,CAST(-1 AS INT) AS [CustomerKey]\n\t\t,[ProductKey]\n\t\t,[OrderDateKey]\n\t\t,[DueDateKey]\n\t\t,[ShipDateKey]\n\t\t,[PromotionKey]\n\t\t,[CurrencyKey]\n\t\t,[SalesTerritoryKey]\n\t\t,[EmployeeKey]\n\t\t,[OrderQuantity]\n\t\t,[UnitPrice]\n\t\t,[ExtendedAmount]\n\t\t,[UnitPriceDiscountPct]\n\t\t,[DiscountAmount]\n\t\t,[ProductStandardCost]\n\t\t,[TotalProductCost]\n\t\t,[SalesAmount]\n\t\t,[TaxAmount]\n\t\t,[FreightAmount]\n\t\t,[CarrierTrackingNumber]\n\t\t,[CustomerPONumber]\n\t\t,[RevisionNumber]\n\tFROM\n\t\t[dbo].[FactResellerSales]\n\tUNION ALL\n\tSELECT\n\t\tCAST(N'Internet' AS NVARCHAR(10)) AS [Channel]\n\t\t,CAST(RIGHT([SalesOrderNumber], (LEN([SalesOrderNumber]) - 2)) AS INT) AS [SalesOrderKey]\n\t\t,((CAST(RIGHT([SalesOrderNumber], (LEN([SalesOrderNumber]) - 2)) AS INT) * 1000) + [SalesOrderLineNumber]) AS [SalesOrderLineKey]\n\t\t,[SalesOrderNumber]\n\t\t,[SalesOrderLineNumber]\n\t\t,CAST(-1 AS INT) AS [ResellerKey]\n\t\t,[CustomerKey]\n\t\t,[ProductKey]\n\t\t,[OrderDateKey]\n\t\t,[DueDateKey]\n\t\t,[ShipDateKey]\n\t\t,[PromotionKey]\n\t\t,[CurrencyKey]\n\t\t,[SalesTerritoryKey]\n\t\t,CAST(-1 AS INT) AS [EmployeeKey]\n\t\t,[OrderQuantity]\n\t\t,[UnitPrice]\n\t\t,[ExtendedAmount]\n\t\t,[UnitPriceDiscountPct]\n\t\t,[DiscountAmount]\n\t\t,[ProductStandardCost]\n\t\t,[TotalProductCost]\n\t\t,[SalesAmount]\n\t\t,[TaxAmount]\n\t\t,[FreightAmount]\n\t\t,[CarrierTrackingNumber]\n\t\t,[CustomerPONumber]\n\t\t,[RevisionNumber]\n\tFROM\n\t\t[dbo].[FactInternetSales];\nGO\n\n\nCREATE TABLE [dbo].[DimAccount](\n\t[AccountKey] [int] IDENTITY(1,1) NOT NULL,\n\t[ParentAccountKey] [int] NULL,\n\t[AccountCodeAlternateKey] [int] NULL,\n\t[ParentAccountCodeAlternateKey] [int] NULL,\n\t[AccountDescription] [nvarchar](50) NULL,\n\t[AccountType] [nvarchar](50) NULL,\n\t[Operator] [nvarchar](50) NULL,\n\t[CustomMembers] [nvarchar](300) NULL,\n\t[ValueType] [nvarchar](50) NULL,\n\t[CustomMemberOptions] [nvarchar](200) NULL\n)\nWITH\n(\n    DISTRIBUTION = REPLICATE,\n    CLUSTERED COLUMNSTORE INDEX\n)\nGO\n\nCREATE TABLE [dbo].[DimCurrency](\n\t[CurrencyKey] [int] IDENTITY(1,1) NOT NULL,\n\t[CurrencyAlternateKey] [nchar](3) NOT NULL,\n\t[CurrencyName] [nvarchar](50) NOT NULL,\n\t[FormatString] [nvarchar](20) NULL\n)\nWITH\n(\n    DISTRIBUTION = REPLICATE,\n    CLUSTERED COLUMNSTORE INDEX\n)\nGO\n\nCREATE TABLE [dbo].[DimDepartmentGroup](\n\t[DepartmentGroupKey] [int] IDENTITY(1,1) NOT NULL,\n\t[ParentDepartmentGroupKey] [int] NULL,\n\t[DepartmentGroupName] [nvarchar](50) NULL\n)\nWITH\n(\n    DISTRIBUTION = REPLICATE,\n    CLUSTERED COLUMNSTORE INDEX\n)\nGO\n\nCREATE TABLE [dbo].[DimEmployee](\n\t[EmployeeKey] [int] IDENTITY(1,1) NOT NULL,\n\t[ParentEmployeeKey] [int] NULL,\n\t[EmployeeNationalIDAlternateKey] [nvarchar](15) NULL,\n\t[ParentEmployeeNationalIDAlternateKey] [nvarchar](15) NULL,\n\t[SalesTerritoryKey] [int] NULL,\n\t[FirstName] [nvarchar](50) NOT NULL,\n\t[LastName] [nvarchar](50) NOT NULL,\n\t[MiddleName] [nvarchar](50) NULL,\n\t[NameStyle] [bit] NOT NULL,\n\t[Title] [nvarchar](50) NULL,\n\t[HireDate] [date] NULL,\n\t[BirthDate] [date] NULL,\n\t[LoginID] [nvarchar](256) NULL,\n\t[EmailAddress] [nvarchar](50) NULL,\n\t[Phone] [nvarchar](25) NULL,\n\t[MaritalStatus] [nchar](1) NULL,\n\t[EmergencyContactName] [nvarchar](50) NULL,\n\t[EmergencyContactPhone] [nvarchar](25) NULL,\n\t[SalariedFlag] [bit] NULL,\n\t[Gender] [nchar](1) NULL,\n\t[PayFrequency] [tinyint] NULL,\n\t[BaseRate] [money] NULL,\n\t[VacationHours] [smallint] NULL,\n\t[SickLeaveHours] [smallint] NULL,\n\t[CurrentFlag] [bit] NOT NULL,\n\t[SalespersonFlag] [bit] NOT NULL,\n\t[DepartmentName] [nvarchar](50) NULL,\n\t[StartDate] [date] NULL,\n\t[EndDate] [date] NULL,\n\t[Status] [nvarchar](50) NULL,\n\t[EmployeePhoto] [varbinary](max) NULL)\nWITH  \n(   \n\tDISTRIBUTION = REPLICATE,\n\tCLUSTERED INDEX (EmployeeKey)  \n) \nGO\n\nCREATE TABLE [dbo].[DimOrganization](\n\t[OrganizationKey] [int] IDENTITY(1,1) NOT NULL,\n\t[ParentOrganizationKey] [int] NULL,\n\t[PercentageOfOwnership] [nvarchar](16) NULL,\n\t[OrganizationName] [nvarchar](50) NULL,\n\t[CurrencyKey] [int] NULL\n)\t\nWITH\n(\n    DISTRIBUTION = REPLICATE,\n    CLUSTERED COLUMNSTORE INDEX\n)\nGO\n\n\nCREATE TABLE [dbo].[DimPromotion](\n\t[PromotionKey] [int] IDENTITY(1,1) NOT NULL,\n\t[PromotionAlternateKey] [int] NULL,\n\t[EnglishPromotionName] [nvarchar](255) NULL,\n\t[SpanishPromotionName] [nvarchar](255) NULL,\n\t[FrenchPromotionName] [nvarchar](255) NULL,\n\t[DiscountPct] [float] NULL,\n\t[EnglishPromotionType] [nvarchar](50) NULL,\n\t[SpanishPromotionType] [nvarchar](50) NULL,\n\t[FrenchPromotionType] [nvarchar](50) NULL,\n\t[EnglishPromotionCategory] [nvarchar](50) NULL,\n\t[SpanishPromotionCategory] [nvarchar](50) NULL,\n\t[FrenchPromotionCategory] [nvarchar](50) NULL,\n\t[StartDate] [datetime] NOT NULL,\n\t[EndDate] [datetime] NULL,\n\t[MinQty] [int] NULL,\n\t[MaxQty] [int] NULL\n)\nWITH\n(\n    DISTRIBUTION = REPLICATE,\n    CLUSTERED COLUMNSTORE INDEX\n)\nGO\n\nCREATE TABLE [dbo].[DimReseller](\n\t[ResellerKey] [int] IDENTITY(1,1) NOT NULL,\n\t[GeographyKey] [int] NULL,\n\t[ResellerAlternateKey] [nvarchar](15) NULL,\n\t[Phone] [nvarchar](25) NULL,\n\t[BusinessType] [varchar](20) NOT NULL,\n\t[ResellerName] [nvarchar](50) NOT NULL,\n\t[NumberEmployees] [int] NULL,\n\t[OrderFrequency] [char](1) NULL,\n\t[OrderMonth] [tinyint] NULL,\n\t[FirstOrderYear] [int] NULL,\n\t[LastOrderYear] [int] NULL,\n\t[ProductLine] [nvarchar](50) NULL,\n\t[AddressLine1] [nvarchar](60) NULL,\n\t[AddressLine2] [nvarchar](60) NULL,\n\t[AnnualSales] [money] NULL,\n\t[BankName] [nvarchar](50) NULL,\n\t[MinPaymentType] [tinyint] NULL,\n\t[MinPaymentAmount] [money] NULL,\n\t[AnnualRevenue] [money] NULL,\n\t[YearOpened] [int] NULL\n)\nWITH\n(\n    DISTRIBUTION = REPLICATE,\n    CLUSTERED COLUMNSTORE INDEX\n)\nGO\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/07-Lab-Query a delta table from a serverless SQL pool')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Query a delta table from a serverless SQL pool",
				"folder": {
					"name": "dp-203/03-Spark"
				},
				"content": {
					"query": "-- This demonstrates how you can use a serverless SQL pool to query delta format files that were created using Spark, \n-- and use the results for reporting or analysis.\n\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://datalakeweslbo.dfs.core.windows.net/files/labs-dp-203/07/delta/products-delta/',\n        FORMAT = 'DELTA'\n    ) AS [result]\n\n\n-- Observe that you can also use the serverless SQL pool to query Delta Lake data in catalog \n-- tables that are defined the Spark metastore.\n\nUSE AdventureWorks;\n\nSELECT * FROM Products;\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "adventureworks",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/07_sql_dedicated_load_data')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-500/03-Synapse"
				},
				"content": {
					"query": "-- !! Make sure to run this against the DP500DWH database!\n\nCREATE SCHEMA [Staging]\nGO\n\nCREATE TABLE [Staging].[FactInternetSales]\n(\n\t[SalesOrderNumber] [nvarchar](20) NOT NULL,\n\t[SalesOrderLineNumber] [tinyint] NOT NULL,\n\t[CustomerKey] [int] NOT NULL,\n\t[ProductKey] [int] NOT NULL,\n\t[OrderDateKey] [int] NOT NULL,\n\t[DueDateKey] [int] NOT NULL,\n\t[ShipDateKey] [int] NULL,\n\t[PromotionKey] [int] NOT NULL,\n\t[CurrencyKey] [int] NOT NULL,\n\t[SalesTerritoryKey] [int] NOT NULL,\n\t[OrderQuantity] [smallint] NOT NULL,\n\t[UnitPrice] [money] NOT NULL,\n\t[ExtendedAmount] [money] NOT NULL,\n\t[UnitPriceDiscountPct] [decimal](7, 4) NOT NULL,\n\t[DiscountAmount] [float] NOT NULL,\n\t[ProductStandardCost] [money] NOT NULL,\n\t[TotalProductCost] [money] NOT NULL,\n\t[SalesAmount] [money] NOT NULL,\n\t[TaxAmount] [money] NOT NULL,\n\t[FreightAmount] [money] NOT NULL,\n\t[CarrierTrackingNumber] [nvarchar](25) NULL,\n\t[CustomerPONumber] [nvarchar](25) NULL,\n\t[RevisionNumber] [tinyint] NOT NULL\n\t)\nWITH\n(\n\tDISTRIBUTION = ROUND_ROBIN,\n\tCLUSTERED COLUMNSTORE INDEX\n)\nGO\n\nCOPY INTO [Staging].[FactInternetSales]\nFROM 'https://datalakeweslbo.dfs.core.windows.net/landing/Allfiles/03/data/FactInternetSales.txt'\nWITH\n(\n\tFILE_TYPE = 'CSV'\n\t,MAXERRORS = 0\n\t,FIELDTERMINATOR = '\\t'\n\t,FIRSTROW = 1\n\t,ERRORFILE = 'https://datalakeweslbo.dfs.core.windows.net/landing/Allfiles/03/errors/'\n    ,ENCODING = 'UTF16'\n)\nGO\n\n\n\n\nCREATE TABLE [Staging].[DimAccount]\n( \n\t[AccountKey] [int]  NOT NULL,\n\t[ParentAccountKey] [int]  NULL,\n\t[AccountCodeAlternateKey] [int]  NULL,\n\t[ParentAccountCodeAlternateKey] [int]  NULL,\n\t[AccountDescription] [nvarchar](50)  NULL,\n\t[AccountType] [nvarchar](50)  NULL,\n\t[Operator] [nvarchar](50)  NULL,\n\t[CustomMembers] [nvarchar](300)  NULL,\n\t[ValueType] [nvarchar](50)  NULL,\n\t[CustomMemberOptions] [nvarchar](200)  NULL\n)\nWITH\n(\n\tDISTRIBUTION = ROUND_ROBIN,\n\tCLUSTERED COLUMNSTORE INDEX\n)\nGO\n\nCOPY INTO [Staging].[DimAccount]\nFROM 'https://datalakeweslbo.dfs.core.windows.net/landing/Allfiles/03/data/DimAccount.txt'\nWITH\n(\n\tFILE_TYPE = 'CSV'\n\t,MAXERRORS = 0\n\t,FIELDTERMINATOR = '\\t'\n\t,FIRSTROW = 1\n\t,ERRORFILE = 'https://datalakeweslbo.dfs.core.windows.net/landing/Allfiles/03/errors/'\n    ,ENCODING = 'UTF16'\n)\nGO\n\nDROP TABLE [Staging].[DimCurrency]\nCREATE TABLE [Staging].[DimCurrency]\n( \n\t[CurrencyKey] [int]  NOT NULL,\n\t[CurrencyAlternateKey] [nchar](3)  NOT NULL,\n\t[CurrencyName] [nvarchar](50)  NOT NULL,\n\t[FormatString] [nvarchar](20)  NULL\n)\nWITH\n(\n\tDISTRIBUTION = ROUND_ROBIN,\n\tCLUSTERED COLUMNSTORE INDEX\n)\nGO\n\nCOPY INTO [Staging].[DimCurrency]\nFROM 'https://datalakeweslbo.dfs.core.windows.net/landing/Allfiles/03/data/DimCurrency.txt'\nWITH\n(\n\tFILE_TYPE = 'CSV'\n\t,MAXERRORS = 0\n\t,FIELDTERMINATOR = '\\t'\n\t,FIRSTROW = 1\n\t,ERRORFILE = 'https://datalakeweslbo.dfs.core.windows.net/landing/Allfiles/03/errors/'\n    ,ENCODING = 'UTF16'\n)\nGO\n\n\n\nCREATE TABLE [Staging].[DimCustomer]\n( \n\t[CustomerKey] [int]  NOT NULL,\n\t[GeographyKey] [int]  NULL,\n\t[CustomerAlternateKey] [nvarchar](15)  NOT NULL,\n\t[Title] [nvarchar](8)  NULL,\n\t[FirstName] [nvarchar](50)  NULL,\n\t[MiddleName] [nvarchar](50)  NULL,\n\t[LastName] [nvarchar](50)  NULL,\n\t[NameStyle] [bit]  NULL,\n\t[BirthDate] [date]  NULL,\n\t[MaritalStatus] [nchar](1)  NULL,\n\t[Suffix] [nvarchar](10)  NULL,\n\t[Gender] [nvarchar](1)  NULL,\n\t[EmailAddress] [nvarchar](50)  NULL,\n\t[YearlyIncome] [money]  NULL,\n\t[TotalChildren] [tinyint]  NULL,\n\t[NumberChildrenAtHome] [tinyint]  NULL,\n\t[EnglishEducation] [nvarchar](40)  NULL,\n\t[SpanishEducation] [nvarchar](40)  NULL,\n\t[FrenchEducation] [nvarchar](40)  NULL,\n\t[EnglishOccupation] [nvarchar](100)  NULL,\n\t[SpanishOccupation] [nvarchar](100)  NULL,\n\t[FrenchOccupation] [nvarchar](100)  NULL,\n\t[HouseOwnerFlag] [nchar](1)  NULL,\n\t[NumberCarsOwned] [tinyint]  NULL,\n\t[AddressLine1] [nvarchar](120)  NULL,\n\t[AddressLine2] [nvarchar](120)  NULL,\n\t[Phone] [nvarchar](20)  NULL,\n\t[DateFirstPurchase] [date]  NULL,\n\t[CommuteDistance] [nvarchar](15)  NULL\n)\nWITH\n(\n\tDISTRIBUTION = ROUND_ROBIN,\n\tCLUSTERED COLUMNSTORE INDEX\n)\nGO\n\nCOPY INTO [Staging].[DimCustomer]\nFROM 'https://datalakeweslbo.dfs.core.windows.net/landing/Allfiles/03/data/DimCustomer.txt'\nWITH\n(\n\tFILE_TYPE = 'CSV'\n\t,MAXERRORS = 0\n\t,FIELDTERMINATOR = '\\t'\n\t,FIRSTROW = 1\n\t,ERRORFILE = 'https://datalakeweslbo.dfs.core.windows.net/landing/Allfiles/03/errors/'\n    ,ENCODING = 'UTF16'\n)\nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/08_sql_date_dimension')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-500/03-Synapse"
				},
				"content": {
					"query": "CREATE TABLE dbo.DimDate\n( \n    DateKey INT NOT NULL,\n    DateAltKey DATETIME NOT NULL,\n    DayOfMonth INT NOT NULL,\n    DayOfWeek INT NOT NULL,\n    DayName NVARCHAR(15) NOT NULL,\n    MonthOfYear INT NOT NULL,\n    MonthName NVARCHAR(15) NOT NULL,\n    CalendarQuarter INT  NOT NULL,\n    CalendarYear INT NOT NULL,\n    FiscalQuarter INT NOT NULL,\n    FiscalYear INT NOT NULL\n)\nWITH\n(\n    DISTRIBUTION = REPLICATE,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\n-- Create a temporary table for the dates we need\nCREATE TABLE #TmpStageDate (DateVal DATE NOT NULL)\n\n-- Populate the temp table with a range of dates\nDECLARE @StartDate DATE\nDECLARE @EndDate DATE\nSET @StartDate = '2019-01-01'\nSET @EndDate = '2022-12-31' \nDECLARE @LoopDate DATE\nSET @LoopDate = @StartDate\nWHILE @LoopDate <= @EndDate\nBEGIN\n    INSERT INTO #TmpStageDate VALUES\n    (\n        @LoopDate\n    ) \n    SET @LoopDate = DATEADD(dd, 1, @LoopDate)\nEND\n\n-- Insert the dates and calculated attributes into the dimension table\nINSERT INTO dbo.DimDate \nSELECT  CAST(CONVERT(VARCHAR(8), DateVal, 112) AS int) , -- date key\n        DateVal, -- date alt key\n        Day(DateVal),  -- day number of month\n        datepart(dw, DateVal), -- day number of week\n        datename(dw, DateVal), -- day name of week\n        Month(DateVal), -- month number of year\n        datename(mm, DateVal), -- month name\n        datepart(qq, DateVal), -- calendar quarter\n        Year(DateVal), -- calendar year\n        CASE\n            WHEN Month(DateVal) IN (1, 2, 3) THEN 3\n            WHEN Month(DateVal) IN (4, 5, 6) THEN 4\n            WHEN Month(DateVal) IN (7, 8, 9) THEN 1\n            WHEN Month(DateVal) IN (10, 11, 12) THEN 2\n        END, -- fiscal quarter (fiscal year runs from Jul to June)\n        CASE\n            WHEN Month(DateVal) < 7 THEN Year(DateVal)\n            ELSE Year(DateVal) + 1\n        END -- Fiscal year \nFROM #TmpStageDate\nGO\n\n\nSELECT TOP 10 * FROM dbo.DimDate",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Configure Azure Synapse Link')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/06-HTAP/15-Synapse-link-sql"
				},
				"content": {
					"query": "/*\nConfigure Azure Synapse Link\n\nMake sure that the dedicated SQL pool has been started\n*/\n\n-- Next, create the target schema\nCREATE SCHEMA SalesLT;\nGO\n\n-- Create a link connection in the Integrate hub (+)\n-- Change Feed is not supported on Free, Basic or Standard tier Single Database (S0,S1,S2) \n-- and Database in Elastic pool with max eDTUs < 100 or max vCore < 1. Please upgrade to a higher Service Objective.\n-- Upscale the database to an S3 (100 DTU)\n-- ESTIMATED COST / MONTH: 147.19 USD\n\nSELECT  oh.SalesOrderID, oh.OrderDate,\n        p.ProductNumber, p.Color, p.Size,\n        c.EmailAddress AS CustomerEmail,\n        od.OrderQty, od.UnitPrice\nFROM SalesLT.SalesOrderHeader AS oh\nJOIN SalesLT.SalesOrderDetail AS od \n    ON oh.SalesOrderID = od.SalesOrderID\nJOIN  SalesLT.Product AS p \n    ON od.ProductID = p.ProductID\nJOIN SalesLT.Customer as c\n    ON oh.CustomerID = c.CustomerID\nORDER BY oh.SalesOrderID;\n\n-- CLEANUP\n\nDROP SCHEMA SalesLT;\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqldwh",
						"poolName": "sqldwh"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Covid19')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-500/05-PowerBI"
				},
				"content": {
					"query": "CREATE DATABASE Covid19\n\nUSE Covid19\n\nIF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nDROP EXTERNAL DATA SOURCE [PandemicDataLake] \nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'PandemicDataLake') \n\tCREATE EXTERNAL DATA SOURCE [PandemicDataLake] \n\tWITH (\n\t\tLOCATION   = 'https://pandemicdatalake.blob.core.windows.net/public', \n\t)\nGo\n\nDROP EXTERNAL TABLE BingCovid19\nCREATE EXTERNAL TABLE BingCovid19 (\n\t[id] int,\n\t[updated] date,\n\t[confirmed] int,\n\t[confirmed_change] int,\n\t[deaths] int,\n\t[deaths_change] smallint,\n\t[recovered] int,\n\t[recovered_change] int,\n\t[latitude] float,\n\t[longitude] float,\n\t[iso2] varchar(8000),\n\t[iso3] varchar(8000),\n\t[country_region] varchar(8000),\n\t[admin_region_1] varchar(8000),\n\t[iso_subdivision] varchar(8000),\n\t[admin_region_2] varchar(8000),\n\t[load_time] datetime2(7)\n\t)\n\tWITH (\n\tLOCATION = 'curated/covid-19/bing_covid-19_data/latest/bing_covid-19_data.parquet',\n\tDATA_SOURCE = [PandemicDataLake],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\nSELECT TOP 100 * FROM BingCovid19\nGO\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "Covid19",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dedicated - Purview Permissions')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/08-Govern data/22-Purview"
				},
				"content": {
					"query": "CREATE USER [purview-weslbo] FROM EXTERNAL PROVIDER\n\n\nEXEC sp_addrolemember 'db_datareader',  [purview-weslbo]\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqldwh",
						"poolName": "sqldwh"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Explore a relational data warehouse')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/04-Dedicated SQL/08-Demo-Explore a relational data warehouse"
				},
				"content": {
					"query": "/* This exercise is shared with the DP-500 (Azure Data Analyst) curriculum and is available at\n   https://microsoftlearning.github.io/DP-500-Azure-Data-Analyst/Instructions/labs/03-Explore-data-warehouse.html\n\nExplore the data warehouse schema\n=================================\n- Make sure the sqldwh has been resumed\n- View the tables in the database\n\nA relational data warehouse is typically based on a schema that consists of fact and dimension tables. \nThe tables are optimized for analytical queries in which numeric metrics in the fact tables are aggregated \nby attributes of the entities represented by the dimension tables - for example, enabling you to aggregate\nInternet sales revenue by product, customer, date, and so on.\n\nExpand the dbo.FactInternetSales table and its Columns folder to see the columns in this table. \nNote that many of the columns are keys that reference rows in the dimension tables. Others are numeric values\n(measures) for analysis. The keys are used to relate a fact table to one or more dimension tables, \noften in a star schema; in which the fact table is directly related to each dimension table (forming a \nmulti-pointed “star” with the fact table at the center).\n\n*/\n\nselect column_name, data_type from information_schema.columns where table_name = 'FactInternetSales' order by ordinal_position\n\n/*\nView the columns for the dbo.DimPromotion table, and note that it has a unique PromotionKey that uniquely\nidentifies each row in the table. It also has an AlternateKey. Usually, data in a data warehouse has been \nimported from one or more transactional sources. The alternate key reflects the business identifier for the \ninstance of this entity in the source, but a unique numeric surrogate key is usually generated to uniquely \nidentify each row in the data warehouse dimension table. One of the benefits of this approach is that it \nenables the data warehouse to contain multiple instances of the same entity at different points in time \n(for example, records for the same customer reflecting their address at the time an order was placed).\n*/\n\nselect column_name, data_type from information_schema.columns where table_name = 'DimPromotion' order by ordinal_position\n\n/*\nView the columns for the dbo.DimProduct, and note that it contains a ProductSubcategoryKey column, \nwhich references the dbo.DimProductSubcategory table, which in turn contains a ProductCategoryKey \ncolumn that references the dbo.DimProductCategory table. In some cases, dimensions are partially normalized \ninto multiple related tables to allow for different levels of granularity - such as products that can be \ngrouped into subcategories and categories. This results in a simple star being extended to a snowflake schema, \nin which the central fact table is related to a dimension table, which is turn related to further dimension tables.\n*/\n\nselect column_name, data_type from information_schema.columns where table_name = 'DimProduct' order by ordinal_position\nselect column_name, data_type from information_schema.columns where table_name = 'DimProductSubcategory' order by ordinal_position\nselect column_name, data_type from information_schema.columns where table_name = 'DimProductCategory' order by ordinal_position\n\n/*\nView the columns for the dbo.DimDate table, and note that it contains multiple columns that reflect different temporal \nattributes of a date - including the day of week, day of month, month, year, day name, month name, and so on. \nTime dimensions in a data warehouse are usually implemented as a dimension table containing a row for each of \nthe smallest temporal units of granularity (often called the grain of the dimension) by which you want to aggregate \nthe measures in the fact tables. In this case, the lowest grain at which measures can be aggregated is an individual date, \nand the table contains a row for each date from the first to the last date referenced in the data. The attributes in the \nDimDate table enable analysts to aggregate measures based on any date key in the fact table, using a consistent set of \ntemporal attributes (for example, viewing orders by month based on the order date). The FactInternetSales table contains \nthree keys that relate to the DimDate table: OrderDateKey, DueDateKey, and ShipDateKey.\n*/\n\nselect column_name, data_type from information_schema.columns where table_name = 'DimDate' order by ordinal_position\n\n/*\nQuery the data warehouse tables\n===============================\n\nNow that you have explored some of the more important aspects of the data warehouse schema, you’re ready to to query \nthe tables and retrieve some data.\n\n1. Query fact and dimension tables\n----------------------------------\nNumeric values in a relational data warehouse are stored in fact tables with related dimension tables that you can use \nto aggregate the data across multiple attributes. This design means that most queries in a relational data warehouse \ninvolve aggregating and grouping data (using aggregate functions and GROUP BY clauses) across related tables (using JOIN clauses).\n\nThe following query should show the Internet sales totals for each year. This query joins the fact table for Internet sales \nto a time dimension table based on the order date, and aggregates the sales amount measure in the fact table by \nthe calendar month attribute of the dimension table.\n*/\n\nSELECT  d.CalendarYear AS Year,\n        SUM(i.SalesAmount) AS InternetSalesAmount\nFROM FactInternetSales AS i\nJOIN DimDate AS d ON i.OrderDateKey = d.DateKey\nGROUP BY d.CalendarYear\nORDER BY Year;\n\n/*\nModify the query as follows to add the month attribute from the time dimension, and then run the modified query.\nNote that the attributes in the time dimension enable you to aggregate the measures in the fact table at multiple \nhierarchical levels - in this case, year and month. This is a common pattern in data warehouses.\n*/\n\nSELECT  d.CalendarYear AS Year,\n        d.MonthNumberOfYear AS Month,\n        SUM(i.SalesAmount) AS InternetSalesAmount\nFROM FactInternetSales AS i\nJOIN DimDate AS d ON i.OrderDateKey = d.DateKey\nGROUP BY d.CalendarYear, d.MonthNumberOfYear\nORDER BY Year, Month;\n\n/*\nModify the query as follows to remove the month and add a second dimension to the aggregation, and then run \nit to view the results (which show yearly Internet sales totals for each region):\n\nNote that geography is a snowflake dimension that is related to the Internet sales fact table through the customer\ndimension. You therefore need two joins in the query to aggregate Internet sales by geography.\n*/\n\nSELECT  d.CalendarYear AS Year,\n        g.EnglishCountryRegionName AS Region,\n        SUM(i.SalesAmount) AS InternetSalesAmount\nFROM FactInternetSales AS i\nJOIN DimDate AS d ON i.OrderDateKey = d.DateKey\nJOIN DimCustomer AS c ON i.CustomerKey = c.CustomerKey\nJOIN DimGeography AS g ON c.GeographyKey = g.GeographyKey\nGROUP BY d.CalendarYear, g.EnglishCountryRegionName\nORDER BY Year, Region;\n\n/*\nModify and re-run the query to add another snowflake dimension and aggregate the yearly regional sales by product category:\n\nThis time, the snowflake dimension for product category requires three joins to reflect the hierarchical relationship \nbetween products, subcategories, and categories.\n*/\n\nSELECT  d.CalendarYear AS Year,\n        pc.EnglishProductCategoryName AS ProductCategory,\n        g.EnglishCountryRegionName AS Region,\n        SUM(i.SalesAmount) AS InternetSalesAmount\nFROM FactInternetSales AS i\nJOIN DimDate AS d ON i.OrderDateKey = d.DateKey\nJOIN DimCustomer AS c ON i.CustomerKey = c.CustomerKey\nJOIN DimGeography AS g ON c.GeographyKey = g.GeographyKey\nJOIN DimProduct AS p ON i.ProductKey = p.ProductKey\nJOIN DimProductSubcategory AS ps ON p.ProductSubcategoryKey = ps.ProductSubcategoryKey\nJOIN DimProductCategory AS pc ON ps.ProductCategoryKey = pc.ProductCategoryKey\nGROUP BY d.CalendarYear, pc.EnglishProductCategoryName, g.EnglishCountryRegionName\nORDER BY Year, ProductCategory, Region;\n\n/*\n2. Use ranking functions\n------------------------\n\nAnother common requirement when analyzing large volumes of data is to group the data by partitions and determine \nthe rank of each entity in the partition based on a specific metric.\n\nThe following SQL retrieves sales values for 2022 over partitions based on country/region name. Observe the following facts about these results:\n\n- There’s a row for each sales order line item.\n- The rows are organized in partitions based on the geography where the sale was made.\n- The rows within each geographical partition are numbered in order of sales amount (from smallest to highest).\n- For each row, the line item sales amount as well as the regional total and average sales amounts are included.\n*/\n\nSELECT  g.EnglishCountryRegionName AS Region,\n      ROW_NUMBER() OVER(PARTITION BY g.EnglishCountryRegionName\n                        ORDER BY i.SalesAmount ASC) AS RowNumber,\n      i.SalesOrderNumber AS OrderNo,\n      i.SalesOrderLineNumber AS LineItem,\n      i.SalesAmount AS SalesAmount,\n      SUM(i.SalesAmount) OVER(PARTITION BY g.EnglishCountryRegionName) AS RegionTotal,\n      AVG(i.SalesAmount) OVER(PARTITION BY g.EnglishCountryRegionName) AS RegionAverage\nFROM FactInternetSales AS i\nJOIN DimDate AS d ON i.OrderDateKey = d.DateKey\nJOIN DimCustomer AS c ON i.CustomerKey = c.CustomerKey\nJOIN DimGeography AS g ON c.GeographyKey = g.GeographyKey\nWHERE d.CalendarYear = 2022\nORDER BY Region;\n\n/*\nApply windowing functions within a GROUP BY query and rank the cities in each region based on their total sales amount:\n\nObserve the following:\n- The results include a row for each city, grouped by region.\n- The total sales (sum of individual sales amounts) is calculated for each city\n- The regional sales total (the sum of the sum of sales amounts for each city in the region) is calculated based on the regional partition.\n- The rank for each city within its regional partition is calculated by ordering the total sales amount per city in descending order.\n*/\n\nSELECT  g.EnglishCountryRegionName AS Region,\n      g.City,\n      SUM(i.SalesAmount) AS CityTotal,\n      SUM(SUM(i.SalesAmount)) OVER(PARTITION BY g.EnglishCountryRegionName) AS RegionTotal,\n      RANK() OVER(PARTITION BY g.EnglishCountryRegionName\n                  ORDER BY SUM(i.SalesAmount) DESC) AS RegionalRank\nFROM FactInternetSales AS i\nJOIN DimDate AS d ON i.OrderDateKey = d.DateKey\nJOIN DimCustomer AS c ON i.CustomerKey = c.CustomerKey\nJOIN DimGeography AS g ON c.GeographyKey = g.GeographyKey\nGROUP BY g.EnglishCountryRegionName, g.City\nORDER BY Region;\n\n/*\n3. Retrieve an approximate count\n--------------------------------\nWhen exploring very large volumes of data, queries can take significant time and resources to run. Often, data analysis doesn’t \nrequire absolutely precise values - a comparison of approximate values may be sufficient.\n\nThe following SQL code retrieves the number of sales orders for each calendar year:\n\nThen review the output that is returned:\n- On the Results tab under the query, view the order counts for each year.\n- On the Messages tab, view the total execution time for the query.\n\n*/\n\nSELECT d.CalendarYear AS CalendarYear,\n   COUNT(DISTINCT i.SalesOrderNumber) AS Orders\nFROM FactInternetSales AS i\nJOIN DimDate AS d ON i.OrderDateKey = d.DateKey\nGROUP BY d.CalendarYear\nORDER BY CalendarYear;\n\n/*\nNow, return an approximate count for each year. Review the output that is returned:\n- On the Results tab under the query, view the order counts for each year. These should be within 2% of the actual counts retrieved by the previous query.\n- On the Messages tab, view the total execution time for the query. This should be shorter than for the previous query.\n*/\n\nSELECT d.CalendarYear AS CalendarYear,\n   APPROX_COUNT_DISTINCT(i.SalesOrderNumber) AS Orders\nFROM FactInternetSales AS i\nJOIN DimDate AS d ON i.OrderDateKey = d.DateKey\nGROUP BY d.CalendarYear\nORDER BY CalendarYear;\n\n/*\nLet's see if there's a difference in the number we get returned (notice that the following query might not be optimized)\n*/\nSELECT d.CalendarYear AS CalendarYear,\n   COUNT(DISTINCT i.SalesOrderNumber) AS OrdersCount,\n   APPROX_COUNT_DISTINCT(i.SalesOrderNumber) AS OrdersApproxCount\nFROM FactInternetSales AS i\nJOIN DimDate AS d ON i.OrderDateKey = d.DateKey\nGROUP BY d.CalendarYear\nORDER BY CalendarYear\n\n/*\n3. Challenge - Analyze reseller sales\n-------------------------------------\nCreate SQL queries in the script to find the following information based on the FactResellerSales fact table and the dimension tables to which it is related:\n- The total quantity of items sold per fiscal year and quarter.\n- The total quantity of items sold per fiscal year, quarter, and sales territory region associated with the employee who made the sale.\n- The total quantity of items sold per fiscal year, quarter, and sales territory region by product category.\n- The rank of each sales territory per fiscal year based on total sales amount for the year.\n- The approximate number of sales order per year in each sales territory\n*/\n\n-- Items sold by Fiscal Year and Quarter\nSELECT  d.FiscalYear AS FY,\n        d.FiscalQuarter AS FQ,\n        SUM(r.OrderQuantity) AS ItemsSold\nFROM FactResellerSales AS r\nJOIN DimDate AS d ON r.OrderDateKey = d.DateKey\nGROUP BY d.FiscalYear, d.FiscalQuarter\nORDER BY FY, FQ;\n\n\n-- Items sold by Fiscal Year, Quarter, and sales territory region\nSELECT  d.FiscalYear AS FY,\n        d.FiscalQuarter AS FQ,\n        t. SalesTerritoryRegion AS SalesTerritory,\n        SUM(r.OrderQuantity) AS ItemsSold\nFROM FactResellerSales AS r\nJOIN DimDate AS d ON r.OrderDateKey = d.DateKey\nJOIN DimEmployee AS e ON r.EmployeeKey = e.EmployeeKey\nJOIN DimSalesTerritory AS t ON e.SalesTerritoryKey = t.SalesTerritoryKey\nGROUP BY d.FiscalYear, d.FiscalQuarter, t. SalesTerritoryRegion\nORDER BY FY, FQ, SalesTerritory\n\n\n-- Items sold by Fiscal Year, Quarter, sales territory region, and product category\nSELECT  d.FiscalYear AS FY,\n        d.FiscalQuarter AS FQ,\n        t. SalesTerritoryRegion AS SalesTerritory,\n        pc.EnglishProductCategoryName AS ProductCategory,\n        SUM(r.OrderQuantity) AS ItemsSold\nFROM FactResellerSales AS r\nJOIN DimDate AS d ON r.OrderDateKey = d.DateKey\nJOIN DimEmployee AS e ON r.EmployeeKey = e.EmployeeKey\nJOIN DimSalesTerritory AS t ON e.SalesTerritoryKey = t.SalesTerritoryKey\nJOIN DimProduct AS p ON r.ProductKey = p.ProductKey\nJOIN DimProductSubcategory AS ps ON p.ProductSubcategoryKey = ps.ProductSubcategoryKey\nJOIN DimProductCategory AS pc ON ps.ProductCategoryKey = pc.ProductCategoryKey\nGROUP BY d.FiscalYear, d.FiscalQuarter, t. SalesTerritoryRegion, pc.EnglishProductCategoryName\nORDER BY FY, FQ, SalesTerritory, ProductCategory\n\n\n-- Ranked sales territories by year based on total sales amount\nSELECT  d.FiscalYear,\n        t. SalesTerritoryRegion AS SalesTerritory,\n        SUM(s.SalesAmount) AS TerritoryTotal,\n        SUM(SUM(s.SalesAmount)) OVER(PARTITION BY d.FiscalYear) AS YearTotal,\n        RANK() OVER(PARTITION BY d.FiscalYear\n                    ORDER BY SUM(s.SalesAmount) DESC) AS RankForYear\nFROM FactResellerSales AS s\nJOIN DimDate AS d ON s.OrderDateKey = d.DateKey\nJOIN DimEmployee AS e ON s.EmployeeKey = e.EmployeeKey\nJOIN DimSalesTerritory AS t ON e.SalesTerritoryKey = t.SalesTerritoryKey\nGROUP BY d.FiscalYear, t.SalesTerritoryRegion\nORDER BY d.FiscalYear;\n\n-- Approximate number of sales orders per fiscal year by territory\nSELECT  d.FiscalYear,\n        t. SalesTerritoryRegion AS SalesTerritory,\n        APPROX_COUNT_DISTINCT(s.SalesOrderNumber) AS ApproxOrders\nFROM FactResellerSales AS s\nJOIN DimDate AS d ON s.OrderDateKey = d.DateKey\nJOIN DimEmployee AS e ON s.EmployeeKey = e.EmployeeKey\nJOIN DimSalesTerritory AS t ON e.SalesTerritoryKey = t.SalesTerritoryKey\nGROUP BY d.FiscalYear, t.SalesTerritoryRegion\nORDER BY d.FiscalYear, ApproxOrders;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqldwh",
						"poolName": "sqldwh"
					},
					"resultLimit": -1
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Load Data into a Relational Data Warehouse')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/04-Dedicated SQL/09-Lab-Load data into a relational data warehouse"
				},
				"content": {
					"query": "/*\n1. Load data from a data lake by using the COPY statement\n---------------------------------------------------------\n\nConfirm that there are 0 rows currently in the StageProduct table.\n*/\n\nSELECT COUNT(1) \nFROM dbo.StageProduct\n\n/*\nRun the following command. 11 rows should have been loaded into the StageProduct table.\n*/\n\nCOPY INTO dbo.StageProduct\n    (ProductID, ProductName, ProductCategory, Color, Size, ListPrice, Discontinued)\nFROM 'https://datalakeweslbo.blob.core.windows.net/files/labs-dp-203/09/data/Product.csv'\nWITH\n(\n    FILE_TYPE = 'CSV',\n    MAXERRORS = 0,\n    IDENTITY_INSERT = 'OFF',\n    FIRSTROW = 2 --Skip header row\n);\n\n/*\nNow let's use the same technique to load another table, this time logging any errors that might occur.\n\nNotice the messages tab:\nQuery completed. Rows were rejected while reading from external source(s). 1 row rejected from table [StageCustomer] \nin plan step 4 of query execution: Bulk load data conversion error (type mismatch or invalid character for the \nspecified codepage) for row starting at byte offset 2261, column 1 (GeographyKey) in data file /labs/09/data/Customer.csv.\n*/\n\nCOPY INTO dbo.StageCustomer\n    (GeographyKey, CustomerAlternateKey, Title, FirstName, MiddleName, LastName, NameStyle, BirthDate, \n    MaritalStatus, Suffix, Gender, EmailAddress, YearlyIncome, TotalChildren, NumberChildrenAtHome, EnglishEducation, \n    SpanishEducation, FrenchEducation, EnglishOccupation, SpanishOccupation, FrenchOccupation, HouseOwnerFlag, \n    NumberCarsOwned, AddressLine1, AddressLine2, Phone, DateFirstPurchase, CommuteDistance)\nFROM 'https://datalakeweslbo.blob.core.windows.net/files/labs-dp-203/09/data/Customer.csv'\nWITH\n(\nFILE_TYPE = 'CSV'\n,MAXERRORS = 5\n,FIRSTROW = 2 -- skip header row\n,ERRORFILE = 'https://datalakeweslbo.blob.core.windows.net/files/labs-dp-203/09/errors/'\n);\n\n/*\nThe source file contains a row with invalid data, so one row is rejected. The code above specifies a maximum of 5 errors, so a single \nerror should not have prevented the valid rows from being loaded. You can view the rows that have been loaded by running the following query.\n*/\n\nSELECT *\nFROM dbo.StageCustomer\n\n/*\nOn the files tab, view the folder of your data lake (files/labs/09) and verify that a new folder named _rejectedrows has been created \n(if you don't see this folder, in the More menu, select Refresh to refresh the view).\n\nOpen the _rejectedrows folder and the date and time specific subfolder it contains, and note that files with names similar to QID123_1_2.Error.Txt \nand QID123_1_2.Row.Txt have been created. You can right-click each of these files and select Preview to see details of the error and the row that was rejected.\n\n- \"Bulk load data conversion error (type mismatch or invalid character for the specified codepage) for row starting at byte offset 2261, column 1 (GeographyKey) in data file /labs/09/data/Customer.csv.\"\n- \"US,AW99,,Billy,L,Jones,FALSE,Dec 12th 2001\"\n\n(notice the value \"US\")\n\nThe use of staging tables enables you to validate or transform data before moving or using it to append to or upsert into any existing dimension tables. \nThe COPY statement provides a simple but high-performance technique that you can use to easily load data from files in a data lake into staging tables, \nand as you've seen, identify and redirect invalid rows.\n*/\n\n/*\n2. Use a CREATE TABLE AS (CTAS) statement\n-----------------------------------------\n\nThe following SQL creates a new table named DimProduct from the staged product data that uses ProductAltKey as its\nhash distribution key and has a clustered columnstore index.\n*/\n\nIF EXISTS (SELECT 1 FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'dbo' AND TABLE_NAME = 'DimProductLab9') DROP TABLE [dbo].[DimProductLab9];\n\nCREATE TABLE dbo.DimProductLab9\nWITH\n(\n    DISTRIBUTION = HASH(ProductAltKey),\n    CLUSTERED COLUMNSTORE INDEX\n)\nAS\nSELECT ROW_NUMBER() OVER(ORDER BY ProductID) AS ProductKey,\n    ProductID AS ProductAltKey,\n    ProductName,\n    ProductCategory,\n    Color,\n    Size,\n    ListPrice,\n    Discontinued\nFROM dbo.StageProduct;\n\n/*\nUse the following query to view the contents of the new DimProduct table:\n\nThe CREATE TABLE AS SELECT (CTAS) expression has various uses, which include:\n\n- Redistributing the hash key of a table to align with other tables for better query performance.\n- Assigning a surrogate key to a staging table based upon existing values after performing a delta analysis.\n- Creating aggregate tables quickly for report purposes.\n\n*/\n\nSELECT ProductKey,\n    ProductAltKey,\n    ProductName,\n    ProductCategory,\n    Color,\n    Size,\n    ListPrice,\n    Discontinued\nFROM dbo.DimProductLab9;\n\n/*\n3. Combine INSERT and UPDATE statements to load a slowly changing dimension table\n---------------------------------------------------------------------------------\nThe DimCustomer table supports type 1 and type 2 slowly changing dimensions (SCDs), where type 1 changes result in \nan in-place update to an existing row, and type 2 changes result in a new row to indicate the latest version of a \nparticular dimension entity instance. Loading this table requires a combination of INSERT statements (to load new customers) \nand UPDATE statements (to apply type 1 or type 2 changes).\n*/\n\n\nINSERT INTO dbo.DimCustomer ([GeographyKey],[CustomerAlternateKey],[Title],[FirstName],[MiddleName],[LastName],[NameStyle],[BirthDate],[MaritalStatus],\n[Suffix],[Gender],[EmailAddress],[YearlyIncome],[TotalChildren],[NumberChildrenAtHome],[EnglishEducation],[SpanishEducation],[FrenchEducation],\n[EnglishOccupation],[SpanishOccupation],[FrenchOccupation],[HouseOwnerFlag],[NumberCarsOwned],[AddressLine1],[AddressLine2],[Phone],\n[DateFirstPurchase],[CommuteDistance])\nSELECT *\nFROM dbo.StageCustomer AS stg\nWHERE NOT EXISTS\n    (SELECT * FROM dbo.DimCustomer AS dim\n    WHERE dim.CustomerAlternateKey = stg.CustomerAlternateKey);\n\n-- Type 1 updates (change name, email, or phone in place)\nUPDATE dbo.DimCustomer\nSET LastName = stg.LastName,\n    EmailAddress = stg.EmailAddress,\n    Phone = stg.Phone\nFROM DimCustomer dim inner join StageCustomer stg\nON dim.CustomerAlternateKey = stg.CustomerAlternateKey\nWHERE dim.LastName <> stg.LastName OR dim.EmailAddress <> stg.EmailAddress OR dim.Phone <> stg.Phone\n\n-- Type 2 updates (address changes triggers new entry)\nINSERT INTO dbo.DimCustomer\nSELECT stg.GeographyKey,stg.CustomerAlternateKey,stg.Title,stg.FirstName,stg.MiddleName,stg.LastName,stg.NameStyle,stg.BirthDate,stg.MaritalStatus,\nstg.Suffix,stg.Gender,stg.EmailAddress,stg.YearlyIncome,stg.TotalChildren,stg.NumberChildrenAtHome,stg.EnglishEducation,stg.SpanishEducation,stg.FrenchEducation,\nstg.EnglishOccupation,stg.SpanishOccupation,stg.FrenchOccupation,stg.HouseOwnerFlag,stg.NumberCarsOwned,stg.AddressLine1,stg.AddressLine2,stg.Phone,\nstg.DateFirstPurchase,stg.CommuteDistance\nFROM dbo.StageCustomer AS stg\nJOIN dbo.DimCustomer AS dim\nON stg.CustomerAlternateKey = dim.CustomerAlternateKey\nAND stg.AddressLine1 <> dim.AddressLine1;\n\n/*\n4. Perform post-load optimization\n---------------------------------\n\nAfter loading new data into the data warehouse, it's recommended to rebuild the table indexes and update statistics on commonly queried columns.\n*/\n\nALTER INDEX ALL ON dbo.DimProductLab9 REBUILD;\n\nCREATE STATISTICS customergeo_stats ON dbo.DimCustomer (GeographyKey);\n\n/*\nGet ready for the next demo (10-pipelines)\n*/\n\nIF EXISTS (SELECT 1 FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'dbo' AND TABLE_NAME = 'DimProductLab10') DROP TABLE [dbo].[DimProductLab10];\n\nSET ANSI_NULLS ON\nGO\nSET QUOTED_IDENTIFIER ON\nGO\nCREATE TABLE [dbo].[DimProductLab10](\n    [ProductKey] [int] IDENTITY NOT NULL,\n    [ProductAltKey] [nvarchar](30) NULL,\n    [ProductName] [nvarchar](50) NULL,\n    [Color] [nvarchar](30) NULL,\n    [Size] [nvarchar](50) NULL,\n    [ListPrice] [money] NULL,\n    [Discontinued] [bit] NULL)\nWITH\n(\n\tDISTRIBUTION = HASH(ProductAltKey),\n\tCLUSTERED COLUMNSTORE INDEX\n);\nGO\n\nINSERT DimProductLab10\nVALUES('AR-5381','Adjustable Race','Red',NULL,1.99,0);\nGO\n\nINSERT DimProductLab10 VALUES('AR-5381','Adjustable Race','Red',NULL,1.99,0);\nGO\n\nDROP TABLE [dbo].[DimProductLab9];",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqldwh",
						"poolName": "sqldwh"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Query Azure Cosmos DB from a serverless SQL pool')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/06-HTAP/14-Lab-Synapse Link CosmosDB"
				},
				"content": {
					"query": "IF (NOT EXISTS(SELECT * FROM sys.credentials WHERE name = 'cosmosdb'))\n    \n    CREATE CREDENTIAL [cosmosdb]\n    WITH IDENTITY = 'SHARED ACCESS SIGNATURE', SECRET = '<removed>'\nGO\n\nSELECT TOP 100 *\nFROM OPENROWSET(​PROVIDER = 'CosmosDB',\n                CONNECTION = 'Account=cosmos-weslbo;Database=AdventureWorks',\n                OBJECT = 'sales',\n                SERVER_CREDENTIAL = 'cosmosdb'\n) AS [sales]\n\nSELECT *\nFROM OPENROWSET(​PROVIDER = 'CosmosDB',\n                CONNECTION = 'Account=cosmos-weslbo;Database=AdventureWorks',\n                OBJECT = 'sales',\n                SERVER_CREDENTIAL = 'cosmosdb'\n)\nWITH (\n    OrderID VARCHAR(10) '$.id',\n    OrderDate VARCHAR(10) '$.orderdate',\n    CustomerID INTEGER '$.customerid',\n    CustomerName VARCHAR(40) '$.customerdetails.customername',\n    CustomerEmail VARCHAR(30) '$.customerdetails.customeremail',\n    Product VARCHAR(30) '$.product',\n    Quantity INTEGER '$.quantity',\n    Price FLOAT '$.price'\n)\nAS sales\nORDER BY OrderID;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Serverless - Purview Permissions')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/08-Govern data/22-Purview"
				},
				"content": {
					"query": "CREATE LOGIN [purview-weslbo] FROM EXTERNAL PROVIDER\n\nUSE default\n\nCREATE USER [purview-weslbo] FROM LOGIN [purview-weslbo]\nALTER ROLE db_datareader ADD MEMBER [purview-weslbo]\n\nUSE SalesDB\n\nCREATE USER [purview-weslbo] FROM LOGIN [purview-weslbo]\nALTER ROLE db_datareader ADD MEMBER [purview-weslbo]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SalesDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Setup and validation')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/05-Pipelines/10-Lab-Build a data pipeline in Azure Synapse Analytics"
				},
				"content": {
					"query": "/*\nGet ready for the next demo (10-pipelines)\n\nThe following code will drop and recreate the product data table - there should be a single row in it.\n*/\n\nIF EXISTS (SELECT 1 FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'dbo' AND TABLE_NAME = 'DimProductLab10') DROP TABLE [dbo].[DimProductLab10];\n\nSET ANSI_NULLS ON\nGO\nSET QUOTED_IDENTIFIER ON\nGO\n CREATE TABLE [dbo].[DimProductLab10](\n    [ProductKey] [int] IDENTITY NOT NULL,\n    [ProductAltKey] [nvarchar](30) NULL,\n    [ProductName] [nvarchar](50) NULL,\n    [Color] [nvarchar](30) NULL,\n    [Size] [nvarchar](50) NULL,\n    [ListPrice] [money] NULL,\n    [Discontinued] [bit] NULL)\nWITH\n(\n\tDISTRIBUTION = HASH(ProductAltKey),\n\tCLUSTERED COLUMNSTORE INDEX\n);\nGO\n\nINSERT DimProductLab10 VALUES('AR-5381','Adjustable Race','Red',NULL,1.99,0);\nGO\n\n/*\nNow go the pipeline dataflow and after the execution, return there to validate the number of records\n*/\n\nSELECT TOP 100 * from DimProductLab10",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqldwh",
						"poolName": "sqldwh"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Lab-Ingest data')]",
			"type": "Microsoft.Synapse/workspaces/kqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "ingest-data.kql",
				"folder": {
					"name": "dp-203/01-Intro to data engineering"
				},
				"content": {
					"query": "// Execute following statements separately\n.create table sales (\n    SalesOrderNumber: string,\n    SalesOrderLineItem: int,\n    OrderDate: datetime,\n    CustomerName: string,\n    EmailAddress: string,\n    Item: string,\n    Quantity: int,\n    UnitPrice: real,\n    TaxAmount: real)\n\n.ingest into table sales 'https://raw.githubusercontent.com/MicrosoftLearning/mslearn-synapse/master/Allfiles/Labs/01/files/sales.csv' \nwith (ignoreFirstRecord = true)",
					"metadata": {
						"language": "kql"
					},
					"currentConnection": {
						"poolName": "adxweslbo",
						"databaseName": "sales-data"
					}
				},
				"type": "KqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Lab-Select data')]",
			"type": "Microsoft.Synapse/workspaces/kqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/01-Intro to data engineering"
				},
				"content": {
					"query": "sales\n| take 1000\n\nsales\n| where Item == 'Road-250 Black, 48'\n\nsales\n| where Item == 'Road-250 Black, 48'\n| where datetime_part('year', OrderDate) > 2020\n\nsales\n| where OrderDate between (datetime(2020-01-01 00:00:00) .. datetime(2020-12-31 23:59:59))\n| summarize TotalNetRevenue = sum(UnitPrice) by Item\n| sort by Item asc",
					"metadata": {
						"language": "kql"
					},
					"currentConnection": {
						"poolName": "adxweslbo",
						"databaseName": "sales-data"
					}
				},
				"type": "KqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01-Analyze data in a data lake with Spark')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/03-Spark/05-Demo-Analyze data in a data with Spark"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "687f0a18-9caa-4a85-81ad-716c9c793916"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/784f1210-8faf-4cf4-b9aa-e50fa084adce/resourceGroups/rg-dp-203/providers/Microsoft.Synapse/workspaces/synapse-weslbo/bigDataPools/sparkpool",
						"name": "sparkpool",
						"type": "Spark",
						"endpoint": "https://synapse-weslbo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Use Spark to explore data\r\n",
							"\r\n",
							"- Select any of the files in the orders (labs-dp-500/02/data/) folder, and then in the New notebook list on the toolbar, select Load to DataFrame. A dataframe is a structure in Spark that represents a tabular dataset.\r\n",
							"\r\n",
							"- In the new Notebook 1 tab that opens, in the Attach to list, select your Spark pool (sparkxxxxxxx). Then use the ▷ Run all button to run all of the cells in the notebook (there’s currently only one!).\r\n",
							"\r\n",
							"- Since this is the first time you’ve run any Spark code in this session, the Spark pool must be started. This means that the first run in the session can take a few minutes. Subsequent runs will be quicker.\r\n",
							"\r\n",
							"- When the code has finished running, review the output beneath the cell in the notebook. It shows the first ten rows in the file you selected, with automatic column names in the form _c0, _c1, _c2, and so on."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://files@datalakeweslbo.dfs.core.windows.net/labs-dp-203/03/data/2019.csv', format='csv', header=True\r\n",
							")\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"- Modify the code so that the spark.read.load function reads data from all of the CSV files in the folder, and the display function shows the first 100 rows. Your code should look like this (with datalakexxxxxxx matching the name of your data lake store)\r\n",
							"\r\n",
							"- The dataframe now includes data from all of the files, but the column names are not useful. Spark uses a “schema-on-read” approach to try to determine appropriate data types for the columns based on the data they contain, and if a header row is present in a text file it can be used to identify the column names (by specifying a header=True parameter in the load function). Alternatively, you can define an explicit schema for the dataframe."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://files@datalakeweslbo.dfs.core.windows.net/labs-dp-203/03/data/*.csv', format='csv')\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"- Run the next cell and verify that the dataframe schema matches the orderSchema you defined. The printSchema function can be useful when using a dataframe with an automatically inferred schema."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"from pyspark.sql.types import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"\r\n",
							"orderSchema = StructType([\r\n",
							"    StructField(\"SalesOrderNumber\", StringType()),\r\n",
							"    StructField(\"SalesOrderLineNumber\", IntegerType()),\r\n",
							"    StructField(\"OrderDate\", DateType()),\r\n",
							"    StructField(\"CustomerName\", StringType()),\r\n",
							"    StructField(\"Email\", StringType()),\r\n",
							"    StructField(\"Item\", StringType()),\r\n",
							"    StructField(\"Quantity\", IntegerType()),\r\n",
							"    StructField(\"UnitPrice\", FloatType()),\r\n",
							"    StructField(\"Tax\", FloatType())\r\n",
							"    ])\r\n",
							"\r\n",
							"df = spark.read.load('abfss://files@datalakeweslbo.dfs.core.windows.net/labs-dp-203/03/data/*.csv', format='csv', schema=orderSchema, header=True)\r\n",
							"display(df.limit(100))"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							" df.printSchema()"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Analyze data in a dataframe\r\n",
							"\r\n",
							"The dataframe object in Spark is similar to a Pandas dataframe in Python, and includes a wide range of functions that you can use to manipulate, filter, group, and otherwise analyze the data it contains.\r\n",
							"\r\n",
							"## Filter a dataframe\r\n",
							"\r\n",
							"Run the next code cell, and review the results. Observe the following details:\r\n",
							"- When you perform an operation on a dataframe, the result is a new dataframe (in this case, a new customers dataframe is created by selecting a specific subset of columns from the df dataframe)\r\n",
							"- Dataframes provide functions such as count and distinct that can be used to summarize and filter the data they contain.\r\n",
							"- The dataframe['Field1', 'Field2', ...] syntax is a shorthand way of defining a subset of column. You can also use select method, so the first line of the code above could be written as customers = df.select(\"CustomerName\", \"Email\")"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							" customers = df['CustomerName', 'Email']\r\n",
							" print(customers.count())\r\n",
							" print(customers.distinct().count())\r\n",
							" display(customers.distinct())"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"- Run the next modified code to view the customers who have purchased the Road-250 Red, 52 product. Note that you can “chain” multiple functions together so that the output of one function becomes the input for the next - in this case, the dataframe created by the select method is the source dataframe for the where method that is used to apply filtering criteria."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							" customers = df.select(\"CustomerName\", \"Email\").where(df['Item']=='Road-250 Red, 52')\r\n",
							" print(customers.count())\r\n",
							" print(customers.distinct().count())\r\n",
							" display(customers.distinct())"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Aggregate and group data in a dataframe\r\n",
							"\r\n",
							"- Run the next code cell, and note that the results show the sum of order quantities grouped by product. The groupBy method groups the rows by Item, and the subsequent sum aggregate function is applied to all of the remaining numeric columns (in this case, Quantity)"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							" productSales = df.select(\"Item\", \"Quantity\").groupBy(\"Item\").sum()\r\n",
							" display(productSales)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"- Run the next code cell, and note that the results show the number of sales orders per year. Note that the select method includes a SQL year function to extract the year component of the OrderDate field, and then an alias method is used to assign a columm name to the extracted year value. The data is then grouped by the derived Year column and the count of rows in each group is calculated before finally the orderBy method is used to sort the res"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							" yearlySales = df.select(year(\"OrderDate\").alias(\"Year\")).groupBy(\"Year\").count().orderBy(\"Year\")\r\n",
							" display(yearlySales)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"yearlySales.write.partitionBy(\"Year\").parquet(\"abfss://files@datalakeweslbo.dfs.core.windows.net/labs-dp-203/03/data/yearlysales\")"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Query data using Spark SQL\r\n",
							"\r\n",
							"As you’ve seen, the native methods of the dataframe object enable you to query and analyze data quite effectively. However, many data analysts are more comfortable working with SQL syntax. Spark SQL is a SQL language API in Spark that you can use to run SQL statements, or even persist data in relational tables.\r\n",
							"\r\n",
							"## Use Spark SQL in PySpark code\r\n",
							"\r\n",
							"The default language in Azure Synapse Studio notebooks is PySpark, which is a Spark-based Python runtime. Within this runtime, you can use the spark.sql library to embed Spark SQL syntax within your Python code, and work with SQL constructs such as tables and views.\r\n",
							"\r\n",
							"Run the next cell and review the results. Observe that:\r\n",
							"\r\n",
							"- The code persists the data in the df dataframe as a temporary view named salesorders. Spark SQL supports the use of temporary views or persisted tables as sources for SQL queries.\r\n",
							"- The spark.sql method is then used to run a SQL query against the salesorders view.\r\n",
							"- The results of the query are stored in a dataframe."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							" df.createOrReplaceTempView(\"salesorders\")\r\n",
							"\r\n",
							" spark_df = spark.sql(\"SELECT * FROM salesorders\")\r\n",
							" display(spark_df)"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Run SQL code in a cell\r\n",
							"\r\n",
							"While it’s useful to be able to embed SQL statements into a cell containing PySpark code, data analysts often just want to work directly in SQL.\r\n",
							"\r\n",
							"Run the cell and review the results. Observe that:\r\n",
							"\r\n",
							"- The %%sql line at the beginning of the cell (called a magic) indicates that the Spark SQL language runtime should be used to run the code in this cell instead of PySpark.\r\n",
							"- The SQL code references the salesorder view that you created previously using PySpark.\r\n",
							"- The output from the SQL query is automatically displayed as the result under the cell."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							" %%sql\r\n",
							" SELECT YEAR(OrderDate) AS OrderYear,\r\n",
							"        SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue\r\n",
							" FROM salesorders\r\n",
							" GROUP BY YEAR(OrderDate)\r\n",
							" ORDER BY OrderYear;"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Visualize data with Spark\r\n",
							"\r\n",
							"A picture is proverbially worth a thousand words, and a chart is often better than a thousand rows of data. While notebooks in Azure Synapse Analytics include a built in chart view for data that is displayed from a dataframe or Spark SQL query, it is not designed for comprehensive charting. However, you can use Python graphics libraries like matplotlib and seaborn to create charts from data in dataframes.\r\n",
							"\r\n",
							"## View results as a chart\r\n",
							"\r\n",
							"Use the View options button at the top right of the chart to display the options pane for the chart. Then set the options as follows and select Apply:\r\n",
							"- **Chart type**: Bar chart\r\n",
							"- **Key**: Item\r\n",
							"- **Values**: Quantity\r\n",
							"- **Series Group**: leave blank\r\n",
							"- **Aggregation**: Sum\r\n",
							"- **Stacked**: Unselected"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							" %%sql\r\n",
							" SELECT * FROM salesorders"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Get started with matplotlib\r\n",
							"\r\n",
							"To visualize the data as a chart, we’ll start by using the matplotlib Python library. This library is the core plotting library on which many others are based, and provides a great deal of flexibility in creating charts"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							" sqlQuery = \"SELECT CAST(YEAR(OrderDate) AS CHAR(4)) AS OrderYear, \\\r\n",
							"                 SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue \\\r\n",
							"             FROM salesorders \\\r\n",
							"             GROUP BY CAST(YEAR(OrderDate) AS CHAR(4)) \\\r\n",
							"             ORDER BY OrderYear\"\r\n",
							" df_spark = spark.sql(sqlQuery)\r\n",
							" df_spark.show()"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Run the next cell and review the results, which consist of a column chart with the total gross revenue for each year. Note the following features of the code used to produce this chart:\r\n",
							"- The matplotlib library requires a Pandas dataframe, so you need to convert the Spark dataframe returned by the Spark SQL query to this format.\r\n",
							"- At the core of the matplotlib library is the pyplot object. This is the foundation for most plotting functionality.\r\n",
							"- The default settings result in a usable chart, but there’s considerable scope to customize it"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							" from matplotlib import pyplot as plt\r\n",
							"\r\n",
							" # matplotlib requires a Pandas dataframe, not a Spark one\r\n",
							" df_sales = df_spark.toPandas()\r\n",
							"\r\n",
							" # Create a bar plot of revenue by year\r\n",
							" plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'])\r\n",
							"\r\n",
							" # Display the plot\r\n",
							" plt.show()"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Run the next code cell and view the results. The chart now includes a little more information.\r\n",
							"\r\n",
							"A plot is technically contained with a Figure. In the previous examples, the figure was created implicitly for you; but you can create it explicitly."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							" # Clear the plot area\r\n",
							" plt.clf()\r\n",
							"\r\n",
							" # Create a bar plot of revenue by year\r\n",
							" plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')\r\n",
							"\r\n",
							" # Customize the chart\r\n",
							" plt.title('Revenue by Year')\r\n",
							" plt.xlabel('Year')\r\n",
							" plt.ylabel('Revenue')\r\n",
							" plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)\r\n",
							" plt.xticks(rotation=45)\r\n",
							"\r\n",
							" # Show the figure\r\n",
							" plt.show()"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Run the next code cell and view the results. The figure determines the shape and size of the plot.\r\n",
							"\r\n",
							"A figure can contain multiple subplots, each on its own axis."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							" # Clear the plot area\r\n",
							" plt.clf()\r\n",
							"\r\n",
							" # Create a Figure\r\n",
							" fig = plt.figure(figsize=(8,3))\r\n",
							"\r\n",
							" # Create a bar plot of revenue by year\r\n",
							" plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')\r\n",
							"\r\n",
							" # Customize the chart\r\n",
							" plt.title('Revenue by Year')\r\n",
							" plt.xlabel('Year')\r\n",
							" plt.ylabel('Revenue')\r\n",
							" plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)\r\n",
							" plt.xticks(rotation=45)\r\n",
							"\r\n",
							" # Show the figure\r\n",
							" plt.show()"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Run the next code cell and view the results. The figure contains the subplots specified in the code."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							" # Clear the plot area\r\n",
							" plt.clf()\r\n",
							"\r\n",
							" # Create a figure for 2 subplots (1 row, 2 columns)\r\n",
							" fig, ax = plt.subplots(1, 2, figsize = (10,4))\r\n",
							"\r\n",
							" # Create a bar plot of revenue by year on the first axis\r\n",
							" ax[0].bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')\r\n",
							" ax[0].set_title('Revenue by Year')\r\n",
							"\r\n",
							" # Create a pie chart of yearly order counts on the second axis\r\n",
							" yearly_counts = df_sales['OrderYear'].value_counts()\r\n",
							" ax[1].pie(yearly_counts)\r\n",
							" ax[1].set_title('Orders per Year')\r\n",
							" ax[1].legend(yearly_counts.keys().tolist())\r\n",
							"\r\n",
							" # Add a title to the Figure\r\n",
							" fig.suptitle('Sales Data')\r\n",
							"\r\n",
							" # Show the figure\r\n",
							" plt.show()"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Use the seaborn library\r\n",
							"\r\n",
							"While matplotlib enables you to create complex charts of multiple types, it can require some complex code to achieve the best results. For this reason, over the years, many new libraries have been built on the base of matplotlib to abstract its complexity and enhance its capabilities. One such library is seaborn.\r\n",
							"\r\n",
							"Run the code and observe that it displays a bar chart using the seaborn library."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							" import seaborn as sns\r\n",
							"\r\n",
							" # Clear the plot area\r\n",
							" plt.clf()\r\n",
							"\r\n",
							" # Create a bar chart\r\n",
							" ax = sns.barplot(x=\"OrderYear\", y=\"GrossRevenue\", data=df_sales)\r\n",
							" plt.show()"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Run the code and note that seaborn enables you to set a consistent color theme for your plots."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							" # Clear the plot area\r\n",
							" plt.clf()\r\n",
							"\r\n",
							" # Set the visual theme for seaborn\r\n",
							" sns.set_theme(style=\"whitegrid\")\r\n",
							"\r\n",
							" # Create a bar chart\r\n",
							" ax = sns.barplot(x=\"OrderYear\", y=\"GrossRevenue\", data=df_sales)\r\n",
							" plt.show()"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Run the code to view the yearly revenue as a line chart."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							" # Clear the plot area\r\n",
							" plt.clf()\r\n",
							"\r\n",
							" # Create a bar chart\r\n",
							" ax = sns.lineplot(x=\"OrderYear\", y=\"GrossRevenue\", data=df_sales)\r\n",
							" plt.show()"
						],
						"outputs": [],
						"execution_count": 27
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01-Spark Transform')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/03-Spark/06-Lab-transform data using Spark in Synapse Analytics"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "60355e5b-23b2-43cc-b7e7-f8ff72c4484f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/784f1210-8faf-4cf4-b9aa-e50fa084adce/resourceGroups/rg-dp-203/providers/Microsoft.Synapse/workspaces/synapse-weslbo/bigDataPools/sparkpool",
						"name": "sparkpool",
						"type": "Spark",
						"endpoint": "https://synapse-weslbo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Transform data by using Spark\n",
							"\n",
							"Apache Spark provides a distributed data processing platform that you can use to perform complex data transformations at scale.\n",
							"\n",
							"\n",
							"## Load source data\n",
							"\n",
							"Let's start by loading some historical sales order data into a dataframe.\n",
							"\n",
							"Review the code in the cell below, which loads the sales order from all of the csv files within the **data** directory. Then click the **&#9655;** button to the left of the cell to run it.\n",
							"\n",
							"> **Note**: The first time you run a cell in a notebook, the Spark pool must be started; which can take several minutes."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"order_details = spark.read.csv('/labs-dp-203/06/data/*.csv', header=True, inferSchema=True)\n",
							"display(order_details.limit(5))"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Transform the data structure\r\n",
							"\r\n",
							"The source data includes a **CustomerName** field, that contains the customer's first and last name. Let's modify the dataframe to separate this field into separate **FirstName** and **LastName** fields."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.functions import split, col\r\n",
							"\r\n",
							"# Create the new FirstName and LastName fields\r\n",
							"transformed_df = order_details.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\r\n",
							"\r\n",
							"# Remove the CustomerName field\r\n",
							"transformed_df = transformed_df.drop(\"CustomerName\")\r\n",
							"\r\n",
							"display(transformed_df.limit(5))"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"The code above creates a new dataframe with the **CustomerName** field removed and two new **FirstName** and **LastName** fields.\r\n",
							"\r\n",
							"You can use the full power of the Spark SQL library to transform the data by filtering rows, deriving, removing, renaming columns, and any applying other required data modifications.\r\n",
							"\r\n",
							"## Save the transformed data\r\n",
							"\r\n",
							"After making the required changes to the data, you can save the results in a supported file format.\r\n",
							"\r\n",
							"> **Note**: Commonly, *Parquet* format is preferred for data files that you will use for further analysis or ingestion into an analytical store. Parquet is a very efficient format that is supported by most large scale data analytics systems. In fact, sometimes your data transformation requirement may simply be to convert data from another format (such as CSV) to Parquet!\r\n",
							"\r\n",
							"Use the following code to save the transformed dataframe in Parquet format (Overwriting the data if it already exists)."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"transformed_df.write.mode(\"overwrite\").parquet('/labs-dp-203/06/transformed_data/orders.parquet')\r\n",
							"print (\"Transformed data saved!\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"In the **files** tab (which should still be open above), navigate to the root **files** container and verify that a new folder named **transformed_data** has been created, containing a file named **orders.parquet**. Then return to this notebook."
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Partition data\n",
							"\n",
							"A common way to optimize performance when dealing with large volumes of data is to partition the data files based on one or more field values. This can significant improve performance and make it easier to filter data.\n",
							"\n",
							"Use the following cell to derive new **Year** and **Month** fields and then save the resulting data in Parquet format, partitioned by year and month."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.functions import year, month, col\r\n",
							"\r\n",
							"dated_df = transformed_df.withColumn(\"Year\", year(col(\"OrderDate\"))).withColumn(\"Month\", month(col(\"OrderDate\")))\r\n",
							"display(dated_df.limit(5))\r\n",
							"dated_df.write.partitionBy(\"Year\",\"Month\").mode(\"overwrite\").parquet(\"/labs-dp-203/06/partitioned_data\")\r\n",
							"print (\"Transformed data saved!\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"In the **files** tab (which should still be open above), navigate to the root **files** container and verify that a new folder named **partitioned_data** has been created, containing a hierachy of folders in the format **Year=*NNNN*** / **Month=*N***, each containing a .parquet file for the orders placed in the corresponding year and month. Then return to this notebook.\r\n",
							"\r\n",
							"You can read this data into a dataframe from any folder in the hierarchy, using explicit values or wildcards for partitioning fields. For example, use the following code to get the sales orders placed in 2020 for all months."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"orders_2020 = spark.read.parquet('/labs-dp-203/06/partitioned_data/Year=2020/Month=*')\r\n",
							"display(orders_2020.limit(5))"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Note that the partitioning columns specified in the file path are omitted in the resulting dataframe.\r\n",
							"\r\n",
							"## Use SQL to transform data\r\n",
							"\r\n",
							"Spark is a very flexible platform, and the **SQL** library that provides the dataframe also enables you to work with data using SQL semantics. You can query and transform data in dataframes by using SQL queries, and persist the results as tables - which are metadata abstractions over files.\r\n",
							"\r\n",
							"First, use the following code to save the original sales orders data (loaded from CSV files) as a table. Technically, this is an *external* table because the **path** parameter is used to specify where the data files for the table are stored (an *internal* table is stored in the system storage for the Spark metastore and managed automatically)."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"order_details.write.saveAsTable('sales_orders', format='parquet', mode='overwrite', path='/labs-dp-203/06/sales_orders_table')"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"In the **files** tab (which should still be open above), navigate to the root **files** container and verify that a new folder named **sales_orders_table** has been created, containing parquet files for the table data. Then return to this notebook.\r\n",
							"\r\n",
							"Now that the table has been created, you can use SQL to transform it. For example, the following code derives new Year and Month columns and then saves the results as a partitioned external table."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"sql_transform = spark.sql(\"SELECT *, YEAR(OrderDate) AS Year, MONTH(OrderDate) AS Month FROM sales_orders\")\r\n",
							"display(sql_transform.limit(5))\r\n",
							"sql_transform.write.partitionBy(\"Year\",\"Month\").saveAsTable('transformed_orders', format='parquet', mode='overwrite', path='/labs-dp-203/06/transformed_orders_table')"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"source": [
							"In the **files** tab (which should still be open above), navigate to the root **files** container and verify that a new folder named **transformed_orders_table** has been created, containing a hierachy of folders in the format **Year=*NNNN*** / **Month=*N***, each containing a .parquet file for the orders placed in the corresponding year and month. Then return to this notebook.\n",
							"\n",
							"Essentially you've performed the same data transformation into partitioned parquet files as s before, but by using SQL instead of native dataframe methods.\n",
							"\n",
							"You can read this data into a dataframe from any folder in the hierarchy as before, but because the data files are also abstracted by a table in the metastore, you can query the data directly using SQL."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"SELECT * FROM transformed_orders\r\n",
							"WHERE Year = 2021\r\n",
							"    AND Month = 1"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Because these are *external* tables, you can drop the tables from the metastore without deleting the files - so the transfomed data remains available for other downstream data analytics or ingestion processes."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"DROP TABLE transformed_orders;\r\n",
							"DROP TABLE sales_orders;"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Cleanup"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from notebookutils import mssparkutils\r\n",
							"\r\n",
							"mssparkutils.fs.rm('/labs-dp-203/06/sales_orders_table', True)\r\n",
							"mssparkutils.fs.rm('/labs-dp-203/06/partitioned_data', True)\r\n",
							"mssparkutils.fs.rm('/labs-dp-203/06/transformed_orders_table', True)\r\n",
							"mssparkutils.fs.rm('/labs-dp-203/06/transformed_data', True)"
						],
						"outputs": [],
						"execution_count": 14
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01-Use Delta Lake')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/03-Spark/07-Lab-Use Delta Lake in Azure Synapse Analytics"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "6e623822-0eee-40d9-a235-bfc4510702ba"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/784f1210-8faf-4cf4-b9aa-e50fa084adce/resourceGroups/rg-dp-203/providers/Microsoft.Synapse/workspaces/synapse-weslbo/bigDataPools/sparkpool",
						"name": "sparkpool",
						"type": "Spark",
						"endpoint": "https://synapse-weslbo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Use Delta Lake with Spark in Azure Synapse Analytics\n",
							"\n",
							"Explore a CSV file on the data lake: /07/data/products.csv"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://files@datalakeweslbo.dfs.core.windows.net/labs-dp-203/07/data/products.csv', format='csv', header=True)\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Load the file data into a delta table"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"delta_table_path = \"/labs-dp-203/07/delta/products-delta\"\n",
							"df.write.format(\"delta\").save(delta_table_path)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Browse the files container\n",
							"\n",
							"On the files tab, use the ↑ icon in the toolbar to return to the root of the files container, and note that a new folder named delta has been created. Open this folder and the products-delta table it contains, where you should see the parquet format file(s) containing the data."
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Load the DeltaTable and update some data\n",
							"\n",
							"The data is loaded into a DeltaTable object and updated. You can see the update reflected in the query results."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from delta.tables import *\n",
							"from pyspark.sql.functions import *\n",
							"\n",
							"# Create a deltaTable object\n",
							"deltaTable = DeltaTable.forPath(spark, delta_table_path)\n",
							"\n",
							"# Update the table (reduce price of product 771 by 10%)\n",
							"deltaTable.update(\n",
							"    condition = \"ProductID == 771\",\n",
							"    set = { \"ListPrice\": \"ListPrice * 0.9\" })\n",
							"\n",
							"# View the updated data as a dataframe\n",
							"deltaTable.toDF().show(10)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Load delta table in data frame\n",
							"\n",
							"The following code loads the delta table data into a data frame from its location in the data lake, verifying that the change you made via a DeltaTable object has been persisted."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"new_df = spark.read.format(\"delta\").load(delta_table_path)\n",
							"new_df.show(10)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Time travel\n",
							"\n",
							"The following code uses the time travel feature of delta lake to view a previous version of the data."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"new_df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
							"new_df.show(10)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Delta table history\n",
							"\n",
							"The history of the last 20 changes to the table is shown - there should be two (the original creation, and the update you made.)"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"deltaTable.history(10).show(20, False, True)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create catalog tables\n",
							"\n",
							"So far you've worked with delta tables by loading data from the folder containing the parquet files on which the table is based. You can define catalog tables that encapsulate the data and provide a named table entity that you can reference in SQL code. Spark supports two kinds of catalog tables for delta lake:\n",
							"\n",
							"- **External tables** that are defined by the path to the parquet files containing the table data.\n",
							"- **Managed tables**, that are defined in the Hive metastore for the Spark pool.\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create an external table\n",
							"\n",
							"The following code creates a new database named AdventureWorks and then creates an external tabled named ProductsExternal in that database based on the path to the parquet files you defined previously. It then displays a description of the table's properties. Note that the Location property is the path you specified."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.sql(\"CREATE DATABASE AdventureWorks\")\n",
							"spark.sql(\"CREATE TABLE AdventureWorks.ProductsExternal USING DELTA LOCATION '{0}'\".format(delta_table_path))\n",
							"spark.sql(\"DESCRIBE EXTENDED AdventureWorks.ProductsExternal\").show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"The following code uses SQL to switch context to the AdventureWorks database (which returns no data) and then query the ProductsExternal table (which returns a resultset containing the products data in the Delta Lake table)."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"USE AdventureWorks;\n",
							"\n",
							"SELECT * FROM ProductsExternal LIMIT 5;"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create a managed table\n",
							"\n",
							"The following code creates a managed tabled named ProductsManaged based on the DataFrame you originally loaded from the products.csv file (before you updated the price of product 771). You do not specify a path for the parquet files used by the table - this is managed for you in the Hive metastore, and shown in the Location property in the table description (in the files/synapse/workspaces/synapsexxxxxxx/warehouse path)."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.write.format(\"delta\").saveAsTable(\"AdventureWorks.ProductsManaged\")\n",
							"spark.sql(\"DESCRIBE EXTENDED AdventureWorks.ProductsManaged\").show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"The following code uses SQL to query the ProductsManaged table."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"USE AdventureWorks;\n",
							"\n",
							"SELECT * FROM ProductsManaged;"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Compare external and managed tables\n",
							"\n",
							"This code lists the tables in the AdventureWorks database."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"USE AdventureWorks;\n",
							"\n",
							"SHOW TABLES;"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"The following code drops the tables from the metastore."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"USE AdventureWorks;\n",
							"\n",
							"DROP TABLE IF EXISTS ProductsExternal;\n",
							"DROP TABLE IF EXISTS ProductsManaged;"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"- Return to the files tab and view the files/delta/products-delta folder. Note that the data files still exist in this location. Dropping the external table has removed the table from the metastore, but left the data files intact.\n",
							"\n",
							"- View the files/synapse/workspaces/synapsexxxxxxx/warehouse folder, and note that there is no folder for the ProductsManaged table data. Dropping a managed table removes the table from the metastore and also deletes the table's data files.\n",
							"\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create a table using SQL\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"USE AdventureWorks;\n",
							"\n",
							"CREATE TABLE Products\n",
							"USING DELTA\n",
							"LOCATION '/labs-dp-203/07/delta/products-delta';"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Observe that the new catalog table was created for the existing Delta Lake table folder, which reflects the changes that were made previously."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"USE AdventureWorks;\n",
							"\n",
							"SELECT * FROM Products LIMIT 10;"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Use delta tables for streaming data\n",
							"\n",
							"Delta lake supports streaming data. Delta tables can be a sink or a source for data streams created using the Spark Structured Streaming API. In this example, you'll use a delta table as a sink for some streaming data in a simulated internet of things (IoT) scenario.\n",
							"\n",
							"Ensure the message Source stream created... is printed. The code you just ran has created a streaming data source based on a folder to which some data has been saved, representing readings from hypothetical IoT devices."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from notebookutils import mssparkutils\n",
							"from pyspark.sql.types import *\n",
							"from pyspark.sql.functions import *\n",
							"\n",
							"# Create a folder\n",
							"inputPath = '/labs-dp-203/07/streaming/'\n",
							"mssparkutils.fs.mkdirs(inputPath)\n",
							"\n",
							"# Create a stream that reads data from the folder, using a JSON schema\n",
							"jsonSchema = StructType([\n",
							"StructField(\"device\", StringType(), False),\n",
							"StructField(\"status\", StringType(), False)\n",
							"])\n",
							"iotstream = spark.readStream.schema(jsonSchema).option(\"maxFilesPerTrigger\", 1).json(inputPath)\n",
							"\n",
							"# Write some event data to the folder\n",
							"device_data = '''{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
							"{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
							"{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
							"{\"device\":\"Dev2\",\"status\":\"error\"}\n",
							"{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
							"{\"device\":\"Dev1\",\"status\":\"error\"}\n",
							"{\"device\":\"Dev2\",\"status\":\"ok\"}\n",
							"{\"device\":\"Dev2\",\"status\":\"error\"}\n",
							"{\"device\":\"Dev1\",\"status\":\"ok\"}'''\n",
							"mssparkutils.fs.put(inputPath + \"data.txt\", device_data, True)\n",
							"print(\"Source stream created...\")"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Write the stream to a delta table"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"delta_stream_table_path = '/labs-dp-203/07/delta/iotdevicedata'\n",
							"checkpointpath = '/labs-dp-203/07/delta/checkpoint'\n",
							"deltastream = iotstream.writeStream.format(\"delta\").option(\"checkpointLocation\", checkpointpath).start(delta_stream_table_path)\n",
							"print(\"Streaming to delta sink...\")"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Read the data in delta format into a dataframe\n",
							"\n",
							"This code reads the streamed data in delta format into a dataframe. Note that the code to load streaming data is no different to that used to load static data from a delta folder."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Read the data in delta format into a dataframe\n",
							"df = spark.read.format(\"delta\").load(delta_stream_table_path)\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create a catalog table based on the streaming sink\n",
							"\n",
							"The following code creates a catalog table named IotDeviceData (in the default database) based on the delta folder. Again, this code is the same as would be used for non-streaming data."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.sql(\"CREATE TABLE IotDeviceData USING DELTA LOCATION '{0}'\".format(delta_stream_table_path))"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Query the streaming delta DeltaTable\n",
							"\n",
							"This code queries the IotDeviceData table, which contains the device data from the streaming source."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"SELECT * FROM IotDeviceData;"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Add some more streaming DataFrame\n",
							"\n",
							"This code writes more hypothetical device data to the streaming source."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Add more data to the source stream\n",
							"more_data = '''{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
							"{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
							"{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
							"{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
							"{\"device\":\"Dev1\",\"status\":\"error\"}\n",
							"{\"device\":\"Dev2\",\"status\":\"error\"}\n",
							"{\"device\":\"Dev1\",\"status\":\"ok\"}'''\n",
							"\n",
							"mssparkutils.fs.put(inputPath + \"more-data.txt\", more_data, True)"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"SELECT * FROM IotDeviceData;"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Stop the stream."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"deltastream.stop()"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Query serverless\n",
							"\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Clean up"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"USE AdventureWorks;\n",
							"\n",
							"DROP TABLE IF EXISTS Products;\n",
							"DROP TABLE IF EXISTS ProductsExternal;\n",
							"DROP TABLE IF EXISTS ProductsManaged;\n",
							"DROP TABLE IF EXISTS iotdevicedata;"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.sql(\"DROP DATABASE AdventureWorks\")"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%fs rm -r /labs-dp-203/07/delta/\n",
							"%fs rm -r /labs-dp-203/07/streaming/"
						],
						"outputs": [],
						"execution_count": 29
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01_query_csv')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-500/03-Synapse"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0c491e99-98e3-4d4f-8eff-64eb36a2aaa2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Module 02 - 01 query multiple files using Spark\r\n",
							"\r\n",
							"Set up variable for later reference. Make sure the name of your datalake is correct"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"datalake = 'datalakecn32xts6vteh6'"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Just read in some data using CSV format \r\n",
							"\r\n",
							"- Notice we do not have header information in the file\r\n",
							"- Notice we have specified to load in ALL csv files from this folder!"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://landing@' + datalake + '.dfs.core.windows.net/Allfiles/01/data/*.csv', format='csv', header=False)\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Notice that the file does not contain any field names in the header. Let's print the schema"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.printSchema()"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Let's define our own schema on read"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql import types\r\n",
							"\r\n",
							"customSchema = types.StructType([\r\n",
							"    types.StructField(\"SalesOrderNumber\", types.StringType(), True),\r\n",
							"    types.StructField(\"SalesTerritoryKey\", types.IntegerType(), True),\r\n",
							"    types.StructField(\"OrderDate\", types.DateType(), True),\r\n",
							"    types.StructField(\"Customer\", types.StringType(), True),\r\n",
							"    types.StructField(\"Email\", types.StringType(), True),\r\n",
							"    types.StructField(\"Adress\", types.StringType(), True),\r\n",
							"    types.StructField(\"Quantity\", types.IntegerType(), True),\r\n",
							"    types.StructField(\"UnitPrice\", types.DoubleType(), True),\r\n",
							"    types.StructField(\"ShippingCost\", types.DoubleType(), True),\r\n",
							"])\r\n",
							"\r\n",
							"df = spark.read \\\r\n",
							"    .csv('abfss://landing@' + datalake + '.dfs.core.windows.net/Allfiles/01/data/*.csv', schema=customSchema)\r\n",
							"\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Now, let's filter this data using Spark SQL. First we register this table as a temporary view"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.createOrReplaceTempView(\"sales\")"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"And now we can apply the **%%sql** magic"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"SELECT * \r\n",
							"FROM SALES \r\n",
							"WHERE Customer LIKE 'E%'"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"We can also apply grouping and mathematical operations like SUM. "
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"SELECT \r\n",
							"    OrderDate,\r\n",
							"    SUM( (Quantity * UnitPrice) + ShippingCost ) AS TotalSales\r\n",
							"FROM Sales\r\n",
							"GROUP BY OrderDate\r\n",
							"ORDER BY OrderDate\r\n",
							"LIMIT 50"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Make sure to explore the CHART feature (built in in the notebook)!\r\n",
							"\r\n",
							"Create a new temporary view by using SQL:"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"CREATE OR REPLACE TEMPORARY VIEW TotalSalesByDate\r\n",
							"AS\r\n",
							"\r\n",
							"SELECT \r\n",
							"    OrderDate,\r\n",
							"    SUM( (Quantity * UnitPrice) + ShippingCost ) AS TotalSales\r\n",
							"FROM Sales\r\n",
							"GROUP BY OrderDate\r\n",
							"ORDER BY OrderDate"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"... and then use the temporary view"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"SELECT * FROM TotalSalesByDate LIMIT 10"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Lets create some chart using matplotlib"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from matplotlib import pyplot as plt\r\n",
							"\r\n",
							"totalOrdersByTerritory = sqlContext.sql(\"SELECT SalesTerritoryKey, COUNT(*) as TotalOrders \\\r\n",
							"                                         FROM Sales \\\r\n",
							"                                         GROUP BY SalesTerritoryKey\\\r\n",
							"                                         ORDER BY SalesTerritoryKey\").toPandas()\r\n",
							"\r\n",
							"# clear the plot area\r\n",
							"plt.clf()\r\n",
							"\r\n",
							"# create a figure\r\n",
							"figure = plt.figure(figsize=(12, 8))\r\n",
							"\r\n",
							"# create a bar plot of total sales by datalake\r\n",
							"plt.bar(x=totalOrdersByTerritory['SalesTerritoryKey'], height=totalOrdersByTerritory['TotalOrders'], color='magenta')\r\n",
							"\r\n",
							"# customize the chart\r\n",
							"plt.title('Order count by sales territory')\r\n",
							"plt.xlabel('Sales territory')\r\n",
							"plt.ylabel('Total number of orders')\r\n",
							"plt.grid(color='#95a5a6', linestyle='--')\r\n",
							"\r\n",
							"# show the plot area\r\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from urllib.request import urlopen\r\n",
							"import json\r\n",
							"with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\r\n",
							"    counties = json.load(response)\r\n",
							"\r\n",
							"import pandas as pd\r\n",
							"df = pd.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/fips-unemp-16.csv\",\r\n",
							"                   dtype={\"fips\": str})\r\n",
							"\r\n",
							"import plotly\r\n",
							"import plotly.express as px\r\n",
							"\r\n",
							"fig = px.choropleth(df, geojson=counties, locations='fips', color='unemp',\r\n",
							"                           color_continuous_scale=\"Viridis\",\r\n",
							"                           range_color=(0, 12),\r\n",
							"                           scope=\"usa\",\r\n",
							"                           labels={'unemp':'unemployment rate'}\r\n",
							"                          )\r\n",
							"fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\r\n",
							"\r\n",
							"# create an html document that embeds the Plotly plot\r\n",
							"h = plotly.offline.plot(fig, output_type='div')\r\n",
							"\r\n",
							"# display this html\r\n",
							"displayHTML(h)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/02_create_charts')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-500/03-Synapse"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "43779aa4-a8c6-4e2d-a9a8-932813c985dc"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Module 02 - 02 Create charts using Spark\r\n",
							"\r\n",
							"Set up variable for later reference. Make sure the name of your datalake is correct"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import types\r\n",
							"\r\n",
							"datalake = 'datalakecn32xts6vteh6'\r\n",
							"\r\n",
							"customSchema = types.StructType([\r\n",
							"    types.StructField(\"SalesOrderNumber\", types.StringType(), True),\r\n",
							"    types.StructField(\"SalesTerritoryKey\", types.IntegerType(), True),\r\n",
							"    types.StructField(\"OrderDate\", types.DateType(), True),\r\n",
							"    types.StructField(\"Customer\", types.StringType(), True),\r\n",
							"    types.StructField(\"Email\", types.StringType(), True),\r\n",
							"    types.StructField(\"Adress\", types.StringType(), True),\r\n",
							"    types.StructField(\"Quantity\", types.IntegerType(), True),\r\n",
							"    types.StructField(\"UnitPrice\", types.DoubleType(), True),\r\n",
							"    types.StructField(\"ShippingCost\", types.DoubleType(), True),\r\n",
							"])\r\n",
							"\r\n",
							"df = spark.read \\\r\n",
							"    .csv('abfss://landing@' + datalake + '.dfs.core.windows.net/Allfiles/01/data/*.csv', schema=customSchema)\r\n",
							"df.createOrReplaceTempView(\"sales\")"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Execute the following Spark SQL And explore the **Chart** feature"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"SELECT \r\n",
							"    OrderDate,\r\n",
							"    SUM( (Quantity * UnitPrice) + ShippingCost ) AS TotalSales\r\n",
							"FROM Sales\r\n",
							"GROUP BY OrderDate\r\n",
							"ORDER BY OrderDate\r\n",
							"LIMIT 50"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Let's create some chart using MATPLOTLIB"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from matplotlib import pyplot as plt\r\n",
							"\r\n",
							"totalOrdersByTerritory = sqlContext.sql(\"SELECT SalesTerritoryKey, COUNT(*) as TotalOrders \\\r\n",
							"                                         FROM Sales \\\r\n",
							"                                         GROUP BY SalesTerritoryKey\\\r\n",
							"                                         ORDER BY SalesTerritoryKey\").toPandas()\r\n",
							"\r\n",
							"# clear the plot area\r\n",
							"plt.clf()\r\n",
							"\r\n",
							"# create a figure\r\n",
							"figure = plt.figure(figsize=(12, 8))\r\n",
							"\r\n",
							"# create a bar plot of total sales by datalake\r\n",
							"plt.bar(x=totalOrdersByTerritory['SalesTerritoryKey'], height=totalOrdersByTerritory['TotalOrders'], color='magenta')\r\n",
							"\r\n",
							"# customize the chart\r\n",
							"plt.title('Order count by sales territory')\r\n",
							"plt.xlabel('Sales territory')\r\n",
							"plt.ylabel('Total number of orders')\r\n",
							"plt.grid(color='#95a5a6', linestyle='--')\r\n",
							"\r\n",
							"# show the plot area\r\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from urllib.request import urlopen\r\n",
							"import json\r\n",
							"with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\r\n",
							"    counties = json.load(response)\r\n",
							"\r\n",
							"import pandas as pd\r\n",
							"df = pd.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/fips-unemp-16.csv\",\r\n",
							"                   dtype={\"fips\": str})\r\n",
							"\r\n",
							"import plotly\r\n",
							"import plotly.express as px\r\n",
							"\r\n",
							"fig = px.choropleth(df, geojson=counties, locations='fips', color='unemp',\r\n",
							"                           color_continuous_scale=\"Viridis\",\r\n",
							"                           range_color=(0, 12),\r\n",
							"                           scope=\"usa\",\r\n",
							"                           labels={'unemp':'unemployment rate'}\r\n",
							"                          )\r\n",
							"fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\r\n",
							"\r\n",
							"# create an html document that embeds the Plotly plot\r\n",
							"h = plotly.offline.plot(fig, output_type='div')\r\n",
							"\r\n",
							"# display this html\r\n",
							"displayHTML(h)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/03_flatten')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-500/03-Synapse"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "758d110e-5e57-423d-ab1e-6869eae1745c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Analyze complex data types in Azure Synapse Analytics\r\n",
							"\r\n",
							"Notebook based on article: https://learn.microsoft.com/en-us/azure/synapse-analytics/how-to-analyze-complex-schema\r\n",
							"\r\n",
							"Complex data types are increasingly common and represent a challenge for data engineers. Analyzing nested schema and arrays can involve time-consuming and complex SQL queries. Additionally, it can be difficult to rename or cast the nested columns data type. Also, when you're working with deeply nested objects, you can encounter performance problems.\r\n",
							"\r\n",
							"Data engineers need to understand how to efficiently process complex data types and make them easily accessible to everyone. In the following example, you use Spark in Azure Synapse Analytics to read and transform objects into a flat structure through data frames. You use the serverless model of SQL in Azure Synapse Analytics to query such objects directly, and return those results as a regular table.\r\n",
							"\r\n",
							"## Create a Spark DataFrame from a JSON string\r\n",
							"\r\n",
							"1. Add the JSON content from the variable to a list.\r\n",
							"2. Create a Spark dataset from the list.\r\n",
							"3. Use spark.read.json to parse the Spark dataset."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"json = \"\"\"{\r\n",
							"    \"id\": \"66532691-ab20-11ea-8b1d-936b3ec64e54\",\r\n",
							"    \"context\": {\r\n",
							"        \"data\": {\r\n",
							"            \"eventTime\": \"2020-06-10T13:43:34.553Z\",\r\n",
							"            \"samplingRate\": \"100.0\",\r\n",
							"            \"isSynthetic\": \"false\"\r\n",
							"        },\r\n",
							"        \"session\": {\r\n",
							"            \"isFirst\": \"false\",\r\n",
							"            \"id\": \"38619c14-7a23-4687-8268-95862c5326b1\"\r\n",
							"        },\r\n",
							"        \"custom\": {\r\n",
							"            \"dimensions\": [\r\n",
							"                {\r\n",
							"                    \"customerInfo\": {\r\n",
							"                        \"ProfileType\": \"ExpertUser\",\r\n",
							"                        \"RoomName\": \"\",\r\n",
							"                        \"CustomerName\": \"diamond\",\r\n",
							"                        \"UserName\": \"XXXX@yahoo.com\"\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                {\r\n",
							"                    \"customerInfo\": {\r\n",
							"                        \"ProfileType\": \"Novice\",\r\n",
							"                        \"RoomName\": \"\",\r\n",
							"                        \"CustomerName\": \"topaz\",\r\n",
							"                        \"UserName\": \"XXXX@outlook.com\"\r\n",
							"                    }\r\n",
							"                }\r\n",
							"            ]\r\n",
							"        }\r\n",
							"    }\r\n",
							"}\"\"\"\r\n",
							"json_list = []\r\n",
							"json_list.append(json)\r\n",
							"\r\n",
							"df = spark.read.json(sc.parallelize(json_list))\r\n",
							"\r\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Define a function to flatten the nested schema\r\n",
							"\r\n",
							"You can use this generic function without change. "
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"def flatten_df(nested_df):\r\n",
							"    stack = [((), nested_df)]\r\n",
							"    columns = []\r\n",
							"\r\n",
							"    while len(stack) > 0:\r\n",
							"        parents, df = stack.pop()\r\n",
							"\r\n",
							"        flat_cols = [\r\n",
							"            col(\".\".join(parents + (c[0],))).alias(\"_\".join(parents + (c[0],)))\r\n",
							"            for c in df.dtypes\r\n",
							"            if c[1][:6] != \"struct\"\r\n",
							"        ]\r\n",
							"\r\n",
							"        nested_cols = [\r\n",
							"            c[0]\r\n",
							"            for c in df.dtypes\r\n",
							"            if c[1][:6] == \"struct\"\r\n",
							"        ]\r\n",
							"\r\n",
							"        columns.extend(flat_cols)\r\n",
							"\r\n",
							"        for nested_col in nested_cols:\r\n",
							"            projected_df = df.select(nested_col + \".*\")\r\n",
							"            stack.append((parents + (nested_col,), projected_df))\r\n",
							"\r\n",
							"    return nested_df.select(columns)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Use the function to flatten the nested schema\r\n",
							"\r\n",
							"In this step, you flatten the nested schema of the data frame (df) into a new data frame. The display function should return 10 columns and 1 row. The array and its nested elements are still there."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.types import StringType, StructField, StructType\r\n",
							"df_flat = flatten_df(df)\r\n",
							"display(df_flat.limit(10))"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Transform the array\r\n",
							"\r\n",
							"Here, you transform the array, context_custom_dimensions, in the data frame df_flat, into a new data frame df_flat_explode. In the following code, you also define which column to select:"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.functions import explode\r\n",
							"from pyspark.sql.functions import flatten\r\n",
							"from pyspark.sql.functions import arrays_zip\r\n",
							"\r\n",
							"df_flat_explode = df_flat.select(\"id\", explode(df_flat.context_custom_dimensions), \"context_session_id\",\"context_data_eventTime\",\"context_data_samplingRate\")\r\n",
							"display(df_flat_explode.limit(10))"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Use the function to flatten the nested schema\r\n",
							"\r\n",
							"Finally, you use the function to flatten the nested schema of the data frame df_flat_explode, into a new data frame, df_flat_explode_flat:"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df_flat_explode_flat = flatten_df(df_flat_explode)\r\n",
							"display(df_flat_explode_flat.limit(10))"
						],
						"outputs": [],
						"execution_count": 11
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/04-Insert data into lake database')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/02-Serverless SQL/04-Demo-Analyze data in a lake database"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "149426a8-ab9a-4cd1-89e2-32f933cffef0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/784f1210-8faf-4cf4-b9aa-e50fa084adce/resourceGroups/rg-dp-203/providers/Microsoft.Synapse/workspaces/synapse-weslbo/bigDataPools/sparkpool",
						"name": "sparkpool",
						"type": "Spark",
						"endpoint": "https://synapse-weslbo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Insert data into Lake Database\n",
							"\n",
							"This notebook will insert a record into the RetailDB lake database. The SalesOrder table will be inserted with 1 new record"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"INSERT INTO `RetailDB`.`SalesOrder` VALUES (99999, CAST('2022-01-01' AS TimeStamp), 1, 6, 5, 1)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Here we validate if the record has been inserted"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"SELECT * FROM `RetailDB`.`SalesOrder` WHERE SalesOrderId = 99999"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"The following code cell will produce an error:\n",
							"\n",
							"**DELETE is only supported with v2 tables.**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\n",
							"DELETE FROM `RetailDB`.`SalesOrder` WHERE SalesOrderId = 99999"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/04_dedicatedpool')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-500/03-Synapse"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "93d32f89-9b0b-45da-815b-6b667743d436"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Read or Write data from Apache Spark pools to a dedicated SQL pool\r\n",
							"\r\n",
							"Article: https://learn.microsoft.com/en-us/training/modules/integrate-sql-apache-spark-pools-azure-synapse-analytics/7-exercise-integrate-sql"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import types\r\n",
							"\r\n",
							"datalake = 'datalakecn32xts6vteh6'\r\n",
							"\r\n",
							"customSchema = types.StructType([\r\n",
							"    types.StructField(\"SalesOrderNumber\", types.StringType(), True),\r\n",
							"    types.StructField(\"SalesTerritoryKey\", types.IntegerType(), True),\r\n",
							"    types.StructField(\"OrderDate\", types.DateType(), True),\r\n",
							"    types.StructField(\"Customer\", types.StringType(), True),\r\n",
							"    types.StructField(\"Email\", types.StringType(), True),\r\n",
							"    types.StructField(\"Adress\", types.StringType(), True),\r\n",
							"    types.StructField(\"Quantity\", types.IntegerType(), True),\r\n",
							"    types.StructField(\"UnitPrice\", types.DoubleType(), True),\r\n",
							"    types.StructField(\"ShippingCost\", types.DoubleType(), True),\r\n",
							"])\r\n",
							"\r\n",
							"df = spark.read \\\r\n",
							"    .csv('abfss://landing@' + datalake + '.dfs.core.windows.net/Allfiles/01/data/*.csv', schema=customSchema)\r\n",
							"df.createOrReplaceTempView(\"sales\")"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"We must run code that uses the Apache Spark pool to Synapse SQL connector in Scala. To do this, we add the %%spark magic to the cell. Run the following in a new code cell to read from the top_purchases view:"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\r\n",
							"\r\n",
							"// Make sure the name of the dedcated SQL pool (SQLPool01 below) matches the name of your SQL pool.\r\n",
							"val df = spark.sqlContext.sql(\"select * from sales\")\r\n",
							"df.write.synapsesql(\"DP500DWH.dbo.TopPurchases\", Constants.INTERNAL)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 14
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/11-Spark Transform')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/05-Pipelines/11-Lab-Apache Spark notebooks in a pipeline"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "e9d3c0d1-d4bb-40a5-b971-ea4f4b44c914"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/784f1210-8faf-4cf4-b9aa-e50fa084adce/resourceGroups/rg-dp-203/providers/Microsoft.Synapse/workspaces/synapse-weslbo/bigDataPools/sparkpool",
						"name": "sparkpool",
						"type": "Spark",
						"endpoint": "https://synapse-weslbo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Transform data by using Spark\n",
							"\n",
							"This notebook transforms sales order data; converting it from CSV to Parquet format and splitting customer name into two separate fields.\n",
							"\n",
							"## Set variables"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"import uuid\r\n",
							"\r\n",
							"# Variable for unique folder name\r\n",
							"runId = uuid.uuid4()"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Make sure we store the data in a lab folder (not in the root ;-)"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"folderName = 'labs-dp-203/11/transformeddata/' + str(runId)\n",
							"folderName"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Load source data\r\n",
							"\r\n",
							"Let's start by loading some historical sales order data into a dataframe."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"order_details = spark.read.csv('/labs-dp-203/11/data/*.csv', header=True, inferSchema=True)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Transform the data structure\r\n",
							"\r\n",
							"The source data includes a **CustomerName** field, that contains the customer's first and last name. Modify the dataframe to separate this field into separate **FirstName** and **LastName** fields."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.functions import split, col\r\n",
							"\r\n",
							"# Create the new FirstName and LastName fields\r\n",
							"transformed_df = order_details.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\r\n",
							"\r\n",
							"# Remove the CustomerName field\r\n",
							"transformed_df = transformed_df.drop(\"CustomerName\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Save the transformed data\r\n",
							"\r\n",
							"Now save the transformed dataframe in Parquet format in a folder specified in a variable (Overwriting the data if it already exists)."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"transformed_df.write.mode(\"overwrite\").parquet('/%s' % folderName)\r\n",
							"print (\"Transformed data saved in %s!\" % folderName)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Lab-Use a Spark pool to analyze data')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Used in DP-203 Lab 01",
				"folder": {
					"name": "dp-203/01-Intro to data engineering"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "78916e14-e553-4de9-a3fd-1c2161e668bf"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/784f1210-8faf-4cf4-b9aa-e50fa084adce/resourceGroups/rg-dp-203/providers/Microsoft.Synapse/workspaces/synapse-weslbo/bigDataPools/sparkpool",
						"name": "sparkpool",
						"type": "Spark",
						"endpoint": "https://synapse-weslbo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Use a Spark pool to analyze data\n",
							"\n",
							"Review the code in the first cell in the notebook, and run it. The first time you run a cell in a notebook, the Spark pool is started - so it may take a minute or so to return any results.\n",
							"\n",
							"This cell will retrieve the name of the current Synapse Workspace, and derive the datalake name from it. We will use this datalake variable later when retrieving files"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Import library\n",
							"from pyspark.context import SparkContext\n",
							"\n",
							"# Create context\n",
							"sc = SparkContext.getOrCreate()\n",
							"\n",
							"# Get configuration\n",
							"tuples = sc.getConf().getAll()\n",
							"\n",
							"\n",
							"# Find spark pool name\n",
							"for element in tuples:\n",
							"    if element[0].find('spark.synapse.workspace.name') != -1:\n",
							"        datalakename = element[1].replace('synapse-', 'datalake')\n",
							"\n",
							"print(datalakename)"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Here's the cell that was generated when selecting a file from the datalake. Notice we set the datalake name variable AND we use an f-string (string interpolation)"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load(f'abfss://files@{datalakename}.dfs.core.windows.net/labs-dp-203/01/product_data/products.csv', format='csv'\r\n",
							"## If header exists uncomment line below\r\n",
							"##, header=True\r\n",
							")\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Set Header=True\n",
							"(because the products.csv file has the column headers in the first line)"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\n",
							"df = spark.read.load(f'abfss://files@{datalakename}.dfs.core.windows.net/labs-dp-203/01/product_data/products.csv', format='csv'\n",
							"    , header=True \n",
							")\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Count by category\n",
							"\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df_counts = df.groupby(df.Category).count()\n",
							"display(df_counts)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create a chart\n",
							"\n",
							"In the results output for the cell, select the Chart view. "
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Query Azure Cosmos DB from Azure Synapse Analytics')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "dp-203/06-HTAP/14-Lab-Synapse Link CosmosDB"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "756e8be1-955c-4cf8-ad5f-7be8174987f1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/784f1210-8faf-4cf4-b9aa-e50fa084adce/resourceGroups/rg-dp-203/providers/Microsoft.Synapse/workspaces/synapse-weslbo/bigDataPools/sparkpool",
						"name": "sparkpool",
						"type": "Spark",
						"endpoint": "https://synapse-weslbo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Query Azure Cosmos DB from Azure Synapse Analytics\n",
							"\n",
							"Read from Cosmos DB analytical store into a Spark DataFrame and display 10 rows from the DataFrame\n",
							"\n",
							"To select a preferred list of regions in a multi-region Cosmos DB account, add .option(\"spark.cosmos.preferredRegions\", \"<Region1>,<Region2>\")\n",
							"\n",
							"The results should include three records; one for each of the items you added to the Cosmos DB database. Each record includes the fields you entered when you created the items as well as some of the metadata fields that were automatically generated.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"df = spark.read\\\n",
							"    .format(\"cosmos.olap\")\\\n",
							"    .option(\"spark.synapse.linkedService\", \"CosmosDb\")\\\n",
							"    .option(\"spark.cosmos.container\", \"sales\")\\\n",
							"    .load()\n",
							"\n",
							"display(df.limit(10))\n",
							"\n",
							"\n",
							"## Notice that the name of the container is CASE SENSITIVE!"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"The following query creates a new dataframe containing only the customerid and customerdetails columns. Observe that the customerdetails column contains the JSON structure for the nested data in the source item. In the table of results that is displayed, you can use the ► icon next to the JSON value to expand it and see the individual fields it contains."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"customer_df = df.select(\"customerid\", \"customerdetails\")\n",
							"display(customer_df)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"The following should include the customername and customeremail from the customerdetails value as columns:\n",
							"\n",
							"Spark enables you to run complex data manipulation code to restructure and explore the data from Cosmos DB. In this case, the PySpark language enables you to navigate the JSON properties hierarchy to retrieve the child fields of the customerdetails field."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"customerdetails_df = df.select(\"customerid\", \"customerdetails.*\")\n",
							"display(customerdetails_df)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Create a new database containing a table that includes data from the Cosmos DB analytical store."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"-- Create a logical database in the Spark metastore\n",
							"CREATE DATABASE salesdb;\n",
							"\n",
							"USE salesdb;\n",
							"\n",
							"-- Create a table from the Cosmos DB container\n",
							"CREATE TABLE salesorders using cosmos.olap options (\n",
							"    spark.synapse.linkedService 'CosmosDb',\n",
							"    spark.cosmos.container 'sales' -- case SENSITIVE\n",
							");\n",
							"\n",
							"-- Query the table\n",
							"SELECT *\n",
							"FROM salesorders;"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Observe that when using Spark SQL, you can retrieve named properties of a JSON structure as columns."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"SELECT id, orderdate, customerdetails.customername, product\n",
							"FROM salesorders\n",
							"ORDER BY id;"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Clean up"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"\n",
							"DROP TABLE salesorders;\n",
							"DROP DATABASE salesdb;"
						],
						"outputs": [],
						"execution_count": 20
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Restaurant Reviews using ChatGPT')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Scenario"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "3438e4c7-0fa0-47e9-9fcb-d85291270a3e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/784f1210-8faf-4cf4-b9aa-e50fa084adce/resourceGroups/rg-dp-203/providers/Microsoft.Synapse/workspaces/synapse-weslbo/bigDataPools/sparkpool",
						"name": "sparkpool",
						"type": "Spark",
						"endpoint": "https://synapse-weslbo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Sentiment review (fake) restaurant reviews using Synapse and ChatGPT\r\n",
							"![Picture](https://github.com/weslbo/dp-203/raw/main/images/e525e0a5-ff67-4d6d-b32a-f35f4e716919.jpg)\r\n",
							"This notebook is based on an article and video from Thomas Costers and Stijn Wynants\r\n",
							"\r\n",
							"- https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/using-openai-gpt-in-synapse-analytics/ba-p/3751815\r\n",
							"- https://www.youtube.com/watch?v=CY4dAWvh60M\r\n",
							"\r\n",
							"One of the SynapseML’s capabilities is providing simple APIs for pre-built intelligent services, such as Azure cognitive services. Azure OpenAI is part of the cognitive services stack, making it accessible from within Synapse Spark pools. In order to use the Azure OpenAI in Synapse Spark, we’ll be using three components. The setup of these components is out of scope for this article.\r\n",
							"\r\n",
							"- A Synapse Analytics workspace with a Spark Pool\r\n",
							"- An Azure OpenAI cognitive service with text-davinci-003 model deployed\r\n",
							"- Azure Key vault to store the OpenAI API key\r\n",
							"\r\n",
							"Use the [Azure OpenAI Studio playground](https://oai.azure.com/portal/playground) to test the following prompt\r\n",
							"\r\n",
							"```json\r\n",
							"Generate a random negative, neutral or positive restaurant review. Use the following json structure: \r\n",
							"{\r\n",
							"    \"restaurant\": \"\",\r\n",
							"    \"location\": \"\",\r\n",
							"    \"date\": \"\",\r\n",
							"    \"review\": \"\"\r\n",
							"}\r\n",
							"````"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"The following code is applicable For Spark3.2 pool. SynapseML can be conveniently installed on Synapse using this piece of configuration"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%%configure -f\r\n",
							"{\r\n",
							"  \"name\": \"synapseml\",\r\n",
							"  \"conf\": {\r\n",
							"      \"spark.jars.packages\": \"com.microsoft.azure:synapseml_2.12:0.11.0,org.apache.spark:spark-avro_2.12:3.3.1\",\r\n",
							"      \"spark.jars.repositories\": \"https://mmlspark.azureedge.net/maven\",\r\n",
							"      \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.12,org.scalactic:scalactic_2.12,org.scalatest:scalatest_2.12,com.fasterxml.jackson.core:jackson-databind\",\r\n",
							"      \"spark.yarn.user.classpath.first\": \"true\",\r\n",
							"      \"spark.sql.parquet.enableVectorizedReader\": \"false\",\r\n",
							"      \"spark.sql.legacy.replaceDatabricksSparkAvro.enabled\": \"true\"\r\n",
							"  }\r\n",
							"}"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Generate fake reviews, using ChatGPT\r\n",
							"\r\n",
							"Now we will genrate 5 reviews. We first create a set of prompts for the number of reviews we want to generate"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from synapse.ml.core.platform import running_on_synapse, find_secret\r\n",
							"from pyspark.sql.types import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"from synapse.ml.cognitive import OpenAICompletion\r\n",
							"\r\n",
							"key = find_secret(\"openaikey\", \"keyvault-weslbo\")  # replace this with your secret and keyvault\r\n",
							"nrOfReviews = 10\r\n",
							"\r\n",
							"completion = (\r\n",
							"    OpenAICompletion()\r\n",
							"    .setSubscriptionKey(key)\r\n",
							"    .setDeploymentName(\"text-davinci-003\")\r\n",
							"    .setUrl(\"https://openai-wedebols-3.openai.azure.com/\")\r\n",
							"    .setMaxTokens(2048)\r\n",
							"    .setPromptCol(\"prompt\")\r\n",
							"    .setErrorCol(\"error\")\r\n",
							"    .setOutputCol(\"response\")\r\n",
							"    .setTemperature(0.7)\r\n",
							")\r\n",
							"\r\n",
							"def generateRestaurantPrompt() -> str:\r\n",
							"    return \"Generate a random negative, neutral or positive restaurant review. Use the following json structure: {\\\"restaurant\\\": \\\"\\\",\\\"location\\\": \\\"\\\",\\\"date\\\": \\\"\\\",\\\"review\\\": \\\"\\\"}\"\r\n",
							"generateRestaurantPrompt_udf = udf(lambda: generateRestaurantPrompt(), StringType())\r\n",
							"\r\n",
							"df_prompts = spark.range(1, nrOfReviews+1) \\\r\n",
							"    .withColumnRenamed(\"restaurant\", \"review\") \\\r\n",
							"    .withColumn(\"prompt\", generateRestaurantPrompt_udf())\r\n",
							"\r\n",
							"display(df_prompts)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Then, we will call the OpenAI service and get the actual (fake) reviews"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df_reviews_json = completion.transform(df_prompts).cache() \\\r\n",
							"    .select(\r\n",
							"        col(\"id\"),\r\n",
							"        col(\"prompt\"),\r\n",
							"        col(\"response.choices.text\").getItem(0).alias(\"json\"),\r\n",
							"        col(\"error\")\r\n",
							"    )\r\n",
							"\r\n",
							"display(df_reviews_json)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Since we get json data back, we have to apply some schema to it and retrieve the actual Restaurant Name and Review text."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"schema = StructType([ \\\r\n",
							"    StructField(\"restaurant\", StringType(), False), \\\r\n",
							"    StructField(\"location\", StringType(), False), \\\r\n",
							"    StructField(\"date\", StringType(), False), \\\r\n",
							"    StructField(\"review\", StringType(), False) \\\r\n",
							"])\r\n",
							"\r\n",
							"df_reviews_table = df_reviews_json.withColumn(\"json\", from_json(col(\"json\"), schema)) \\\r\n",
							"    .select(col(\"id\"), col(\"json.*\"), col(\"error\"))\r\n",
							"\r\n",
							"display(df_reviews_table)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Use SQL"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_reviews_table.createOrReplaceTempView(\"reviews\")"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"SELECT * FROM reviews ORDER BY location"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Detect sentiment\r\n",
							"\r\n",
							"Now that we have are restaurants and reviews, it's time to detect the sentiment. Again, we can use the OpenAI playground to test our prompt \r\n",
							"\r\n",
							"```text\r\n",
							"Classify the sentiment of following restaurant review.\r\n",
							"Classifications: [Positive, Negative, Neutral]\r\n",
							"Review: \"\"\"The food here is so delicious. The crepes are made to perfection and the servers are so friendly and helpful. I highly recommend it!\"\"\"\r\n",
							"Classification:\r\n",
							"``` \r\n",
							"\r\n",
							"When we are ready we the prompt, it's time to generate the prompt within our dataset."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"def generateSentimentPrompt(s: str) -> str:\r\n",
							"    return \"Classify the sentiment of following restaurant review.\\nClassifications: [Positive, Negative, Neutral]\\nReview: \" + s +\"\\nClassification:\"\r\n",
							"generateSentimentPrompt_udf = udf(lambda s: generateSentimentPrompt(s), StringType())\r\n",
							"\r\n",
							"df_sentiment_prompt = df_reviews_table.withColumn(\"prompt\", generateSentimentPrompt_udf(col(\"review\")))\r\n",
							"display(df_sentiment_prompt)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Now, we apply the transformation. This is where the actual call happens towards OpenAI api"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df_sentiment_json = completion.transform(df_sentiment_prompt).cache() \\\r\n",
							"    .select(\r\n",
							"        col(\"id\"),\r\n",
							"        col(\"restaurant\"),\r\n",
							"        col(\"review\"),\r\n",
							"        col(\"response.choices.text\").getItem(0).alias(\"sentiment\")\r\n",
							"    )\r\n",
							"\r\n",
							"display(df_sentiment_json)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Review the output above!\r\n",
							"\r\n",
							"Todo:\r\n",
							"1. Make sure quotes are properly handled (otherwise you get 'undefined')\r\n",
							"2. I only get positive sentiment... Probably need to tweak the openai deployment model...."
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ShipGenerator')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Scenario"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "55454808-4e84-4dc6-baaf-a142567026b0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/784f1210-8faf-4cf4-b9aa-e50fa084adce/resourceGroups/rg-dp-203/providers/Microsoft.Synapse/workspaces/synapse-weslbo/bigDataPools/sparkpool",
						"name": "sparkpool",
						"type": "Spark",
						"endpoint": "https://synapse-weslbo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"vessel_names = ['Sea Serpent', 'Ocean\\'s Fury', 'The Kraken', 'Neptune\\'s Trident', 'The Leviathan', 'The Siren\\'s Call', 'The Mermaid\\'s Song', 'The Sea Witch', 'The Black Pearl', 'The Flying Dutchman', 'Davy Jones\\' Locker', 'Poseidon\\'s Wrath', 'The Jolly Roger', 'Captain Hook\\'s Revenge','Treasure Island','Calypso\\'s Curse','Moby Dick','White Whale','Nemo\\'s Nautilus','The Salty Dog','The Seafarer','The Mariner','The Corsair','The Buccaneer','The Privateer','The Swashbuckler','The Cutlass ',' The Long John Silver ',' The Blackbeard ',' The Bluebeard']\r\n",
							"\r\n",
							"data = [\r\n",
							"    {\r\n",
							"        \"device_code\": \"WSO200\",\r\n",
							"        \"device_name\": \"Ultrasonic Wind/Weather Station\",\r\n",
							"        \"parameter\": \"Wind Speed Range\",\r\n",
							"        \"comment\": \"0 to 99 knots (1 to 185 kph)\",\r\n",
							"        \"data_type\": \"number\",\r\n",
							"        \"min_value\": 0,\r\n",
							"        \"max_value\": 99,\r\n",
							"        \"frequency_in_ms\": 100\r\n",
							"    },\r\n",
							"    {\r\n",
							"        \"device_code\": \"WSO200\",\r\n",
							"        \"device_name\": \"Ultrasonic Wind/Weather Station\",\r\n",
							"        \"parameter\": \"Wind Speed Resolution\",\r\n",
							"        \"comment\": \"0.1 knot (0.19 kph)\",\r\n",
							"        \"data_type\": \"number\",\r\n",
							"        \"min_value\": 0.0,\r\n",
							"        \"max_value\": 1.0,\r\n",
							"        \"frequency_in_ms\": 100\r\n",
							"    },\r\n",
							"    {\r\n",
							"        \"device_code\": \"WSO200\",\r\n",
							"        \"device_name\": \"Ultrasonic Wind/Weather Station\",\r\n",
							"        \"parameter\": \"Wind Speed Accuracy\",\r\n",
							"        \"comment\": \"+/-10% or 1 knot (1.9kph), Tilt 0° (operates to 30° tilt)\",\r\n",
							"        \"data_type\": \"number\",\r\n",
							"        \"min_value\": 0,\r\n",
							"        \"max_value\": 360,\r\n",
							"        \"frequency_in_ms\": 100\r\n",
							"    },\r\n",
							"    {\r\n",
							"        \"device_code\": \"WSO200\",\r\n",
							"        \"device_name\": \"Ultrasonic Wind/Weather Station\",\r\n",
							"        \"parameter\": \"Wind Direction Accuracy\",\r\n",
							"        \"comment\": \"+/-5°\",\r\n",
							"        \"data_type\": \"number\",\r\n",
							"        \"min_value\": 0,\r\n",
							"        \"max_value\": 360,\r\n",
							"        \"frequency_in_ms\": 100\r\n",
							"    },\r\n",
							"    {\r\n",
							"        \"device_code\": \"WSO200\",\r\n",
							"        \"device_name\": \"Ultrasonic Wind/Weather Station\",\r\n",
							"        \"parameter\": \"Temperature Range\",\r\n",
							"        \"comment\": \"-25° to 55° C\",\r\n",
							"        \"data_type\": \"number\",\r\n",
							"        \"min_value\": -25,\r\n",
							"        \"max_value\": 55,\r\n",
							"        \"frequency_in_ms\": 500\r\n",
							"    },\r\n",
							"    {\r\n",
							"        \"device_code\": \"WSO200\",\r\n",
							"        \"device_name\": \"Ultrasonic Wind/Weather Station\",\r\n",
							"        \"parameter\": \"Temperature Resolution\",\r\n",
							"        \"comment\": \"0.18°F (0.1°C)\",\r\n",
							"        \"data_type\": \"number\",\r\n",
							"        \"min_value\": 0.0,\r\n",
							"        \"max_value\": 1.0,\r\n",
							"        \"frequency_in_ms\": 500\r\n",
							"    },\r\n",
							"    {\r\n",
							"        \"device_code\": \"WSO200\",\r\n",
							"        \"device_name\": \"Ultrasonic Wind/Weather Station\",\r\n",
							"        \"parameter\": \"Temperature Accuracy\",\r\n",
							"        \"comment\": \"+/-1.8°F (1°C)\",\r\n",
							"        \"data_type\": \"number\",\r\n",
							"        \"min_value\": 0.0,\r\n",
							"        \"max_value\": 1.0,\r\n",
							"        \"frequency_in_ms\": 500\r\n",
							"    },\r\n",
							"    {\r\n",
							"        \"device_code\": \"WSO200\",\r\n",
							"        \"device_name\": \"Ultrasonic Wind/Weather Station\",\r\n",
							"        \"parameter\": \"Relative Humidity Range\",\r\n",
							"        \"comment\": \"0-99% RH\",\r\n",
							"        \"data_type\": \"number\",\r\n",
							"        \"min_value\": 0,\r\n",
							"        \"max_value\": 99,\r\n",
							"        \"frequency_in_ms\": 1000\r\n",
							"    },\r\n",
							"    {\r\n",
							"        \"device_code\": \"WSO200\",\r\n",
							"        \"device_name\": \"Ultrasonic Wind/Weather Station\",\r\n",
							"        \"parameter\": \"Relative Humidity Resolution\",\r\n",
							"        \"comment\": \"1% RH\",\r\n",
							"        \"data_type\": \"number\",\r\n",
							"        \"min_value\": 0,\r\n",
							"        \"max_value\": 99,\r\n",
							"        \"frequency_in_ms\": 1000\r\n",
							"    },\r\n",
							"    {\r\n",
							"        \"device_code\": \"WSO200\",\r\n",
							"        \"device_name\": \"Ultrasonic Wind/Weather Station\",\r\n",
							"        \"parameter\": \"Relative Humidity Accuracy +/-4% RH\",\r\n",
							"        \"data_type\": \"number\",\r\n",
							"        \"min_value\": -4,\r\n",
							"        \"max_value\": 4,\r\n",
							"        \"frequency_in_ms\": 1000\r\n",
							"    },\r\n",
							"    {\r\n",
							"        \"device_code\": \"WSO200\",\r\n",
							"        \"device_name\": \"Ultrasonic Wind/Weather Station\",\r\n",
							"        \"parameter\": \"Barometric Pressure Range\",\r\n",
							"        \"comment\": \"26 to 32” HG (880 to 1080 mb)\",\r\n",
							"        \"data_type\": \"number\",\r\n",
							"        \"min_value\": 800,\r\n",
							"        \"max_value\": 1080,\r\n",
							"        \"frequency_in_ms\": 10000\r\n",
							"    },\r\n",
							"    {\r\n",
							"        \"device_code\": \"WSO200\",\r\n",
							"        \"device_name\": \"Ultrasonic Wind/Weather Station\",\r\n",
							"        \"parameter\": \"Barometric Pressure Resolution\",\r\n",
							"        \"comment\": \"0.03” HG (1 mb)\",\r\n",
							"        \"data_type\": \"number\",\r\n",
							"        \"min_value\": 0.0,\r\n",
							"        \"max_value\": 1.1,\r\n",
							"        \"frequency_in_ms\": 10000\r\n",
							"    },\r\n",
							"    {\r\n",
							"        \"device_code\": \"WSO200\",\r\n",
							"        \"device_name\": \"Ultrasonic Wind/Weather Station\",\r\n",
							"        \"parameter\": \"Barometric Pressure Accuracy\",\r\n",
							"        \"comment\": \"+/-0.09” HG (+/-3 mb)\",\r\n",
							"        \"data_type\": \"number\",\r\n",
							"        \"min_value\": 0.0,\r\n",
							"        \"max_value\": 1.0,\r\n",
							"        \"frequency_in_ms\": 10000\r\n",
							"    },\r\n",
							"    {\r\n",
							"        \"device_code\": \"GPS200\",\r\n",
							"        \"device_name\": \"Maretron GPS200\",\r\n",
							"        \"parameter\": \"System Time\",\r\n",
							"        \"comment\": \"\",\r\n",
							"        \"data_type\": \"datetime\",\r\n",
							"        \"frequency_in_ms\": 1000\r\n",
							"    },\r\n",
							"    {\r\n",
							"        \"device_code\": \"GPS200\",\r\n",
							"        \"device_name\": \"Maretron GPS200\",\r\n",
							"        \"parameter\": \"Speed\",\r\n",
							"        \"comment\": \"\",\r\n",
							"        \"data_type\": \"number\",\r\n",
							"        \"min_value\": 0.0,\r\n",
							"        \"max_value\": 200,\r\n",
							"        \"frequency_in_ms\": 1000\r\n",
							"    },\r\n",
							"    {\r\n",
							"        \"device_code\": \"GPS200\",\r\n",
							"        \"device_name\": \"Maretron GPS200\",\r\n",
							"        \"parameter\": \"GNSS Satellites in View\",\r\n",
							"        \"comment\": \"\",\r\n",
							"        \"data_type\": \"number\",\r\n",
							"        \"min_value\": 0.0,\r\n",
							"        \"max_value\": 10,\r\n",
							"        \"frequency_in_ms\": 1000\r\n",
							"    },\r\n",
							"    {\r\n",
							"        \"device_code\": \"GPS200\",\r\n",
							"        \"device_name\": \"Maretron GPS200\",\r\n",
							"        \"parameter\": \"GNSS Position Data\",\r\n",
							"        \"comment\": \"\",\r\n",
							"        \"data_type\": \"string\",\r\n",
							"        \"frequency_in_ms\": 1000\r\n",
							"    }\r\n",
							"]"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkpool')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 4,
					"minNodeCount": 3
				},
				"nodeCount": 0,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.2",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sqldwh')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		}
	]
}